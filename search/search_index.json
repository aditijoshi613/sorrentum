{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"Buildmeister_process/","text":"Buildmeister process General Notification system Buildmeister instructions update_amp_submodule fails Post-mortem analysis (TBD) General Buildmeister rotates every 2 weeks To see who is the Buildmeister now refer to Buildmeister gsheet Each rotation should be confirmed by a 'handshake' between the outgoing Buildmeister and the new one in the related Telegram chat Buildmeister is responsible for: Pushing team members to fix broken tests Conducting post-mortem analysis Why did the break happen? What can we avoid the problem, especially through process and automation Testing workflows are available via github actions: Testing workflows that need to be checked: cmamp: Fast tests Slow tests dev_tools: Fast tests Slow tests Refer to .github dir in the repo for update schedule of GH actions Additional information about the tests and gdoc Notification system @CK_cmamp_buildbot notifies the team about breaks via Telegram channel CK build notifications A notification contains: Failing tests type: fast/slow/super-slow Repo Branch Event Link to a failing run Example: - Buildmeister instructions You receive a break notification from @CK_cmamp_buildbot Have a look at the message Do it right away, this is always your highest priority task Notify the team Post on the CK build notifications Telegram channel what tests broke, e.g., FAILED knowledge_graph/vendors/test/test_utils.py::TestClean::test_clean If unsure about the cause of failure (chance that failure is temporary): Do a quick run locally for failed test If test is specific and can not be run locally, rerun the regressions Ask if somebody knows what is the problem If you know who is in charge of that test (you can use git blame ) ask directly If the offender says that it's fixing the bug right away, let him / her do it Otherwise file a bug to track the issue File an Issue in GH / ZH to report the failing tests and the errors Example: https://app.zenhub.com/workspaces/cm-615371012ed326001e044788/issues/alphamatic/dev_tools/318 Issue title template Build fail - {repo} {test type} ({run number}) Example: Build fail - Cmamp fast_tests (1442077107) Paste the URL of the failing run Example: https://github.com/alphamatic/dev_tools/actions/runs/1497955663 Provide as much information as possible to give an understanding of the problem List all the tests with FAILED status in a github run, e.g., FAILED knowledge_graph/vendors/test/test_p1_utils.py::TestClean::test_clean FAILED knowledge_graph/vendors/nbsc/test/test_nbsc_utils.py::TestExposeNBSCMetadata::test_expose_nbsc_metadata Stack trace or part of it (if it's too large) ``` Traceback (most recent call last): File \"/.../automl/hypotheses/test/test*rh_generator.py\", line 104, in test1 kg_metadata, * = p1ut.load_release(version=\"0.5.2\") File \"/.../knowledge_graph/vendors/utils.py\", line 53, in load_release % version, File \"/.../amp/helpers/dbg.py\", line 335, in dassert_dir_exists _dfatal(txt, msg, *args) File \"/.../amp/helpers/dbg.py\", line 97, in _dfatal dfatal(dfatal_txt) File \"/.../amp/helpers/dbg.py\", line 48, in dfatal raise assertion_type(ret) AssertionError: ############################################################################## Failed assertion * dir='/fsx/research/data/kg/releases/timeseries_db/v0.5.2' doesn't exist or it's not a dir The requested version 0.5.2 has no directory associated with it. ``` Add the issue to the BUILD - Breaks Epic so that we can track it If the failures are not connected to each other, file separate issues for each of the potential root cause Keep issues grouped according to the codebase organization Post the issue reference on Telegram channel CK build notifications You can quickly discuss there who will take care of the broken tests, assign that person You can use git blame to see who wrote the test Otherwise, assign it to the person who can reroute Our policy is \"fix it or revert\" The build needs to go back to green within 1 hr Either the person responsible for the break fixes the bug within 1 hour Or you need to push the responsible person to disable the test Do not make the decision about disabling the test yourself! First, check with the responsible person, and if he / she is ok with disabling, do it NB! Disabling the test is not the first choice, it's a measure of last resort! Regularly check issues that belong to the Epic BUILD - Breaks . You have to update the break issues if the problem was solved or partially solved. Pay special attention to the failures which resulted in disabling tests When your time of the Buildmeister duties is over, confirm the rotation with the next responsible person in the related Telegram chat. update_amp_submodule fails When this happens, the first thing to do is attempt to update the amp pointer manually Instructions: > cd src/dev_tools1 > git checkout master > git pull --recurse-submodules > cd amp > git checkout master > git pull origin master > cd .. > git add \"amp\" > git commit -m \"Update amp pointer\" There is also an invoke target git_roll_amp_forward that does an equivalent operation Post-mortem analysis (TBD) We want to understand on why builds are broken so that we can improve the system to make it more robust In order to do that, we need to understand the failure modes of the system For this reason we keep a log of all the issues and what was the root cause After each break fill the Buildmeister spreadsheet sheet \"Post-mortem breaks analysis\" Date column: Enter the date when the break took place Keep the bug ordered in reverse chronological order (i.e., most recent dates first) Repo column: Specify the repo where break occurred amp ... Test type column: Specify the type of the failing tests Fast Slow Super-slow Link column: Provide a link to a failing run Reason column: Specify the reason of the break Merged a branch with broken tests Master was not merged in a branch Merged broken slow tests without knowing that Underlying data changed Issue column: Provide the link to the ZH issue with the break description Solution column: Provide the solution description of the problem Problem that led to the break was solved Failing tests were disabled, i.e. problem was not solved","title":"Buildmeister process"},{"location":"Buildmeister_process/#buildmeister-process","text":"General Notification system Buildmeister instructions update_amp_submodule fails Post-mortem analysis (TBD)","title":"Buildmeister process"},{"location":"Buildmeister_process/#general","text":"Buildmeister rotates every 2 weeks To see who is the Buildmeister now refer to Buildmeister gsheet Each rotation should be confirmed by a 'handshake' between the outgoing Buildmeister and the new one in the related Telegram chat Buildmeister is responsible for: Pushing team members to fix broken tests Conducting post-mortem analysis Why did the break happen? What can we avoid the problem, especially through process and automation Testing workflows are available via github actions: Testing workflows that need to be checked: cmamp: Fast tests Slow tests dev_tools: Fast tests Slow tests Refer to .github dir in the repo for update schedule of GH actions Additional information about the tests and gdoc","title":"General"},{"location":"Buildmeister_process/#notification-system","text":"@CK_cmamp_buildbot notifies the team about breaks via Telegram channel CK build notifications A notification contains: Failing tests type: fast/slow/super-slow Repo Branch Event Link to a failing run Example: -","title":"Notification system"},{"location":"Buildmeister_process/#buildmeister-instructions","text":"You receive a break notification from @CK_cmamp_buildbot Have a look at the message Do it right away, this is always your highest priority task Notify the team Post on the CK build notifications Telegram channel what tests broke, e.g., FAILED knowledge_graph/vendors/test/test_utils.py::TestClean::test_clean If unsure about the cause of failure (chance that failure is temporary): Do a quick run locally for failed test If test is specific and can not be run locally, rerun the regressions Ask if somebody knows what is the problem If you know who is in charge of that test (you can use git blame ) ask directly If the offender says that it's fixing the bug right away, let him / her do it Otherwise file a bug to track the issue File an Issue in GH / ZH to report the failing tests and the errors Example: https://app.zenhub.com/workspaces/cm-615371012ed326001e044788/issues/alphamatic/dev_tools/318 Issue title template Build fail - {repo} {test type} ({run number}) Example: Build fail - Cmamp fast_tests (1442077107) Paste the URL of the failing run Example: https://github.com/alphamatic/dev_tools/actions/runs/1497955663 Provide as much information as possible to give an understanding of the problem List all the tests with FAILED status in a github run, e.g., FAILED knowledge_graph/vendors/test/test_p1_utils.py::TestClean::test_clean FAILED knowledge_graph/vendors/nbsc/test/test_nbsc_utils.py::TestExposeNBSCMetadata::test_expose_nbsc_metadata Stack trace or part of it (if it's too large) ``` Traceback (most recent call last): File \"/.../automl/hypotheses/test/test*rh_generator.py\", line 104, in test1 kg_metadata, * = p1ut.load_release(version=\"0.5.2\") File \"/.../knowledge_graph/vendors/utils.py\", line 53, in load_release % version, File \"/.../amp/helpers/dbg.py\", line 335, in dassert_dir_exists _dfatal(txt, msg, *args) File \"/.../amp/helpers/dbg.py\", line 97, in _dfatal dfatal(dfatal_txt) File \"/.../amp/helpers/dbg.py\", line 48, in dfatal raise assertion_type(ret) AssertionError: ############################################################################## Failed assertion * dir='/fsx/research/data/kg/releases/timeseries_db/v0.5.2' doesn't exist or it's not a dir The requested version 0.5.2 has no directory associated with it. ``` Add the issue to the BUILD - Breaks Epic so that we can track it If the failures are not connected to each other, file separate issues for each of the potential root cause Keep issues grouped according to the codebase organization Post the issue reference on Telegram channel CK build notifications You can quickly discuss there who will take care of the broken tests, assign that person You can use git blame to see who wrote the test Otherwise, assign it to the person who can reroute Our policy is \"fix it or revert\" The build needs to go back to green within 1 hr Either the person responsible for the break fixes the bug within 1 hour Or you need to push the responsible person to disable the test Do not make the decision about disabling the test yourself! First, check with the responsible person, and if he / she is ok with disabling, do it NB! Disabling the test is not the first choice, it's a measure of last resort! Regularly check issues that belong to the Epic BUILD - Breaks . You have to update the break issues if the problem was solved or partially solved. Pay special attention to the failures which resulted in disabling tests When your time of the Buildmeister duties is over, confirm the rotation with the next responsible person in the related Telegram chat.","title":"Buildmeister instructions"},{"location":"Buildmeister_process/#update_amp_submodule-fails","text":"When this happens, the first thing to do is attempt to update the amp pointer manually Instructions: > cd src/dev_tools1 > git checkout master > git pull --recurse-submodules > cd amp > git checkout master > git pull origin master > cd .. > git add \"amp\" > git commit -m \"Update amp pointer\" There is also an invoke target git_roll_amp_forward that does an equivalent operation","title":"update_amp_submodule fails"},{"location":"Buildmeister_process/#post-mortem-analysis-tbd","text":"We want to understand on why builds are broken so that we can improve the system to make it more robust In order to do that, we need to understand the failure modes of the system For this reason we keep a log of all the issues and what was the root cause After each break fill the Buildmeister spreadsheet sheet \"Post-mortem breaks analysis\" Date column: Enter the date when the break took place Keep the bug ordered in reverse chronological order (i.e., most recent dates first) Repo column: Specify the repo where break occurred amp ... Test type column: Specify the type of the failing tests Fast Slow Super-slow Link column: Provide a link to a failing run Reason column: Specify the reason of the break Merged a branch with broken tests Master was not merged in a branch Merged broken slow tests without knowing that Underlying data changed Issue column: Provide the link to the ZH issue with the break description Solution column: Provide the solution description of the problem Problem that led to the break was solved Failing tests were disabled, i.e. problem was not solved","title":"Post-mortem analysis (TBD)"},{"location":"Code_review/","text":"Code review General rules about code review Read the Google code review best practices Code review workflows Pull request From the code author point of view Why we review code PR checklist The golden rule of code review Be clear in the PR request about what you want Do not mix changes and refactoring / shuffling code Double check before sending a PR Reviewing other people's code is usually not fun The first reviews are painful Apply review comments everywhere Look at the code top-to-bottom Answering comments after a review Apply changes to a review quickly Ask for another review Workflow of a review in terms of GH labels Link PR to GH issue Fix later From the code reviewer point of view Post-commit review Code walk-through Close the PR and delete the branch Give priority to code review Multiple reviewers problem Remember \"small steps ahead\" Nothing is too small Final GH comment General rules about code review Read the Google code review best practices From the developer\\'s perspective From the reviewer\\'s perspective Where the Google guide says \"CL\", think \"PR\" Read it (several times, if you need to) Think about it Understand the rationale Code review workflows Pull request Our usual review process is to work in a branch and create a pull request See the Git notes for details The name of the pull request is generated with ghi_show.py and looks like PTask2704 make exchange contracts get contracts applicable to series From the code author point of view Why we review code We spend time reviewing each other code so that we can: Build a better product, by letting other people look for bugs Propagate knowledge of the code base through the team Learn from each other PR checklist From Google reviewer checklist : In asking (and doing) a code review, you should make sure that: The code is well-designed. The functionality is good for the users of the code. The code isn't more complex than it needs to be. The developer isn't implementing things they might need in the future but don't know they need now. Code has appropriate unit tests. Tests are well-designed. The developer used clear names for everything. Comments are clear and useful, and mostly explain why instead of what. Code is appropriately documented. The code conforms to our style guides. The golden rule of code review Make life easy for the reviewers Aka \"Do not upset the reviewers, otherwise they won't let you merge your code\" Remember that reviewing other people's code is hard and unrewarding work Do your best for not frustrating the reviewers If you are in doubt \"it's probably clear, although I am not 100% sure\", err on giving more information and answer potential questions Be clear in the PR request about what you want Summarize what was done in the PR Refer to the GH task, but the task alone might not be sufficient A PR can implement only part of a complex task Which part is it implementing? Why is it doing it in a certain way? If the code is not ready for merge, but you want a \"pre-review\" convert PR to a draft E.g., ask for an architectural review Draft PRs can not be merged Is it blocking? Do not abuse asking for a quick review All code is important and we do our best to review code quickly and carefully If it\\'s blocking a ping on IM is a good idea Do not mix changes and refactoring / shuffling code The job of the reviewers become frustrating when the author mixes: Refactoring / moving code; and Changes It is time consuming or impossible for a reviewer to understand what happened: What is exactly changed? What was moved where? In those cases reviewers have the right to ask the PR to be broken in pieces One approach for the PR author is to: Do a quick PR to move code around (e.g., refactoring) or purely cosmetic You can ask the reviewer to take a quick look Do the next PRs with the actual changes Another approach is to develop in a branch and break the code into PRs as the code firms up In this case you need to be very organized and be fluent in using Git: both qualities are expected of you E.g., develop in a branch (e.g., gp_scratch ) Create a branch from it (e.g., TaskXYZ_do_this_and_that ) or copy the files from gp_scratch to TaskXYZ_do_this_and_that Edit the files to make the PR self-consistent Do a PR for TaskXYZ_do_this_and_that Keep working in gp_scratch while the review is moving forward Make changes to the TaskXYZ_do_this_and_that as requested Merge TaskXYZ_do_this_and_that to master Merge master back into gp_scratch and keep moving Double check before sending a PR After creating a PR take a look at it to make sure things look good, e.g., Are there merge problems? Did you forget some file? Skim through the PR to make sure that people can understand what you changed Reviewing other people's code is usually not fun Reviewing code is time-consuming and tedious So do everything you can to make the reviewer's job easier Don't cut corners If a reviewer is confused about something, other readers (including you in 1 year) likely would be too What is obvious to you as the author is often not obvious to readers Readability is paramount You should abhor write-only code The first reviews are painful One needs to work on the same code over and over Just think about the fact that the reviewer is also reading (still crappy) code over and over Unfortunately it is needed pain to get to the quality of code we need to make progress as a team Apply review comments everywhere Apply a review comment everywhere, not just where the reviewer pointed out the issue E.g., reviewer says: \"Please replace _LOG.warning(\"Hello %s\".format(name)) with _LOG.warning(\"Hello %s\", name) \" You are expected to do this replacement: In the current review In all future code you write In old code, as you come across it in the course of your work Of course don't start modifying the old code in this review, but open a clean-up bug, if you need a reminder Look at the code top-to-bottom E.g., if you do a search & replace, make sure everything is fine Answering comments after a review It's better to answer comments in chunks so we don't get an email per comment Use \"start a review\" (not in conversation) If one of the comment is urgent (e.g., other comments depend on this) you can send it as single comment When you answer a comment, mark it as resolved Apply changes to a review quickly In the same way the reviewers are expected to review PRs within 24 hours, the author of a PR is expected to apply the requested changes quickly, ideally in few hours If it takes longer, then either the PR was too big or the quality of the PR was too low If it takes too long to apply the changes: The reviewers (and the authors) might forget what is the context of the requested changes It becomes more difficult (or even impossible) to merge, since the code base is continuously changing It creates dependencies among your PRs Remember that you should not be adding more code to the same PR, but only fix the problems and then open a PR with new code Other people that rely on your code are blocked Ask for another review Once you are done with resolving all the comments ask for another review Workflow of a review in terms of GH labels The current meaning of the labels are: See GitHub ZenHub workflows doc Link PR to GH issue Mention the corresponding issue in the PR description to ease the navigation E.g., see an example Fix later It's ok for an author to file a follow up Issue (e.g., with a clean up), by pointing the new Issue to the comments to address, and move on with merge The Issue needs to be addressed immediately after From the code reviewer point of view Post-commit review You can comment on a PR already merged You can comment on the relevant lines in a commit straight to master (this is the exception) Code walk-through It is best to create a branch with the files you want to review Add TODOs in the code (so that the PR will pick up those sections) File bugs for the more involved changes Try to get a top to bottom review of a component once every N weeks (N = 2, 3) Sometimes the structure of the Close the PR and delete the branch When code is merged into master by one of the reviewers through the UI one can select the delete branch option Otherwise you can delete the branch using the procedure in Git Give priority to code review We target to give feedback on a PR within 24hr so that the author is not blocked for too long Usually we respond in few hours Multiple reviewers problem When there are multiple reviewers for the same PR there can be some problem Ok to keep moving fast and avoid blocking Block only if it is controversial Merge when we are confident that the other is ok The other can catch up with post-commit review A good approach is to monitor recently merged PRs in GH to catch up Remember \"small steps ahead\" Follow the Google approach of merging a PR that is a strict improvement. Nothing is too small Each reviewer reviews the code pointing out everything that can be a problem Problems are highlighted even if small or controversial Not all of those comments might not be implemented by the author Of course if different approaches are really equivalent but reviewers have their own stylistic preference, this should not be pointed, unless it's a matter of consistency or leave the choice to the author Final GH comment Once you are done with the detailed review of the code, you need to Write a short comment Decide what is the next step for the PR, e.g., Comment Submit general feedback without explicit approval Approve Submit feedback and approve merging these changes Request changes Submit feedback that must be addressed before merging We use an integrator / developer manager workflow, initially with Paul and GP testing and merging most of the PRs We use the 3 possible options in the following way: Comment When reviewers want the changes to be applies and then look at the resulting changes to decide the next steps In practice this means \"make the changes and then we'll discuss more\" E.g., this is of course the right choice for a pre-PR Approve No more changes: time to merge! Often it is accompanied with the comment \"LGMT\" (Looks Good To Me) Request changes This typically means \"if you address the comments we can merge\" In practice this is more or less equivalent to \"Comment\"","title":"Code review"},{"location":"Code_review/#code-review","text":"General rules about code review Read the Google code review best practices Code review workflows Pull request From the code author point of view Why we review code PR checklist The golden rule of code review Be clear in the PR request about what you want Do not mix changes and refactoring / shuffling code Double check before sending a PR Reviewing other people's code is usually not fun The first reviews are painful Apply review comments everywhere Look at the code top-to-bottom Answering comments after a review Apply changes to a review quickly Ask for another review Workflow of a review in terms of GH labels Link PR to GH issue Fix later From the code reviewer point of view Post-commit review Code walk-through Close the PR and delete the branch Give priority to code review Multiple reviewers problem Remember \"small steps ahead\" Nothing is too small Final GH comment","title":"Code review"},{"location":"Code_review/#general-rules-about-code-review","text":"","title":"General rules about code review"},{"location":"Code_review/#read-the-google-code-review-best-practices","text":"From the developer\\'s perspective From the reviewer\\'s perspective Where the Google guide says \"CL\", think \"PR\" Read it (several times, if you need to) Think about it Understand the rationale","title":"Read the Google code review best practices"},{"location":"Code_review/#code-review-workflows","text":"","title":"Code review workflows"},{"location":"Code_review/#pull-request","text":"Our usual review process is to work in a branch and create a pull request See the Git notes for details The name of the pull request is generated with ghi_show.py and looks like PTask2704 make exchange contracts get contracts applicable to series","title":"Pull request"},{"location":"Code_review/#from-the-code-author-point-of-view","text":"","title":"From the code author point of view"},{"location":"Code_review/#why-we-review-code","text":"We spend time reviewing each other code so that we can: Build a better product, by letting other people look for bugs Propagate knowledge of the code base through the team Learn from each other","title":"Why we review code"},{"location":"Code_review/#pr-checklist","text":"From Google reviewer checklist : In asking (and doing) a code review, you should make sure that: The code is well-designed. The functionality is good for the users of the code. The code isn't more complex than it needs to be. The developer isn't implementing things they might need in the future but don't know they need now. Code has appropriate unit tests. Tests are well-designed. The developer used clear names for everything. Comments are clear and useful, and mostly explain why instead of what. Code is appropriately documented. The code conforms to our style guides.","title":"PR checklist"},{"location":"Code_review/#the-golden-rule-of-code-review","text":"Make life easy for the reviewers Aka \"Do not upset the reviewers, otherwise they won't let you merge your code\" Remember that reviewing other people's code is hard and unrewarding work Do your best for not frustrating the reviewers If you are in doubt \"it's probably clear, although I am not 100% sure\", err on giving more information and answer potential questions","title":"The golden rule of code review"},{"location":"Code_review/#be-clear-in-the-pr-request-about-what-you-want","text":"Summarize what was done in the PR Refer to the GH task, but the task alone might not be sufficient A PR can implement only part of a complex task Which part is it implementing? Why is it doing it in a certain way? If the code is not ready for merge, but you want a \"pre-review\" convert PR to a draft E.g., ask for an architectural review Draft PRs can not be merged Is it blocking? Do not abuse asking for a quick review All code is important and we do our best to review code quickly and carefully If it\\'s blocking a ping on IM is a good idea","title":"Be clear in the PR request about what you want"},{"location":"Code_review/#do-not-mix-changes-and-refactoring-shuffling-code","text":"The job of the reviewers become frustrating when the author mixes: Refactoring / moving code; and Changes It is time consuming or impossible for a reviewer to understand what happened: What is exactly changed? What was moved where? In those cases reviewers have the right to ask the PR to be broken in pieces One approach for the PR author is to: Do a quick PR to move code around (e.g., refactoring) or purely cosmetic You can ask the reviewer to take a quick look Do the next PRs with the actual changes Another approach is to develop in a branch and break the code into PRs as the code firms up In this case you need to be very organized and be fluent in using Git: both qualities are expected of you E.g., develop in a branch (e.g., gp_scratch ) Create a branch from it (e.g., TaskXYZ_do_this_and_that ) or copy the files from gp_scratch to TaskXYZ_do_this_and_that Edit the files to make the PR self-consistent Do a PR for TaskXYZ_do_this_and_that Keep working in gp_scratch while the review is moving forward Make changes to the TaskXYZ_do_this_and_that as requested Merge TaskXYZ_do_this_and_that to master Merge master back into gp_scratch and keep moving","title":"Do not mix changes and refactoring / shuffling code"},{"location":"Code_review/#double-check-before-sending-a-pr","text":"After creating a PR take a look at it to make sure things look good, e.g., Are there merge problems? Did you forget some file? Skim through the PR to make sure that people can understand what you changed","title":"Double check before sending a PR"},{"location":"Code_review/#reviewing-other-peoples-code-is-usually-not-fun","text":"Reviewing code is time-consuming and tedious So do everything you can to make the reviewer's job easier Don't cut corners If a reviewer is confused about something, other readers (including you in 1 year) likely would be too What is obvious to you as the author is often not obvious to readers Readability is paramount You should abhor write-only code","title":"Reviewing other people's code is usually not fun"},{"location":"Code_review/#the-first-reviews-are-painful","text":"One needs to work on the same code over and over Just think about the fact that the reviewer is also reading (still crappy) code over and over Unfortunately it is needed pain to get to the quality of code we need to make progress as a team","title":"The first reviews are painful"},{"location":"Code_review/#apply-review-comments-everywhere","text":"Apply a review comment everywhere, not just where the reviewer pointed out the issue E.g., reviewer says: \"Please replace _LOG.warning(\"Hello %s\".format(name)) with _LOG.warning(\"Hello %s\", name) \" You are expected to do this replacement: In the current review In all future code you write In old code, as you come across it in the course of your work Of course don't start modifying the old code in this review, but open a clean-up bug, if you need a reminder","title":"Apply review comments everywhere"},{"location":"Code_review/#look-at-the-code-top-to-bottom","text":"E.g., if you do a search & replace, make sure everything is fine","title":"Look at the code top-to-bottom"},{"location":"Code_review/#answering-comments-after-a-review","text":"It's better to answer comments in chunks so we don't get an email per comment Use \"start a review\" (not in conversation) If one of the comment is urgent (e.g., other comments depend on this) you can send it as single comment When you answer a comment, mark it as resolved","title":"Answering comments after a review"},{"location":"Code_review/#apply-changes-to-a-review-quickly","text":"In the same way the reviewers are expected to review PRs within 24 hours, the author of a PR is expected to apply the requested changes quickly, ideally in few hours If it takes longer, then either the PR was too big or the quality of the PR was too low If it takes too long to apply the changes: The reviewers (and the authors) might forget what is the context of the requested changes It becomes more difficult (or even impossible) to merge, since the code base is continuously changing It creates dependencies among your PRs Remember that you should not be adding more code to the same PR, but only fix the problems and then open a PR with new code Other people that rely on your code are blocked","title":"Apply changes to a review quickly"},{"location":"Code_review/#ask-for-another-review","text":"Once you are done with resolving all the comments ask for another review","title":"Ask for another review"},{"location":"Code_review/#workflow-of-a-review-in-terms-of-gh-labels","text":"The current meaning of the labels are: See GitHub ZenHub workflows doc","title":"Workflow of a review in terms of GH labels"},{"location":"Code_review/#link-pr-to-gh-issue","text":"Mention the corresponding issue in the PR description to ease the navigation E.g., see an example","title":"Link PR to GH issue"},{"location":"Code_review/#fix-later","text":"It's ok for an author to file a follow up Issue (e.g., with a clean up), by pointing the new Issue to the comments to address, and move on with merge The Issue needs to be addressed immediately after","title":"Fix later"},{"location":"Code_review/#from-the-code-reviewer-point-of-view","text":"","title":"From the code reviewer point of view"},{"location":"Code_review/#post-commit-review","text":"You can comment on a PR already merged You can comment on the relevant lines in a commit straight to master (this is the exception)","title":"Post-commit review"},{"location":"Code_review/#code-walk-through","text":"It is best to create a branch with the files you want to review Add TODOs in the code (so that the PR will pick up those sections) File bugs for the more involved changes Try to get a top to bottom review of a component once every N weeks (N = 2, 3) Sometimes the structure of the","title":"Code walk-through"},{"location":"Code_review/#close-the-pr-and-delete-the-branch","text":"When code is merged into master by one of the reviewers through the UI one can select the delete branch option Otherwise you can delete the branch using the procedure in Git","title":"Close the PR and delete the branch"},{"location":"Code_review/#give-priority-to-code-review","text":"We target to give feedback on a PR within 24hr so that the author is not blocked for too long Usually we respond in few hours","title":"Give priority to code review"},{"location":"Code_review/#multiple-reviewers-problem","text":"When there are multiple reviewers for the same PR there can be some problem Ok to keep moving fast and avoid blocking Block only if it is controversial Merge when we are confident that the other is ok The other can catch up with post-commit review A good approach is to monitor recently merged PRs in GH to catch up","title":"Multiple reviewers problem"},{"location":"Code_review/#remember-small-steps-ahead","text":"Follow the Google approach of merging a PR that is a strict improvement.","title":"Remember \"small steps ahead\""},{"location":"Code_review/#nothing-is-too-small","text":"Each reviewer reviews the code pointing out everything that can be a problem Problems are highlighted even if small or controversial Not all of those comments might not be implemented by the author Of course if different approaches are really equivalent but reviewers have their own stylistic preference, this should not be pointed, unless it's a matter of consistency or leave the choice to the author","title":"Nothing is too small"},{"location":"Code_review/#final-gh-comment","text":"Once you are done with the detailed review of the code, you need to Write a short comment Decide what is the next step for the PR, e.g., Comment Submit general feedback without explicit approval Approve Submit feedback and approve merging these changes Request changes Submit feedback that must be addressed before merging We use an integrator / developer manager workflow, initially with Paul and GP testing and merging most of the PRs We use the 3 possible options in the following way: Comment When reviewers want the changes to be applies and then look at the resulting changes to decide the next steps In practice this means \"make the changes and then we'll discuss more\" E.g., this is of course the right choice for a pre-PR Approve No more changes: time to merge! Often it is accompanied with the comment \"LGMT\" (Looks Good To Me) Request changes This typically means \"if you address the comments we can merge\" In practice this is more or less equivalent to \"Comment\"","title":"Final GH comment"},{"location":"Codebase_clean_up_scripts/","text":"Codebase clean-up scripts Using the script approach Rationale for the script approach Notes on replace_text.py Usage examples Instructions for the PR author Example Instructions for the subrepo integrator Using the script approach We want to apply clean-up changes to the code base with a script Ideally we would like to apply all the changes automatically through the script E.g.: in SorrTask258 a script that replaces pytest.raises with self.assertRaises everywhere in the code We are ok to make the vast majority (like 95%) of the changes automatically, and the rest manually We want to keep together in a single PR the script performing automatically the changes and the manual changes from the outcome of the script We don't want to check in the PR, the outcome of running the script We can create a separate PR to communicate with the reviewers the output of running the script The reviewer should run the script on all the repos, run the unit tests, and merge (through a PR as usual) Typically we use replace_text.py and wrap it up in a sh script For more complex problems we write a custom regex-based script Rationale for the script approach Since we have multiple repos, we can't always easily replace code in one repo (e.g., with PyCharm) and have all the other repos work properly So we create a script that applies the correct changes to all the repos When developing a change for the entire repo, we want to be able to \"reset\" the work and apply the change from scratch Often during the review of the PR: the code in master might be changing creating conflicts the reviewers might ask some changes Using the non-script approach, this creates a lot of manual changes. Instead with the script approach we can check out master, run the script to apply the changes automatically, regress and merge Notes on replace_text.py Replace an instance of text in all: .py file contents .ipynb file contents .txt file contents filenames --old : regular expression or string that should be replaced with --new --new : regular expression or string that should replace --old --preview : see script result without making actual changes --only_dirs : provide space-separated list of directories to process only --only_files : provide space-separated list of files to process only --exclude_files : provide space-separated list of files to exclude from replacements --exclude_dirs : provide space-separated list of dir to exclude from replacements --ext : process files with specified extensions defaults are py, ipynb, txt, sh use _all_ for all files Usage examples See SorrIssue259 and the related PR for reference We wanted to make _to_multiline_cmd() from helpers/lib_tasks_utils.py a public function This would require to rename _to_multiline_cmd() to to_multiline_cmd() with the script This script will make the replacement smoothly everywhere in the code except for the dirs specified --exclude_dirs flag. See SorrIssue258 and the related PR for reference We wanted to replace pytest.raises with self.assertRaises This script will replace it everywhere in the code Note the use of --ext flag to specify the file extentions the script should work on Of course the changes need to be applied in one repo and then propagated to all the other repos if the tests are successful Instructions for the PR author Create a local branch called ...TaskXYZ_..._script containing: The code that needs to be changed manually E.g.: Replacing pytest.raises with self.assertRaises More contextual changes E.g.: Adding unit tests to the new functions The script for the replacement of the caller named after the GH issue The script should: Prepare the target Git client Merge this script branch with the manual changes Make the automated changes E.g.: Rename a function or replace certain word in comments / docstring Notes in the files that need to be changed manually after the automatic script Run from scratch the script getting the regression to pass Any time there is a change needed by hand, the change should be added to the script branch The goal is to be able to run the script File a PR of the ...TaskXYZ_..._script (Optional) Create a PR with the result of the script The author can request a review on this PR, but still the goal is to automate as much as possible Finally the PR author merges the PR with the results of the script Example The name of script should be related to the task. E.g: SorrTask259_Make_to_multi_line_cmd_public.sh The script should have a system call to replace_text.py to execute the required functionality as provided in the above examples Create a PR only with the script and the changes Instructions for the subrepo integrator Do a git checkout of the ...TaskXYZ_..._script Run the script Review carefully the changes to make sure we are not screwing things up Run the regressions Merge the resulting ...TaskXYZ... PR Ensure that the ...TaskXYZ_..._script is merged in master Delete the ...TaskXYZ_..._script","title":"Codebase clean-up scripts"},{"location":"Codebase_clean_up_scripts/#codebase-clean-up-scripts","text":"Using the script approach Rationale for the script approach Notes on replace_text.py Usage examples Instructions for the PR author Example Instructions for the subrepo integrator","title":"Codebase clean-up scripts"},{"location":"Codebase_clean_up_scripts/#using-the-script-approach","text":"We want to apply clean-up changes to the code base with a script Ideally we would like to apply all the changes automatically through the script E.g.: in SorrTask258 a script that replaces pytest.raises with self.assertRaises everywhere in the code We are ok to make the vast majority (like 95%) of the changes automatically, and the rest manually We want to keep together in a single PR the script performing automatically the changes and the manual changes from the outcome of the script We don't want to check in the PR, the outcome of running the script We can create a separate PR to communicate with the reviewers the output of running the script The reviewer should run the script on all the repos, run the unit tests, and merge (through a PR as usual) Typically we use replace_text.py and wrap it up in a sh script For more complex problems we write a custom regex-based script","title":"Using the script approach"},{"location":"Codebase_clean_up_scripts/#rationale-for-the-script-approach","text":"Since we have multiple repos, we can't always easily replace code in one repo (e.g., with PyCharm) and have all the other repos work properly So we create a script that applies the correct changes to all the repos When developing a change for the entire repo, we want to be able to \"reset\" the work and apply the change from scratch Often during the review of the PR: the code in master might be changing creating conflicts the reviewers might ask some changes Using the non-script approach, this creates a lot of manual changes. Instead with the script approach we can check out master, run the script to apply the changes automatically, regress and merge","title":"Rationale for the script approach"},{"location":"Codebase_clean_up_scripts/#notes-on-replace_textpy","text":"Replace an instance of text in all: .py file contents .ipynb file contents .txt file contents filenames --old : regular expression or string that should be replaced with --new --new : regular expression or string that should replace --old --preview : see script result without making actual changes --only_dirs : provide space-separated list of directories to process only --only_files : provide space-separated list of files to process only --exclude_files : provide space-separated list of files to exclude from replacements --exclude_dirs : provide space-separated list of dir to exclude from replacements --ext : process files with specified extensions defaults are py, ipynb, txt, sh use _all_ for all files","title":"Notes on replace_text.py"},{"location":"Codebase_clean_up_scripts/#usage-examples","text":"See SorrIssue259 and the related PR for reference We wanted to make _to_multiline_cmd() from helpers/lib_tasks_utils.py a public function This would require to rename _to_multiline_cmd() to to_multiline_cmd() with the script This script will make the replacement smoothly everywhere in the code except for the dirs specified --exclude_dirs flag. See SorrIssue258 and the related PR for reference We wanted to replace pytest.raises with self.assertRaises This script will replace it everywhere in the code Note the use of --ext flag to specify the file extentions the script should work on Of course the changes need to be applied in one repo and then propagated to all the other repos if the tests are successful","title":"Usage examples"},{"location":"Codebase_clean_up_scripts/#instructions-for-the-pr-author","text":"Create a local branch called ...TaskXYZ_..._script containing: The code that needs to be changed manually E.g.: Replacing pytest.raises with self.assertRaises More contextual changes E.g.: Adding unit tests to the new functions The script for the replacement of the caller named after the GH issue The script should: Prepare the target Git client Merge this script branch with the manual changes Make the automated changes E.g.: Rename a function or replace certain word in comments / docstring Notes in the files that need to be changed manually after the automatic script Run from scratch the script getting the regression to pass Any time there is a change needed by hand, the change should be added to the script branch The goal is to be able to run the script File a PR of the ...TaskXYZ_..._script (Optional) Create a PR with the result of the script The author can request a review on this PR, but still the goal is to automate as much as possible Finally the PR author merges the PR with the results of the script","title":"Instructions for the PR author"},{"location":"Codebase_clean_up_scripts/#example","text":"The name of script should be related to the task. E.g: SorrTask259_Make_to_multi_line_cmd_public.sh The script should have a system call to replace_text.py to execute the required functionality as provided in the above examples Create a PR only with the script and the changes","title":"Example"},{"location":"Codebase_clean_up_scripts/#instructions-for-the-subrepo-integrator","text":"Do a git checkout of the ...TaskXYZ_..._script Run the script Review carefully the changes to make sure we are not screwing things up Run the regressions Merge the resulting ...TaskXYZ... PR Ensure that the ...TaskXYZ_..._script is merged in master Delete the ...TaskXYZ_..._script","title":"Instructions for the subrepo integrator"},{"location":"Coding_Style_Guide/","text":"Sorrentum - Python Style Guide Meta Disclaimer References High-Level Principles Follow the DRY principle The writer is the reader Encapsulate what changes Least surprise principle Pay the technical debt End-to-end first Unit test everything Don't get attached to code Always plan before writing code Think hard about naming Look for inconsistencies No ugly hacks Our coding suggestions Being careful with naming Follow the conventions Follow spelling rules Search good names, avoid bad names General naming rules Do not be stingy Do not abbreviate just to save characters When to use abbreviations Avoid code stutter Comments and docstrings General conventions Descriptive vs imperative style Docstrings style Comments style Replace empty lines in code with comments Referring to an object in code comments Avoid distracting comments Commenting out code Use type hints Interval notation If you find a bug or obsolete docstring/TODO in the code Linter Remove linter messages When to disable linter messages Prefer non-inlined linter comments Don't mix real changes with linter changes Logging Always use logging instead of prints Our logging idiom Logging level Use positional args when logging Exceptions don't allow positional args Report warnings Assertions Validate values before an assignment Encode the assumptions using assertions Use positional args when asserting Report as much information as possible in an assertion Imports Don't use evil import * Cleaning up the evil import * Avoid from ... import ... Exceptions to the import style Always import with a full path from the root of the repo / submodule Baptizing module import Examples of imports Scripts Use Python and not bash for scripting Skeleton for a script Some useful patterns Use scripts and not notebooks for long-running jobs Follow the same structure Use clear names for the scripts Functions Avoid using non-exclusive bool arguments Try to make functions work on multiple types Avoid hard-wired column name dependencies Single exit point from a function Order of function parameters Problem Decision Consistency of ordering of function parameters Style for default parameter Problem Decision Rationale Calling functions with default parameters Problem Decision Rationale Don't repeat non-default parameters Problem Decision Rationale Writing clear beautiful code Keep related code close Order functions in topological order Distinguish public and private functions Keep public functions organized in a logical order Do not make tiny wrappers Regex Do not introduce another \u201cconcept\u201d unless really needed Return None or keep one type Avoid wall-of-text functions Writing robust code Don\u2019t let your functions catch the default-itis Explicitly bind default parameters Don\u2019t hardwire params in a function call Make if-elif-else complete Add TODOs when needed Common Python mistakes == vs is type() vs isinstance() Unit tests Convention for naming tests Refactoring When moving / refactoring code Write script for renamings Architectural and design pattern Research quality vs production quality Always separate what changes from what stays the same Organize scripts as pipelines Make filename unique Incremental behavior Run end-to-end Think about scalability Use command line for reproducibility Structure the code in terms of filters Code style for different languages SQL Conventions (Addendum) Be patient Goal Keep the rules simple Allow turning off the automatic tools Make the spell-checker happy Meta What we call the \"rules\" is actually just a convention Not about the absolute best way of doing something in all cases Optimized for the common case Can become cumbersome or weird to follow for some corner cases We prefer simple rather than optimal rules that can be applied in most of the cases without thinking or going to check the documentation The rules are striving to achieve consistency and robustness E.g., see \"tab vs space\" flame-war from the 90s We care about consistency rather than arguing about which approach is better in each case The rules are optimized for the average developer / data scientist and not for power users The rules try to minimize the maintenance burden We don't want a change somewhere to propagate everywhere We want to minimize the propagation of a change Some of the rules are evolving based on what we are seeing through the reviews Disclaimer This document was forked from Google Python Style Guide , therefore, the numbering of chapters sets off where the Style Guide ends. Make sure to familiarize yourself with it before proceeding to the rest of the doc, since it is the basis of our team\u2019s code style. Another important source is The Pragmatic Programmer by David Thomas and Andrew Hunt. While not Python-specific, it provides an invaluable set of general principles by which any person working with code (software developer, DevOps or data scientist) should abide. Read it on long commutes, during lunch, and treat yourself to a physical copy on Christmas. The book is summarized here , but do not deprive yourself of the engaging manner in which Thomas & Hunt elaborate on these points -- on top of it all, it is a very, very enjoyable read. References Coding Google Python Style Guide (GPSG) Code convention from PEP8 Documentation Docstring convention from PEP257 Google documentation best practices Commenting style Sphinx Sphinx tutorial Design Google philosophical stuff Unix rules (although a bit cryptic sometimes) High-Level Principles In this paragraph we summarize the high-level principles that we follow for designing and implementing code and research. We should be careful in adding principles here. Ideally principles should be non-overlapping and generating all the other lower level principles we follow (like a basis for a vector space) Follow the DRY principle The writer is the reader Make code easy to read even if it is more difficult to write Code is written 1x and read 100x Remember that even if things are perfectly clear now to the person that wrote the code, in a couple of months the code will look foreign to whoever wrote the code. So make your future-self's life easier by following the conventions and erring on the side of documenting for the reader. Encapsulate what changes Separate what changes from what stays the same Least surprise principle Try to make sure that the reader is not surprised Pay the technical debt Any unpaid debt is guaranteed to bite you when you don't expect it Still some debt is inevitable: try to find the right trade-off End-to-end first Always focus on implementing things end-to-end, then improve each block Remember the analogy of building the car through the skateboard, the bike, etc. Compare this approach to building wheels, chassis, with a big-bang integration at the end Unit test everything Code that matters needs to be unit tested Code that doesn't matter should not be checked in the repo The logical implication is: all code checked in the repo should be unit tested Don't get attached to code It's ok to delete, discard, retire code that is not useful any more Don't take it personally when people suggest changes or simplification Always plan before writing code File a GitHub issue Think about what to do and how to do it Ask for help or for a review The best code is the one that we avoid to write through a clever mental kung-fu move Think hard about naming Finding a name for a code object, notebook, is extremely difficult but very important to build a mental map Spend the needed time on it Look for inconsistencies Stop for a second after you have, before sending out: Implemented code or a notebook Written documentation Written an e-mail ... Reset your mind and look at everything with fresh eyes like if it was the first time you saw it Does everything make sense to someone that sees this for the first time? Can (and should) it be improved? Do you see inconsistencies, potential issues? It will take less and less time to become good at this No ugly hacks We don't tolerate \"ugly hacks\", i.e., hacks that require lots of work to be undone (much more than the effort to do it right in the first place) Especially an ugly design hack, e.g., a Singleton, or some unnecessary dependency between distant pieces of code Ugly hacks spreads everywhere in the code base Our coding suggestions Being careful with naming Follow the conventions Name executable files (scripts) and library functions using verbs (e.g., download.py , download_data() ) Name classes and (non-executable) files using nouns (e.g., Downloader() , downloader.py ) For decorators we don't use a verb as we do for normal functions, but rather an adjective or a past tense verb, e.g., python def timed(f): \"\"\" Add a timer decorator around a specified function. \"\"\" \u2026 Follow spelling rules We spell commands in lower-case, and programs with initial upper case: \"Git\" (as program), \"git\" (as the command) We distinguish \"research\" (not \"default\", \"base\") vs \"production\" We use different names for indicating the same concept, e.g., dir , path , folder Preferred term is dir Name of columns The name of columns should be ..._col and not ..._col_name or _column Timestamp We spell timestamp , we do not abbreviate it as ts We prefer timestamp to datetime E.g., start_timestamp instead of start_datetime Abbreviations JSON, CSV, DB, etc., are abbreviations and thus should be capitalized in comments and docstrings, and treated as abbreviations in code when it doesn't conflict with other rules E.g., convert_to_CSV , but csv_file_name as a variable name that is not global Profit-and-loss: PnL instead of pnl or PNL Search good names, avoid bad names General naming rules Naming things properly is one of the most difficult task of a programmer / data scientist The name needs to be (possibly) short and memorable However, don't be afraid to use long names, if needed, e.g., process_text_with_full_pipeline_twitter_v1 \u0421larity is more important than number of bytes used The name should capture what the object represents, without reference to things that can change or to details that are not important The name should refer to what objects do (i.e., mechanisms), rather than how we use them (i.e., policies) The name needs to be non-controversial: people need to be able to map the name in their mental model The name needs to sound good in English Bad : AdapterSequential sounds bad Good : SequentialAdapter sounds good Some examples of how NOT to do naming: raw_df is a terrible name \"raw\" with respect to what? Cooked? Read-After-Write race condition? person_dict is bad What if we switch from a dictionary to an object? Then we need to change the name everywhere! The name should capture what the data structure represents (its semantics) and not how it is implemented Do not be stingy Why calling an object TimeSeriesMinStudy instead of TimeSeriesMinuteStudy ? Saving 3 letters is not worth The reader might interpret Min as Minimal (or Miniature , Minnie , Minotaur ) If you don't like to type, we suggest you get a better keyboard, e.g., this Do not abbreviate just to save characters Abbreviations just to save space are rarely beneficial to the reader. E.g., Fwd (forward) Bwd (backward) Act (actual) Exp (expected) When to use abbreviations We could relax this rule for short lived functions and variables in order to save some visual noise. Sometimes an abbreviation is so short and common that it's ok to leave it E.g., Df (dataframe) Srs (series) Idx (index) Id (identifier) Val (value) Var (variable) Args (arguments) Kwargs (keyword arguments) Col (column) Vol (volatility) while volume is always spelled out Avoid code stutter An example of code stutter: you want to add a function that returns git root path in a module git Bad Name is get_git_root_path() ```python import helpers.git as git ... git.get_git_root_path() ``` You see that the module is already specifying we are talking about Git Good Name is get_root_path() ```python import helpers.git as git ... git.get_root_path() ``` This is not only aesthetic reason but a bit related to a weak form of DRY Comments and docstrings General conventions Code needs to be properly commented We follow python standard PEP 257 for commenting PEP 257 standardizes what comments should express and how they should do it (e.g., use triple quotes for commenting a function), but does not specify what markup syntax should be used to describe comments Different conventions have been developed for documenting interfaces ReST Google (which is cross-language, e.g., C++, python, ...) Epytext Numpydoc Descriptive vs imperative style We decided to use imperative style for our comments and docstrings Pylint and other python QA tools favor an imperative style From PEP 257 The docstring is a phrase ending in a period. It prescribes the function or method's effect as a command (\"Do this\", \"Return that\"), not as a description; e.g. don't write \"Returns the pathname ...\". Docstrings style We follow ReST (aka re-Structured Text) style for docstrings which is: The most widely supported in the python community Supported by all doc generation tools (e.g., epydoc, sphinx) Default in Pycharm Default in pyment Supported by pydocstyle (which does not support Google style as explained here ) Example of a function definition with ReST styled docstring: ```python def my_function(param1: str) -> str: \"\"\" A one-line description of what the function does. A longer description (possibly on multiple lines) with a more detailed explanation of what the function does, trying to not be redundant with the parameter / return description below. The focus is on the interface and what the user should know to use the function and not how the function is implemented. :param param1: this is a first param :return: this is a description of what is returned \"\"\" ``` We pick lowercase after :param XYZ: ... unless the first word is a proper noun or type A full ReST docstring styling also requires to specify params and return types, however type hinting makes it redundant so you should use only type hinting Put docstrings in triple quotation marks Bad python Generate \"random returns\". Good python \"\"\" Generate \"random returns\". \"\"\" Sometimes functions are small enough so we just use a 1-liner docstring without detailed params and return descriptions. Just do not put text and docstring brackets in one line Bad python \"\"\"This is not our approach.\"\"\" Good python \"\"\" This is our approach. \"\"\" More examples of and discussions on python docstrings Comments style Comments follow the same style of docstrings, e.g., imperative style with period . at the end Bad python # This comment is not imperative and has no period at the end Good python # Make comments imperative and end them with a period. Always place comments above the lines that they are referring to. Avoid writing comments on the same line as code since they require extra maintenance (e.g., when the line becomes too long) Bad python print(\"hello world\") # Introduce yourself. Good python # Introduce yourself. print(\"hello world\") The only exception is commenting if-elif-else statments: we comment them underneath the each statement in order to explain the code that belongs to the each statement particularly Bad python # Set remapping based on the run type. if is_prod: ... else: ... Good python # if is_prod: # Set remapping for database data used in production. ... else: # Set remapping for file system data used in simulation. ... If you want to separate an if statement from a bunch of code preceeding it, you can leave an empty comment before it Replace empty lines in code with comments If you feel that you need an empty line in the code, it probably means that a specific chunk of code is a logical piece of code performing a cohesive function ```python ... end_y = end_dt.year paths = list() ... - Instead of putting an empty line, you should put a comment describing at high level what the code does. python ... end_y = end_dt.year # Generate a list of file paths for Parquet dataset. paths = list() ... - If you don't want to add comments, just comment the empty line. python ... end_y = end_dt.year # paths = list() ... ``` - The problem with empty lines is that they are visually confusing since one empty line is used also to separate functions. For this reason we suggest using an empty comment Referring to an object in code comments In general, avoid this whenever possible Code object names (e.g., function, class, params) are often subject to change, so we need to take care of them everywhere. It is very hard to track all of them in comments so replace the names with their actual meaning Bad python # Generate a list of file paths for `ParquetDataset`. Good python # Generate a list of file paths for Parquet dataset. However, sometimes it is necessary. In this case refer to objects in the code using Markdown. This is useful for distinguishing the object code from the real-life object Bad python # The dataframe df_tmp is used for ... Good python # The dataframe `df_tmp` is used for ... Avoid distracting comments Use comments to explain the high level logic / goal of a piece of code and not the details, e.g., do not comment things that are obvious Bad python # Print results. _LOG.info(\"Results are %s\", ...) Commenting out code When we comment out code, we should explain why it is no longer relevant Bad python is_alive = pd.Series(True, index=metadata.index) # is_alive = kgutils.annotate_alive(metadata, self.alive_cutoff) Good python # TODO(*): As discussed in PTask5047 for now we set all timeseries to be alive. # is_alive = kgutils.annotate_alive(metadata, self.alive_cutoff) is_alive = pd.Series(True, index=metadata.index) Use type hints We expect new code to use type hints whenever possible See PEP 484 Type hints cheat sheet At some point we will start adding type hints to old code We plan to start using static analyzers (e.g., mypy ) to check for bugs from type mistakes and to enforce type hints at run-time, whenever possible Interval notation Intervals are represented with [a, b), (a, b], (a, b), [a, b] We don't use the other style [a, b[ If you find a bug or obsolete docstring/TODO in the code The process is: Do a git blame to find who wrote the code If it's an easy bug, you can fix it and ask for a review from the author You can comment on a PR (if there is one) You can file a bug on Github with Clear info on the problem How to reproduce it, ideally a unit test Stacktrace Linter The linter is in charge of reformatting the code according to our conventions and reporting potential problems You can find instructions on how to run linter at the First review process doc Remove linter messages When the linter reports a problem: We assume that linter messages are correct, until the linter is proven wrong We try to understand what is the rationale for the linter's complaints We then change the code to follow the linter's suggestion and remove the lint If you think a message is too pedantic, please file a bug with the example and as a team we will consider whether to exclude that message from our list of linter suggestions If you think the message is a false positive, then try to change the code to make the linter happy E.g., if the code depends on some run-time behavior that the linter can't infer, then you should question whether that behavior is really needed A human reader would probably be as confused as the linter is When to disable linter messages If you really believe you should override the linter in this particular case, then use something like: python # pylint: disable=some-message,another-one You then need to explain in a comment why you are overriding the linter. Don't use linter code numbers, but the symbolic name whenever possible: Bad python # pylint: disable=W0611 import config.logging_settings Good python # pylint: disable=unused-import # This is needed when evaluating code at run-time that depends from # this import. import config.logging_settings Prefer non-inlined linter comments As for the general comments, we prefer make linter comments non-inlined However, sometimes there is no other choice than an inlined comment to get the linter to understand which line we are referring to, so in rare cases it is OK: Bad but ok if needed python import config.logging_settings # pylint: disable=unused-import Good python # pylint: disable=line-too-long expected_df_as_str = \"\"\"# df= asset_id last_price start_datetime timestamp_db end_datetime 2000-01-01 09:31:00-05:00 1000 999.874540 2000-01-01 09:30:00-05:00 2000-01-01 09:31:00-05:00 2000-01-01 09:32:00-05:00 1000 1000.325254 2000-01-01 09:31:00-05:00 2000-01-01 09:32:00-05:00 2000-01-01 09:33:00-05:00 1000 1000.557248 2000-01-01 09:32:00-05:00 2000-01-01 09:33:00-05:00\"\"\" # pylint: enable=line-too-long Don't mix real changes with linter changes We don't commit changes that modify the code together with linter reformatting, unless the linting is applied to the changes we just made The reason for not mixing real and linter changes is that for a PR or to just read the code it is difficult to understand what really changed vs what was just a cosmetic modification If you are worried the linter might change your code in a way you don't like, e.g., Screwing up some formatting you care about for some reason, or Suggesting changes that you are worried might introduce bugs you can commit your code and then do a \"lint commit\" with a message \"CMTaskXYZ: Lint\" In this way you have a backup state that you can rollback to, if you want If you run the linter and see that the linter is reformatting / modifying pieces of code you din't change, it means that our team mate forgot to lint their code git blame can figure out the culprit You can send him / her a ping to remind her to lint, so you don't have to clean after him / her In this case, the suggested approach is: Commit your change to a branch / stash Run the linter by itself on the files that need to be cleaned, without any change Run the unit tests to make sure nothing is breaking You can fix lints or just do formatting: it's up to you You can make this change directly on master or do a PR if you want to be extra sure: your call Logging Always use logging instead of prints Always use logging and never print() to monitor the execution Our logging idiom In order to use our logging framework (e.g., -v from command lines, and much more) use: ```python import helpers.hdbg as hdbg _LOG = logging.getLogger( name ) hdbg.init_logger(verbosity=logging.DEBUG) _LOG.debug(\"I am a debug function about %s\", a) ``` In this way one can decide how much debug info is needed (see Unix rule of silence) E.g., when there is a bug one can run with -v DEBUG and see what's happening right before the bug Logging level Use _LOG.warning for messages to the final user related to something unexpected where the code is making a decision that might be controversial E.g., processing a dir that is supposed to contain only .csv files the code finds a non- .csv file and decides to skip it, instead of breaking Use _LOG.info to communicate to the final user, e.g., When the script is started Where the script is saving its results A progress bar indicating the amount of work completed Use _LOG.debug to communicate information related to the internal behavior of code Do not pollute the output with information a regular user does not care about Make sure the script prints when the work is terminated, e.g., \"DONE\" or \"Results written to ...\" This is useful to indicate that the script did not die in the middle: sometimes this happens silently and it is reported only from the OS return code Use positional args when logging Bad python _LOG.debug(\"cmd=%s %s %s\" % (cmd1, cmd2, cmd3)) _LOG.debug(\"cmd=%s %s %s\".format(cmd1, cmd2, cmd3)) _LOG.debug(\"cmd={cmd1} {cmd2} {cmd3}\") Good python _LOG.debug(\"cmd=%s %s %s\", cmd1, cmd2, cmd3) All the statements are equivalent from the functional point of view The reason is that in the second case the string is not built unless the logging is actually performed, which limits time overhead from logging Exceptions don't allow positional args For some reason people tend to believe that using the logging / dassert approach of positional param to exceptions Bad (use positional args) python raise ValueError(\"Invalid server_name='%s'\", server_name) Good (use string interpolation) python raise ValueError(\"Invalid server_name='%s'\" % server_name) Best (use string format) python raise ValueError(f\"Invalid server_name='{server_name}'\") The constructor of an exception accepts a string Using the string f-format is best since It's more readable There is little time overhead since if you get to the exception probably the code is going to terminate, and it's not in a hot loop Report warnings If there is a something that is suspicious but you don't feel like it's worthwhile to assert, report a warning with: python _LOG.warning(...) If you know that if there is a warning then there are going to be many many warnings Print the first warning Send the rest to warnings.log At the end of the run, reports \"there are warnings in warnings.log\" Assertions Validate values before an assignment We consider this as an extension of a pre-condition (\"only assign values that are correct\") rather than a postcondition Often is more compact since it doesn't have reference to self Bad python self._tau = tau hdbg.dassert_lte(self._tau, 0) Good python hdbg.dassert_lte(tau, 0) self._tau = tau Exceptions When we handle a default assignment, it's more natural to implement a post-condition: python col_rename_func = col_rename_func or (lambda x: x) hdbg.dassert_isinstance(col_rename_func, collections.Callable) Encode the assumptions using assertions If your code makes an assumption don\u2019t just write a comment, but implement an assertion so the code can\u2019t be executed if the assertion is not verified (instead of failing silently) python hdbg.dassert_lt(start_date, end_date) Use positional args when asserting dassert_* is modeled after logging so for the same reasons one should use positional args Bad python hdbg.dassert_eq(a, 1, \"No info for %s\" % method) Good python hdbg.dassert_eq(a, 1, \"No info for %s\", method) Report as much information as possible in an assertion When using a dassert_* you want to give to the user as much information as possible to fix the problem E.g., if you get an assertion after 8 hours of computation you don't want to have to add some logging and run for 8 hours to just know what happened A dassert_* typically prints as much info as possible, but it can't report information that is not visible to it: Bad python hdbg.dassert(string.startswith(\"hello\")) You don't know what is value of string is Good python hdbg.dassert(string.startswith(\"hello\"), \"string='%s'\", string) Note that often is useful to add ' (single quotation mark) to fight pesky spaces that make the value unclear, or to make the error as readable as possible Imports Don't use evil import * Do not use in notebooks or code the evil import * Bad python from helpers.sql import * Good python import helpers.sql as hsql The from ... import * : Pollutes the namespace with the symbols and spreads over everywhere, making it painful to clean up Obscures where each function is coming from, removing the context that comes with the namespace Is evil in many other ways Cleaning up the evil import * To clean up the mess you can: For notebooks Find & replace (e.g., using jupytext and Pycharm) Change the import and run one cell at the time For code Change the import and use linter on file to find all the problematic spots One of the few spots where the evil import * is ok is in the __init__.py to tweak the path of symbols exported by a library This is an advanced topic and you should rarely use it Avoid from ... import ... Import should always start from import : python import library as short_name import library.sublibrary as short_name This rule applies to imports of third party libraries and our library Because of this rule we have to always specify a short import of a parent lib before every code object that does not belong to the file: Bad python from helpers.sql import get_connection, get_connection_from_env_vars, \\ DBConnection, wait_connection_from_db, execute_insert_query Good python import helpers.sql as hsql ... ... hsql.get_connection() The problem with the from ... import ... is that it: Creates lots of maintenance effort E.g., anytime you want a new function you need to update the import statement Creates potential collisions of the same name E.g., lots of modules have a read_data() function Impairs debugging Importing directly in the namespace loses information about the module E.g., read_documents() is not clear: what documents? np.read_documents() at least gives information of which packages is it coming from and enables us to track it down to the code Exceptions to the import style We try to minimize the exceptions to this rule to avoid to keep this rule simple, rather than discussing about The current agreed upon exceptions are: For typing it is ok to do: python from typing import Iterable, List in order to avoid typing everywhere, since we want to use type hints as much as possible Always import with a full path from the root of the repo / submodule Bad python import exchange_class Good python import im_v2.ccxt.data.extract.exchange_class In this way your code can run without depending upon your current dir Baptizing module import Each module that can be imported should have a docstring at the very beginning (before any code) describing how it should be imported ```python \"\"\" Import as: import im_v2.ccxt.data.client.ccxt_clients as imvcdccccl \"\"\" ``` The import abbreviations are called 'short imports' and usually consist of 7-9 first letters of all the words that comprise path to file DO NOT simply give a random short import name Run linter to generate short import for a file automatically For some most files we specify short imorts by hand so thay may contain less symbols, e.g., hdbg The goal is to have always the same imports so it's easy to move code around, without collisions Examples of imports Example 1 Bad python from im_v2.ccxt.data.client import ccxt_clients as ccxtcl Good python import im_v2.ccxt.data.client.ccxt_clients as imvcdccccl Example 2 Bad python from edgar.shared import headers_extractor as he Good python import edgar.shared.headers_extractor as eshheext Example 3 Bad python from helpers import hdbg Good python import helpers.hdbg as hdbg Scripts Use Python and not bash for scripting We prefer to use python instead of bash scripts with very few exceptions E.g., scripts that need to modify the environment by setting env vars, like setenv.sh The problem with bash scripts is that it's too easy to put together a sequence of commands to automate a workflow Quickly things always become more complicated than what you thought, e.g., You might want to interrupt if one command in the script fails You want to use command line options You want to use logging to see what's going on inside the script You want to do a loop with a regex check inside Thus you need to use the more complex features of bash scripting and bash scripting is absolutely horrible, much worse than perl (e.g., just think of if [ ... ] vs if [[ ... ]] ) Our approach is to make simple to create scripts in python that are equivalent to sequencing shell commands, so that can evolve in complex scripts Skeleton for a script The ingredients are: dev_scripts/script_skeleton.py : a template to write simple scripts you can copy and modify it helpers/hsystem.py : a set of utilities that make simple to run shell commands (e.g., capturing their output, breaking on error or not, tee-ing to file, logging, ...) helpers has lots of useful libraries The official reference for a script is dev_scripts/script_skeleton.py You can copy this file and change it A simple example is: dev_scripts/git/gup.py A complex example is: dev_scripts/replace_text.py Some useful patterns Some useful patterns / idioms that are supported by the framework are: Incremental mode: you skip an action if its outcome is already present (e.g., skipping creating a dir, if it already exists and it contains all the results) Non-incremental mode: clean and execute everything from scratch Dry-run mode: the commands are written to screen instead of being executed Use scripts and not notebooks for long-running jobs We prefer to use scripts to execute code that might take long time (e.g., hours) to run, instead of notebooks Pros of script All the parameters are completely specified by a command line Reproducible and re-runnable Cons of notebooks Tend to crash / hang for long jobs Not easy to understand if the notebook is doing progress Not easy to get debug output Notebooks are designed for interactive computing / debugging and not batch jobs You can experiment with notebooks, move the code into a library, and wrap it in a script Follow the same structure All python scripts that are meant to be executed directly should: Be marked as executable files with: > chmod +x foo_bar.py Have the python code should start with the standard Unix shebang notation: python #!/usr/bin/env python This line tells the shell to use the python defined in the environment In this way you can execute directly without prepending with python Have a: python if __name__ == \"__main__\": ... Ideally use argparse to have a minimum of customization Use clear names for the scripts In general scripts (like functions) should have a name like \"action_verb\". Bad Examples of bad script names are timestamp_extractor.py and timestamp_extractor_v2.py Which timestamp data set are we talking about? What type of timestamps are we extracting? What is the difference about these two scripts? We need to give names to scripts that help people understand what they do and the context in which they operate We can add a reference to the task that originated the work (to give more context) Good E.g., for a script generating a dataset there should be an (umbrella) bug for this dataset, that we refer in the bug name, e.g., TaskXYZ_edgar_timestamp_dataset_extractor.py Also where the script is located should give some clue of what is related to Functions Avoid using non-exclusive bool arguments While a simple True / False switch may suffice for today's needs, very often more flexibility is eventually needed If more flexibility is needed for a bool argument, you are faced with the choice: Adding another parameter (then parameter combinations grow exponentially and may not all make sense) Changing the parameter type to something else Either way, you have to change the function interface To maintain flexibility from the start, opt for a str parameter \"mode\", which is allowed to take a small well-defined set of values. If an implicit default is desirable, consider making the default value of the parameter None . This is only a good route if the default operation is non-controversial / intuitively obvious. Try to make functions work on multiple types We encourage implementing functions that can work on multiple related types: Bad : implement demean_series() , demean_dataframe() Good : implement a function demean(obj) that can work with pd.Series and pd.DataFrame One convention is to call obj the variable whose type is not known until run-time In this way we take full advantage of duck typing to achieve something similar to C++ function overloading (actually even more expressive) Try to return the same type of the input, if possible E.g., the function called on a pd.Series returns a pd.Series Avoid hard-wired column name dependencies When working with dataframes, we often want need handle certain columns differently, or perform an operation on a strict subset of columns In these cases, it is tempting to assume that the special columns will have specific names, e.g., \"datetime\" The problem is that column names are Rarely obvious (e.g., compare \"datetime\" vs \"timestamp\" vs \"Datetime\" ) Tied to specific use cases The function you are writing may be written for a specific use case today, but what if it is more general If someone wants to reuse your function in a different setting where different column names make sense, why should they have to conform to your specific use case's needs? May overwrite existing column names For example, you may decided to call a column \"output\" , but what if the dataframe already has a column with that name? To get around this, allow the caller to communicate to the function the names of any special columns Good python def func(datetime_col: str): ... Make sure that you require column names only if they are actually used by the function If you must use hard-write column names internally or for some application, define the column name in the library file as a global variable, like python DATETIME_COL = \"datetime\" Users of the library can now access the column name through imports This prevents hidden column name dependencies from spreading like a virus throughout the codebase Single exit point from a function Consider the following Bad function python def _get_zero_element(list_: List): if not list_: return None else: return list_[0] Linter message is im.kibot/utils.py:394: [R1705(no-else-return), ExpiryContractMapper.extract_contract_expiry] Unnecessary \"else\" after \"return\" [pylint] Try to have a single exit point from a function, since this guarantees that the return value is always the same In general returning different data structures from the same function (e.g., a list in one case and a float in another) is indication of bad design There are exceptions like a function that works on different types (e.g., accepts a dataframe or a series and then returns a dataframe or a series, but the input and output is the same) Returning different types (e.g., float and string) is also bad Returning a type or None is typically ok Try to return values that are consistent so that the client doesn't have to switch statement, using isinstance(...) E.g., return a float and if the value can't be computed return np.nan (instead of None ) so that the client can use the return value in a uniform way Function examples with single exit point python def _get_zero_element(list_: List): if not list_: ret = np.nan else: ret = list_[0] return ret or python def _get_zero_element(list_: List): ret = np.nan if not list_ else list_[0] return ret However in rare cases it is OK to have functions like: python def ...(...): # Handle simple cases. ... if ...: return # lots of code ... return Order of function parameters Problem We want to have a standard, simple, and logical order for specifying the arguments of a function Decision The preferred order is: Input parameters Output parameters In-out parameters Default parameters Consistency of ordering of function parameters Try to: Keep related variables close to each other Keep the order of parameters similar across functions that have similar interface Enforcing these rules is based on best effort Pycharm is helpful when changing order of parameters Use linter to check consistency of types between function definition and invocation Style for default parameter Problem How to assign default parameters in a function to make them clear and distinguishable? Decision We make all the default parameters keyword-only This means that we should always specify default parameters using a keyword When building a function, always put default parameters after * It's ok to use a default parameter in the interface as long as it is a Python scalar (which is immutable by definition) Good python def function( value: int = 5, *, dir_name: str = \"hello_world\", ): You should not use list, maps, objects, etc. as the default value but pass None and then initialize the default param inside the function Bad python def function( *, obj: Object = Object(), list_: List[int] = [], ): Good python def function( *, obj: Optional[Object] = None, list_: Optional[List[int]] = None, ): if obj is None: obj = Object() if list_ is None: list_ = [] We use a None default value when a function needs to be wrapped and the default parameter needs to be propagated Good ```python def function1( ..., *, dir_name: Optional[str] = None, ): dir_name = dir_name or \"/very_long_path\" def function2( ..., *, dir_name: Optional[str] = None, ): function1(..., dir_name=dir_name) ``` Rationale Pros of the Good vs Bad style When you wrap multiple functions, each function needs to propagate the default parameters, which: - violates DRY; and - adds maintenance burden (if you change the innermost default parameter, you need to change all of them!) With the proposed approach, all the functions use None , until the innermost function resolves the parameters to the default values The interface is cleaner Implementation details are hidden (e.g., why should the caller know what is the default path?) Mutable parameters can not be passed through (see here )) Cons: One needs to add Optional to the type hint Calling functions with default parameters Problem You have a function python def func( task_name : str, dataset_dir : str, *, clobber : bool = clobber, ): ... How should it be invoked? Decision We prefer to Assign directly the positional parameters Bind explicitly the parameters with a default value using their name Do not put actual parameter values to the function call but specify them right before Bad python func(\"some_task_name\", \"/dir/subdir\", clobber=False) Good python task_name = \"some_task_name\" dataset_dir = \"/dir/subdir\" clobber = False func(task_name, dataset_dir, clobber=clobber) Rationale Pros of Good vs Bad style If a new parameter with a default value is added to the function func before clobber : The Good idiom doesn't need to be changed All instances of the Bad idiom need to be updated The Bad idiom might keep working but with silent failures Of course mypy and Pycharm might point this out The Good style highlights which default parameters are being overwritten, by using the name of the parameter Overwriting a default parameter is an exceptional situation that should be explicitly commented Cons: None Don't repeat non-default parameters Problem Given a function with the following interface: python def mult_and_sum(multiplier_1, multiplier_2, sum_): return multiplier_1 * multiplier_2 + sum_ how to invoke it? Decision Positional arguments are not default, so not keyword-only for consistency Bad python a = 1 b = 2 c = 3 mult_and_sum(multiplier_1=a, multiplier_2=b, sum_=c) Good python a = 1 b = 2 c = 3 mult_and_sum(a, b, c) Rationale Pros of Good vs Bad Non-default parameters in Python require all the successive parameters to be name-assigned This causes maintenance burden The Bad approach is in contrast with our rule for the default parameters We want to highlight which parameters are overriding the default The Bad approach in practice requires all positional parameters to be assigned explicitly causing: Repetition in violation of DRY (e.g., you need to repeat the same parameter everywhere); and Maintainance burden (e.g., if you change the name of a function parameter you need to change all the invocations) The Bad style is a convention used in no language (e.g., C, C++, Java) All languages allow binding by parameter position Only some languages allow binding by parameter name The Bad makes the code very wide, creating problems with our 80 columns rule Cons of Good vs Bad One could argue that the Bad form is clearer IMO the problem is in the names of the variables, which are uninformative, e.g., a better naming achieves the same goal of clarity python mul1 = 1 mul2 = 2 sum_ = 3 mult_and_sum(mul1, mul2, sum_) Writing clear beautiful code Keep related code close E.g., keep code that computes data close to the code that uses it. This holds also for notebooks: do not compute all the data structure and then analyze them. It\u2019s better to keep the section that \u201creads data\u201d close to the section that \u201cprocesses it\u201d. In this way it\u2019s easier to see \u201cblocks\u201d of code that are dependent from each other, and run only a cluster of cells. Order functions in topological order Order functions / classes in topological order so that the ones at the top of the files are the \"innermost\" and the ones at the end of the files are the \"outermost\" In this way, reading the code top to bottom one should not find a forward reference that requires skipping back and forth Linter reorders functions and classes in the topological order so make sure you run it after adding new ones Distinguish public and private functions The public functions foo_bar() (not starting with _ ) are the ones that make up the interface of a module and that are called from other modules and from notebooks Use private functions like _foo_bar() when a function is a helper of another private or public function Also follow the \u201ckeep related code close\u201d close by keeping the private functions close to the functions (private or public) that are using them Some references: StackOverflow Keep public functions organized in a logical order Keep the public functions in an order related to the use representing the typical flow of use, e.g., Common functions, used by all other functions Read data Process data Save data You can use banners to separate layers of the code. Use the banner long 80 cols (e.g., I have a vim macro to create banners that always look the same) and be consistent with empty lines before / empty and so on. The banner is a way of saying \u201call these functions belong together\u201d. ```python # ############################################################################# # Read data. # ############################################################################# def _helper1_to_func1(): ... def _helper2_to_func1(): ... def func1_read_data1(): _helper1_to_func1() ... _helper2_to_func2() # ############################################################################# # Process data. # ############################################################################# ... # ############################################################################# # Save data. # ############################################################################# ... ``` Ideally each section of code should use only sections above, and be used by sections below (aka \u201cUnix layer approach\u201d). If you find yourself using too many banners this is in indication that code might need to be split into different classes or files Although we don\u2019t have agreed upon rules, it might be ok to have large files as long as they are well organized. E.g., in pandas code base, all the code for DataFrame is in a single file long many thousands of lines (!), but it is nicely separated in sections that make easy to navigate the code Too many files can become problematic, since one needs to start jumping across many files: in other words it is possible to organize the code too much (e.g. what if each function is in a single module?) Let\u2019s try to find the right balance. It might be a good idea to use classes to split the code, but also OOP can have a dark side E.g., using OOP only to reorganize the code instead of introducing \u201cconcepts\u201d IMO the worst issue is that they don\u2019t play super-well with Jupyter autoreload Do not make tiny wrappers Examples of horrible functions: How many characters do we really saved? If typing is a problem, learn to touch type. python def is_exists(path: str) -> None: return os.path.exists(path) or python def make_dirs(path: str) -> List[str]: os.makedirs(path) This one can be simply replaced by os.path.dirname python def folder_name(f_name: str) -> str: if f_name[-1] != \"/\": return f_name + \"/\" return f_name Regex The rule of thumb is to compile a regex expression, e.g., python backslash_regex = re.compile(r\"\\\\\") only if it's called more than once, otherwise the overhead of compilation and creating another var is not justified Do not introduce another \u201cconcept\u201d unless really needed We want to introduce degrees of freedom and indirection only when we think this can be useful to make the code easy to maintain, read, and expand. If we add degrees of freedom everywhere just because we think that at some point in the future this might be useful, then there is very little advantage and large overhead. Introducing a new variable, function, class introduces a new concept that one needs to keep in mind. People that read the code, needs to go back and forth in the code to see what each concept means. Think about the trade-offs and be consistent. Example 1 python def fancy_print(txt): print \"fancy: \", txt Then people that change the code need to be aware that there is a function that prints in a special way. The only reason to add this shallow wrapper is that, in the future, we believe we want to change all these calls in the code. Example 2 python SNAPSHOT_ID = \"SnapshotId\" Another example is parametrizing a value used in a single function. If multiple functions need to use the same value, then this practice can be a good idea. If there is a single function using this, one should at least keep it local to the function. Still note that introducing a new concept can also create confusion. What if we need to change the code to: python SNAPSHOT_ID = \"TigerId\" then the variable and its value are in contrast. Return None or keep one type Functions that return different types can make things complicated downstream, since the callers need to be aware of all of it and handle different cases. This also complicates the docstring, since one needs to explicitly explain what the special values mean, all the types and so on. In general returning multiple types is an indication that there is a problem. Of course this is a trade-off between flexibility and making the code robust and easy to understand, e.g., In the following example it is better to either return None (to clarify that something special happened) or an empty dataframe pd.DataFrame(None) to allow the caller code being indifferent to what is returned. Bad python if \"Tags\" not in df.columns: df[\"Name\"] = np.nan else: df[\"Name\"] = df[\"Tags\"].apply(extract_name) Good python if \"Tags\" not in df.columns: df[\"Name\"] = None else: df[\"Name\"] = df[\"Tags\"].apply(extract_name) Avoid wall-of-text functions Bad python def get_timestamp_data(raw_df: pd.DataFrame) -> pd.DataFrame: timestamp_df = get_raw_timestamp(raw_df) documents_series = raw_df.progress_apply(he.extract_documents_v2, axis=1) documents_df = pd.concat(documents_series.values.tolist()) documents_df.set_index(api.cfg.INDEX_COLUMN, drop=True, inplace=True) documents_df = documents_df[ documents_df[api.cfg.DOCUMENTS_DOC_TYPE_COL] != \"\"] types = documents_df.groupby( api.cfg.DOCUMENTS_IDX_COL)[api.cfg.DOCUMENTS_DOC_TYPE_COL].unique() timestamp_df[api.cfg.TIMESTAMP_DOC_TYPES_COL] = types is_xbrl_series = raw_df[api.cfg.DATA_COLUMN].apply(he.check_xbrl) timestamp_df[api.cfg.TIMESTAMP_DOC_ISXBRL_COL] = is_xbrl_series timestamp_df[api.cfg.TIMESTAMP_DOC_EX99_COL] = timestamp_df[ api.cfg.TIMESTAMP_DOC_TYPES_COL].apply( lambda x: any(['ex-99' in t.lower() for t in x])) timestamp_df = timestamp_df.rename(columns=api.cfg.TIMESTAMP_COLUMN_RENAMES) return timestamp_df This function is correct but it has few problems (e.g., lack of a docstring, lots of unclear concepts, abuse of constants). Good ```python def get_timestamp_data(raw_df: pd.DataFrame) -> pd.DataFrame: \"\"\" Get data containing timestamp information. :param raw_df: input non-processed data :return: timestamp data \"\"\" # Get data containing raw timestamp information. timestamp_df = get_raw_timestamp(raw_df) # Extract the documents with data type information. documents_series = raw_df.progress_apply(he.extract_documents_v2, axis=1) documents_df = pd.concat(documents_series.values.tolist()) documents_df.set_index(api.cfg.INDEX_COLUMN, drop=True, inplace=True) documents_df = documents_df[ documents_df[api.cfg.DOCUMENTS_DOC_TYPE_COL] != \"\" ] types = documents_df.groupby( api.cfg.DOCUMENTS_IDX_COL )[api.cfg.DOCUMENTS_DOC_TYPE_COL].unique() # Set columns about types of information contained. timestamp_df[api.cfg.TIMESTAMP_DOC_TYPES_COL] = types is_xbrl_series = raw_df[api.cfg.DATA_COLUMN].apply(he.check_xbrl) timestamp_df[api.cfg.TIMESTAMP_DOC_ISXBRL_COL] = is_xbrl_series timestamp_df[api.cfg.TIMESTAMP_DOC_EX99_COL] = timestamp_df[ api.cfg.TIMESTAMP_DOC_TYPES_COL ].apply(lambda x: any([\"ex-99\" in t.lower() for t in x])) # Rename columns to canonical representation. timestamp_df = timestamp_df.rename(columns=api.cfg.TIMESTAMP_COLUMN_RENAMES) return timestamp_df ``` You should at least split the functions in chunks using # or even better comment what each chunk of code does. Writing robust code Don\u2019t let your functions catch the default-itis Default-itis is a disease of a function that manifests itself by getting too many default parameters. Default params should be used only for parameters that 99% of the time are constant. In general we require the caller to be clear and specify all the params. Functions catch defaultitis when the programmer is lazy and wants to change the behavior of a function without changing all the callers and unit tests. Resist this urge! grep is friend. Pycharm does this refactoring automatically. Explicitly bind default parameters It\u2019s best to explicitly bind functions with the default params so that if the function signature changes, your functions doesn\u2019t confuse a default param was a positional one. Bad python hdbg.dassert( args.form or args.form_list, \"You must specify one of the parameters: --form or --form_list\", ) Good python hdbg.dassert( args.form or args.form_list, msg=\"You must specify one of the parameters: --form or --form_list\", ) Don\u2019t hardwire params in a function call Bad python esa_df = universe.get_esa_universe_mapped(False, True) It is difficult to read and understand without looking for the invoked function (aka write-only code) and it\u2019s brittle since a change in the function params goes unnoticed. Good python gvkey = False cik = True esa_df = universe.get_esa_universe_mapped(gvkey, cik) It\u2019s better to be explicit (as usual) This solution is robust since it will work as long as gvkey and cik are the only needed params, which is as much as we can require from the called function. Make if-elif-else complete In general all the if-elif-else statements should to be complete, so that the code is robust. Bad python hdbg.dassert_in( frequency, [\"D\", \"T\"] \"Only daily ('D') and minutely ('T') frequencies are supported.\", ) if frequency == \"T\": ... if frequency == \"D\": ... Good python if frequency == \"T\": ... elif frequency == \"D\": ... else: raise ValueError(\"The %s frequency is not supported\" % frequency) This code is robust and correct Still the if-elif-else is enough and the assertion is not needed DRY here wins: you don't want to have to keep two pieces of code in sync The last line is a catch-all that makes sure even if we modify the previous It makes sense to check early only when you want to fail before doing more work E.g., sanity checking the parameters of a long running function, so that it doesn't run for 1 hr and then crash because the name of the file is incorrect Add TODOs when needed When there is something that you know you should have done, but you didn\u2019t have time to do, add a TODO, possibly using your github name e.g., python # TODO(gp): \u2026 In this way it\u2019s easy to grep for your TODOs, which becomes complicated when using different names. Be clear on the meaning of TODO A TODO(Batman): clean this up can be interpreted as \"Batman suggested to clean this up\" \"Batman should clean this up\" \"Batman has the most context to explain this problem or fix it\" On the one hand, git blame will report who created the TODO, so the first meaning is redundant. On the other hand, since we follow a shared ownership of the code, the second meaning should be quite infrequent. In fact the code has mostly TODO(*) todos, where * relates to all the team members Given pros and cons, the proposal is to use the first meaning. This is also what Google style guide suggests here If the TODO is associated with a Github issue, you can simply put the issue number and description inside the TODO, e.g., python # TODO(Grisha): \"Handle missing tiles\" CmTask #1775. You can create a TODO for somebody else, or you can create a Upsource comment / review or Github bug, depending on how important the issue is If the TODO is general, e.g., anybody can fix it, then you can avoid to put a name. This should not be abused since it creates a culture when people don\u2019t take responsibility for their mistakes. You can use P1, P2 to indicate if the issue is critical or not. E.g., P0 is the default for saying this is important, P1 is more of a \u201cnice to have\u201d. python # TODO(Sergey): P1 This can be implemented in pandas using a range generation. Common Python mistakes == vs is is checks whether two variables point to the same object (aka reference equality), while == checks if the two pointed objects are equivalent (value equality). For checking against types like None we want to use is , is not Bad python if var == None: _Good__ python if var is None: For checking against values we want to use == Bad python if unit is \"minute\": _Good__ python if unit == \"minute\": For more info checks here type() vs isinstance() type(obj) == list is worse since we want to test for reference equality (the type of object is a list) and not the type of obj is equivalent to a list. isinstance caters for inheritance (an instance of a derived class is an instance of a base class, too), while checking for equality of type does not (it demands identity of types and rejects instances of subtypes, AKA subclasses). Bad python if type(obj) is list: _Good__ python if isinstance(obj, list): For more info check here Unit tests Provide a minimal end-to-end unit testing (which creates a conda environment and then run a few unit tests) Use Pytest https://docs.pytest.org/en/latest/ unittest library Usually we are happy with Lightly testing the tricky functions Some end-to-end test to make sure the code is working Use your common sense E.g., no reason to test code that will be used only once To run unit tests in a single file ``` pytest datetime_utils_test.py -x -s ``` TODO(Dan/Samarth): Add a link to unit test doc when it is converted to md. Convention for naming tests According to PEP8 names of classes should always be camel case. On the other hand, if we are testing a function foo_bar() we prefer to call the testing code Test_foo_bar instead of TestFooBar . We suggest to name the class / method in the same way as the object we are testing, e.g.,: For testing a class FooBar we use a test class TestFooBar For testing methods of the class FooBar , e.g., FooBar.baz() , we use a test method TestFooBar.test_baz() For testing a protected method _gozilla() of FooBar we use test methods test__gozilla (note the double underscore). This is needed to distinguish testing the public method FooBar.gozilla() and FooBar._gozilla() We are ok with mixing camel case and snake case to mirror the code being tested. We prefer to name classes TestFooBar1 and methods TestFooBar1.test1() , even if there is a single class / method, to make it easier to add another test class, without having to rename class and check_string files. We are ok with using suffixes like 01 , 02 , \u2026 , when we believe it's important that methods are tested in a certain order (e.g., from the simplest to the most complex) Refactoring When moving / refactoring code If you move files, refactor code, move functions around make sure that: Code and notebook work (e.g., imports and caller of the functions) Documentation is updated (this is difficult, so best effort is enough) For code find all the places that have been modified ``` grep -r \"create_dataframe\" * edgar/form_4/notebooks/Task252_EDG4_Coverage_of_our_universe_from_Forms4.ipynb: \"documents, transactions = edu.create_dataframes(\\n\", edgar/form_4/notebooks/Task313_EDG4_Understand_Form_4_amendments.ipynb: \"documents, transactions = edu.create_dataframes(\\n\", edgar/form_4/notebooks/Task193_EDG4_Compare_form4_against_Whale_Wisdom_and_TR.ipynb: \"documents, transactions, owners, footnotes = edu.create_dataframes(\\n\", ``` Or if you use mighty Pycharm, Ctrl + Mouse Left Click (Shows you all places where this function or variable was used) and try to fix them, at least to give your best shot at making things work You can edit directly the notebooks without opening, or open and fix it. Good examples how you can safely rename anything for Pycharm users: https://www.jetbrains.com/help/Pycharm/rename-refactorings.html But remember, you must know how to do it without fancy IDE like Pycharm. If it\u2019s important code: Run unit tests Run notebooks (see here ) Write script for renamings When you need to rename any code object that is being used in many files, use dev_scripts/replace_text.py to write a script that will implement your task Read the script docstring for detailed information about how to use it You DO NOT use replace_text.py directly. Instead, create an executable .sh script that uses replace_text.py Look for examples at dev_scripts/cleanup_scripts Commit the created script to the mentioned folder so then your team members can use it to implement renaming in other libs Architectural and design pattern Research quality vs production quality Code belonging to top level libraries (e.g., //amp/core , //amp/helpers ) and production (e.g., //.../db , vendors ) needs to meet high quality standards, e.g., Well commented Following our style guide Thoroughly reviewed Good design Comprehensive unit tests Research code in notebook and python can follow slightly looser standards, e.g., Sprinkled with some TODOs Not perfectly general The reason is that: Research code is still evolving and we want to keep the structure flexible We don't want to invest the time in making it perfect if the research doesn't pan out Note that research code still needs to be: Understandable / usable by not authors Well commented Follow the style guide Somehow unit tested We should be able to raise the quality of a piece of research code to production quality when that research goes into production Always separate what changes from what stays the same In both main code and unit test it's not a good idea to repeat the same code Bad Copy-paste-modify Good Refactor the common part in a function and then change the parameters used to call the function Example: What code is clearer to you, VersionA or VersionB? Can you spot the difference between the 2 pieces of code? Version A ```python stopwords = nlp_ut.read_stopwords_json(_STOPWORDS_PATH) texts = [\"a\", \"an\", \"the\"] stop_words = nlp_ut.get_stopwords( categories=[\"articles\"], stopwords=stopwords ) actual_result = nlp_ut.remove_tokens(texts, stop_words=stop_words) expected_result = [] self.assertEqual(actual_result, expected_result) ... texts = [\"do\", \"does\", \"is\", \"am\", \"are\", \"be\", \"been\", \"'s\", \"'m\", \"'re\"] stop_words = nlp_ut.get_stopwords( categories=[\"auxiliary_verbs\"], stopwords=stopwords, ) actual_result = nlp_ut.remove_tokens(texts, stop_words=stop_words) expected_result = [] self.assertEqual(actual_result, expected_result) ``` Version B ```python def _helper(texts, categories, expected_result): stopwords = nlp_ut.read_stopwords_json(_STOPWORDS_PATH) stop_words = nlp_ut.get_stopwords( categories=categories, stopwords=stopwords ) actual_result = nlp_ut.remove_tokens(texts, stop_words=stop_words) expected_result = [] self.assertEqual(actual_result, expected_result) texts = [\"a\", \"an\", \"the\"] categories = [\"articles\"] expected_result = [] _helper(texts, categories, expected_result) ... texts = [\"do\", \"does\", \"is\", \"am\", \"are\", \"be\", \"been\", \"'s\", \"'m\", \"'re\"] categories = [\"auxiliary_verbs\"] expected_result = [] _helper(texts, categories, expected_result) ``` Yes, Version A is Bad and Version B is Good Organize scripts as pipelines One can organize complex computations in stages of a pipeline E.g., to parse EDGAR forms Download -> (raw data) -> header parser -> (pq data) -> XBLR / XML / XLS parser -> (pq data) -> custom transformation One should be able to run the entire pipeline or just a piece E.g., one can run the header parser from the raw data, save the result to file, then read this file back, and run the XBLR parser Ideally one would always prefer to run the pipeline from scratch, but sometimes the stages are too expensive to compute over and over, so using chunks of the pipeline is better This can also mixed with the \u201cincremental mode\u201d, so that if one stage has already been run and the intermediate data has been generated, that stage is skipped Each stage can save files in a tmp_dir/stage_name The code should be organized to allow these different modes of operations, but there is not always need to be super exhaustive in terms of command line options E.g., I implement the various chunks of the pipeline in a library, separating functions that read / save data after a stage and then assemble the pieces into a throw-away script where I hardwire the file names and so on Make filename unique Problem We have a lot of structure / boilerplate in our project around RH hypotheses. E.g., there are corresponding files for all the RH like: RHxyz/configs.py RHxyz/pipeline.py It is not clear if it's better to make filenames completely unique by repeating the RH , e.g., RH1E_configs.py , or let the directories disambiguate. Note that we are not referring to other common files like utils.py , which are made unique by their position in the file system and by the automatic shortening of the imports. Decision Invoking the principle of 'explicit is better than implicit', the proposal is to repeat the prefix. Bad : RH1E/configs.py Good : RH1E/RH1E_configs.py Rationale Pros of the repetition (e.g., RH1E/RH1E_configs.py ): The filename is unique so there is no dependency on where you are Since pytest requires all files to be unique, we need to repeat the prefix for the test names and the rule is \"always make the names of the files unique\" We are going to have lots of these files and we want to minimize the risk of making mistakes Cons of the repetition: Stuttering What happens if there are multiple nested dirs? Do we repeat all the prefixes? This seems to be an infrequent case Incremental behavior Often we need to run the same code over and over E.g., because the code fails on an unexpected point and then we need to re-run from the beginning We use options like: --incremental --force --start_date --end_date --output_file Check existence output file before start function (or a thread when using parallelism) which handle data of the corresponding period If --incremental is set and output file already exists then skip the computation and report Log.info(\u201cSkipping processing file %s as requested\u201d, \u2026) If --incremental is not set If output file exists then we issue a log.warn and abort the process If output file exists and param --force , then report a log.warn and rewrite output file Run end-to-end Try to run things end-to-end (and from scratch) so we can catch these unexpected issues and code defensively E.g., we found out that TR data is malformed sometimes and only running end-to-end we can catch all the weird cases This also helps with scalability issues, since if takes 1 hr for 1 month of data and we have 10 years of data is going to take 120 hours (=5 days) to run on the entire data set Think about scalability Do experiments to try to understand if a code solution can scale to the dimension of the data we have to deal with E.g., inserting data by doing SQL inserts of single rows are not scalable for pushing 100GB of data Remember that typically we need to run the same scripts multiple times (e.g., for debug and / or production) Use command line for reproducibility Try to pass params through command line options when possible In this way a command line contains all the set-up to run an experiment Structure the code in terms of filters Focus on build a set of \"filters\" split into different functions, rather than a monolithic flow Organize the code in terms of a sequence of transformations that can be run in sequence, e.g., Create SQL tables Convert json data to csv Normalize tables Load csv files into SQL Sanity check the SQL (e.g., mismatching TR codes, missing dates) Patch up SQL (e.g., inserting missing TR codes and reporting them to us so we can check with TR) Code style for different languages SQL You can use the package https://github.com/andialbrecht/sqlparse to format SQL queries There is also an on-line version of the same formatter at https://sqlformat.org Conventions (Addendum) Be patient For some reason talking about conventions makes people defensive and uncomfortable, sometimes. Conventions are not a matter of being right or wrong, but to consider pros and cons of different approaches, and make the decision only once instead of discussing the same problem every time. In this way we can focus on achieving the Ultimate Goal. If you are unsure or indifferent to a choice, be flexible and let other persons that seem to be less flexible decide. Goal The goal of the conventions is to simplify our job by removing ambiguity There is no right or wrong: that's why it's a convention and not a law of nature On the flip-side, if there is a right and wrong, then what we are discussing probably shouldn\u2019t be considered as a convention We don't want to spend time discussing inconsequential points We don't want reviewers to pick lints instead of focusing on architectural issues and potential bugs Remove cognitive burden of being distracted by \"this is an annoying lint\" (or at least perceived lint) Once a convention is stable, we would like to automate enforcing it by the linter Ideally the linter should fix our mistakes so we don't even have to think about them, and reviewers don't have to be distracted with pointing out the lints Keep the rules simple E.g., assume that we accepted the following rules: Git is capitalized if it refers to the tool and it's not capitalized when it refers to the command (this is what Git documentation suggests) Python is written capitalized (this is what Python documentation suggests) pandas is written lowercase, unless it is a beginning of the line in which case it's capitalized, but it's better to try to avoid to start a sentence with it (this is what pandas + English convention seems to suggest) Any other library could suggest a different convention based on the preference of its author, who tries to finally force people to follow his / her convention \u2026) All these rules require mental energy to be followed and readers will spend time checking that these rules are enforced, rather than focusing on bugs and architecture. In this case we want to leverage the ambiguity of \"it's unclear what is the correct approach\" by simplifying the rule E.g., every name of tools or library is always capitalized This is simple to remember and automatically enforce Allow turning off the automatic tools We understand that tools can't always understand the context and the subtleties of human thoughts, and therefore they yield inevitably to false positives. Then we always want to permit disabling the automatic checks / fixes e.g., by using directives in comments or special syntax (e.g., anything in a ... or \u2026 block should be leaved untouched) It can be tricky determining when an exception is really needed and when overriding the tool becomes a slippery slope for ignoring the rules. Patience and flexibility is advised here. Make the spell-checker happy The spell-checker is not always right: false positives are often very annoying We prefer to find a way to make the spell-checker happy rather than argue that the spell-checker is wrong and ignore it The risk with overriding the spell-checker (and any other tool) is that the decision is not binary anymore correct / not-correct and can't be automated and requires mental energy to see if the flagged error is real or not. E.g., insample is flagged as erroneous, so we convert it into in-sample . The solution for the obvious cases of missing a word (e.g., a technical word) is to add words to the vocabulary. This still needs to be done by everyone, until we find a way to centralize the vocabulary. E.g., untradable is a valid English word, but Pycharm's spell-checker doesn't recognize it. TODO(*): Should we add it to the dictionary or write it as \"un-tradable\"? Still we don't want to override the spell-checker when an alternative lower-cost solution is available. E.g., in-sample instead of insample out-of-sample instead of oos We decided that hyper-parameter can be written without hyphen: hyperparameter","title":"Sorrentum - Python Style Guide"},{"location":"Coding_Style_Guide/#sorrentum-python-style-guide","text":"Meta Disclaimer References High-Level Principles Follow the DRY principle The writer is the reader Encapsulate what changes Least surprise principle Pay the technical debt End-to-end first Unit test everything Don't get attached to code Always plan before writing code Think hard about naming Look for inconsistencies No ugly hacks Our coding suggestions Being careful with naming Follow the conventions Follow spelling rules Search good names, avoid bad names General naming rules Do not be stingy Do not abbreviate just to save characters When to use abbreviations Avoid code stutter Comments and docstrings General conventions Descriptive vs imperative style Docstrings style Comments style Replace empty lines in code with comments Referring to an object in code comments Avoid distracting comments Commenting out code Use type hints Interval notation If you find a bug or obsolete docstring/TODO in the code Linter Remove linter messages When to disable linter messages Prefer non-inlined linter comments Don't mix real changes with linter changes Logging Always use logging instead of prints Our logging idiom Logging level Use positional args when logging Exceptions don't allow positional args Report warnings Assertions Validate values before an assignment Encode the assumptions using assertions Use positional args when asserting Report as much information as possible in an assertion Imports Don't use evil import * Cleaning up the evil import * Avoid from ... import ... Exceptions to the import style Always import with a full path from the root of the repo / submodule Baptizing module import Examples of imports Scripts Use Python and not bash for scripting Skeleton for a script Some useful patterns Use scripts and not notebooks for long-running jobs Follow the same structure Use clear names for the scripts Functions Avoid using non-exclusive bool arguments Try to make functions work on multiple types Avoid hard-wired column name dependencies Single exit point from a function Order of function parameters Problem Decision Consistency of ordering of function parameters Style for default parameter Problem Decision Rationale Calling functions with default parameters Problem Decision Rationale Don't repeat non-default parameters Problem Decision Rationale Writing clear beautiful code Keep related code close Order functions in topological order Distinguish public and private functions Keep public functions organized in a logical order Do not make tiny wrappers Regex Do not introduce another \u201cconcept\u201d unless really needed Return None or keep one type Avoid wall-of-text functions Writing robust code Don\u2019t let your functions catch the default-itis Explicitly bind default parameters Don\u2019t hardwire params in a function call Make if-elif-else complete Add TODOs when needed Common Python mistakes == vs is type() vs isinstance() Unit tests Convention for naming tests Refactoring When moving / refactoring code Write script for renamings Architectural and design pattern Research quality vs production quality Always separate what changes from what stays the same Organize scripts as pipelines Make filename unique Incremental behavior Run end-to-end Think about scalability Use command line for reproducibility Structure the code in terms of filters Code style for different languages SQL Conventions (Addendum) Be patient Goal Keep the rules simple Allow turning off the automatic tools Make the spell-checker happy","title":"Sorrentum - Python Style Guide"},{"location":"Coding_Style_Guide/#meta","text":"What we call the \"rules\" is actually just a convention Not about the absolute best way of doing something in all cases Optimized for the common case Can become cumbersome or weird to follow for some corner cases We prefer simple rather than optimal rules that can be applied in most of the cases without thinking or going to check the documentation The rules are striving to achieve consistency and robustness E.g., see \"tab vs space\" flame-war from the 90s We care about consistency rather than arguing about which approach is better in each case The rules are optimized for the average developer / data scientist and not for power users The rules try to minimize the maintenance burden We don't want a change somewhere to propagate everywhere We want to minimize the propagation of a change Some of the rules are evolving based on what we are seeing through the reviews","title":"Meta"},{"location":"Coding_Style_Guide/#disclaimer","text":"This document was forked from Google Python Style Guide , therefore, the numbering of chapters sets off where the Style Guide ends. Make sure to familiarize yourself with it before proceeding to the rest of the doc, since it is the basis of our team\u2019s code style. Another important source is The Pragmatic Programmer by David Thomas and Andrew Hunt. While not Python-specific, it provides an invaluable set of general principles by which any person working with code (software developer, DevOps or data scientist) should abide. Read it on long commutes, during lunch, and treat yourself to a physical copy on Christmas. The book is summarized here , but do not deprive yourself of the engaging manner in which Thomas & Hunt elaborate on these points -- on top of it all, it is a very, very enjoyable read.","title":"Disclaimer"},{"location":"Coding_Style_Guide/#references","text":"Coding Google Python Style Guide (GPSG) Code convention from PEP8 Documentation Docstring convention from PEP257 Google documentation best practices Commenting style Sphinx Sphinx tutorial Design Google philosophical stuff Unix rules (although a bit cryptic sometimes)","title":"References"},{"location":"Coding_Style_Guide/#high-level-principles","text":"In this paragraph we summarize the high-level principles that we follow for designing and implementing code and research. We should be careful in adding principles here. Ideally principles should be non-overlapping and generating all the other lower level principles we follow (like a basis for a vector space)","title":"High-Level Principles"},{"location":"Coding_Style_Guide/#follow-the-dry-principle","text":"","title":"Follow the DRY principle"},{"location":"Coding_Style_Guide/#the-writer-is-the-reader","text":"Make code easy to read even if it is more difficult to write Code is written 1x and read 100x Remember that even if things are perfectly clear now to the person that wrote the code, in a couple of months the code will look foreign to whoever wrote the code. So make your future-self's life easier by following the conventions and erring on the side of documenting for the reader.","title":"The writer is the reader"},{"location":"Coding_Style_Guide/#encapsulate-what-changes","text":"Separate what changes from what stays the same","title":"Encapsulate what changes"},{"location":"Coding_Style_Guide/#least-surprise-principle","text":"Try to make sure that the reader is not surprised","title":"Least surprise principle"},{"location":"Coding_Style_Guide/#pay-the-technical-debt","text":"Any unpaid debt is guaranteed to bite you when you don't expect it Still some debt is inevitable: try to find the right trade-off","title":"Pay the technical debt"},{"location":"Coding_Style_Guide/#end-to-end-first","text":"Always focus on implementing things end-to-end, then improve each block Remember the analogy of building the car through the skateboard, the bike, etc. Compare this approach to building wheels, chassis, with a big-bang integration at the end","title":"End-to-end first"},{"location":"Coding_Style_Guide/#unit-test-everything","text":"Code that matters needs to be unit tested Code that doesn't matter should not be checked in the repo The logical implication is: all code checked in the repo should be unit tested","title":"Unit test everything"},{"location":"Coding_Style_Guide/#dont-get-attached-to-code","text":"It's ok to delete, discard, retire code that is not useful any more Don't take it personally when people suggest changes or simplification","title":"Don't get attached to code"},{"location":"Coding_Style_Guide/#always-plan-before-writing-code","text":"File a GitHub issue Think about what to do and how to do it Ask for help or for a review The best code is the one that we avoid to write through a clever mental kung-fu move","title":"Always plan before writing code"},{"location":"Coding_Style_Guide/#think-hard-about-naming","text":"Finding a name for a code object, notebook, is extremely difficult but very important to build a mental map Spend the needed time on it","title":"Think hard about naming"},{"location":"Coding_Style_Guide/#look-for-inconsistencies","text":"Stop for a second after you have, before sending out: Implemented code or a notebook Written documentation Written an e-mail ... Reset your mind and look at everything with fresh eyes like if it was the first time you saw it Does everything make sense to someone that sees this for the first time? Can (and should) it be improved? Do you see inconsistencies, potential issues? It will take less and less time to become good at this","title":"Look for inconsistencies"},{"location":"Coding_Style_Guide/#no-ugly-hacks","text":"We don't tolerate \"ugly hacks\", i.e., hacks that require lots of work to be undone (much more than the effort to do it right in the first place) Especially an ugly design hack, e.g., a Singleton, or some unnecessary dependency between distant pieces of code Ugly hacks spreads everywhere in the code base","title":"No ugly hacks"},{"location":"Coding_Style_Guide/#our-coding-suggestions","text":"","title":"Our coding suggestions"},{"location":"Coding_Style_Guide/#being-careful-with-naming","text":"","title":"Being careful with naming"},{"location":"Coding_Style_Guide/#follow-the-conventions","text":"Name executable files (scripts) and library functions using verbs (e.g., download.py , download_data() ) Name classes and (non-executable) files using nouns (e.g., Downloader() , downloader.py ) For decorators we don't use a verb as we do for normal functions, but rather an adjective or a past tense verb, e.g., python def timed(f): \"\"\" Add a timer decorator around a specified function. \"\"\" \u2026","title":"Follow the conventions"},{"location":"Coding_Style_Guide/#follow-spelling-rules","text":"We spell commands in lower-case, and programs with initial upper case: \"Git\" (as program), \"git\" (as the command) We distinguish \"research\" (not \"default\", \"base\") vs \"production\" We use different names for indicating the same concept, e.g., dir , path , folder Preferred term is dir Name of columns The name of columns should be ..._col and not ..._col_name or _column Timestamp We spell timestamp , we do not abbreviate it as ts We prefer timestamp to datetime E.g., start_timestamp instead of start_datetime Abbreviations JSON, CSV, DB, etc., are abbreviations and thus should be capitalized in comments and docstrings, and treated as abbreviations in code when it doesn't conflict with other rules E.g., convert_to_CSV , but csv_file_name as a variable name that is not global Profit-and-loss: PnL instead of pnl or PNL","title":"Follow spelling rules"},{"location":"Coding_Style_Guide/#search-good-names-avoid-bad-names","text":"","title":"Search good names, avoid bad names"},{"location":"Coding_Style_Guide/#general-naming-rules","text":"Naming things properly is one of the most difficult task of a programmer / data scientist The name needs to be (possibly) short and memorable However, don't be afraid to use long names, if needed, e.g., process_text_with_full_pipeline_twitter_v1 \u0421larity is more important than number of bytes used The name should capture what the object represents, without reference to things that can change or to details that are not important The name should refer to what objects do (i.e., mechanisms), rather than how we use them (i.e., policies) The name needs to be non-controversial: people need to be able to map the name in their mental model The name needs to sound good in English Bad : AdapterSequential sounds bad Good : SequentialAdapter sounds good Some examples of how NOT to do naming: raw_df is a terrible name \"raw\" with respect to what? Cooked? Read-After-Write race condition? person_dict is bad What if we switch from a dictionary to an object? Then we need to change the name everywhere! The name should capture what the data structure represents (its semantics) and not how it is implemented","title":"General naming rules"},{"location":"Coding_Style_Guide/#do-not-be-stingy","text":"Why calling an object TimeSeriesMinStudy instead of TimeSeriesMinuteStudy ? Saving 3 letters is not worth The reader might interpret Min as Minimal (or Miniature , Minnie , Minotaur ) If you don't like to type, we suggest you get a better keyboard, e.g., this","title":"Do not be stingy"},{"location":"Coding_Style_Guide/#do-not-abbreviate-just-to-save-characters","text":"Abbreviations just to save space are rarely beneficial to the reader. E.g., Fwd (forward) Bwd (backward) Act (actual) Exp (expected)","title":"Do not abbreviate just to save characters"},{"location":"Coding_Style_Guide/#when-to-use-abbreviations","text":"We could relax this rule for short lived functions and variables in order to save some visual noise. Sometimes an abbreviation is so short and common that it's ok to leave it E.g., Df (dataframe) Srs (series) Idx (index) Id (identifier) Val (value) Var (variable) Args (arguments) Kwargs (keyword arguments) Col (column) Vol (volatility) while volume is always spelled out","title":"When to use abbreviations"},{"location":"Coding_Style_Guide/#avoid-code-stutter","text":"An example of code stutter: you want to add a function that returns git root path in a module git Bad Name is get_git_root_path() ```python import helpers.git as git ... git.get_git_root_path() ``` You see that the module is already specifying we are talking about Git Good Name is get_root_path() ```python import helpers.git as git ... git.get_root_path() ``` This is not only aesthetic reason but a bit related to a weak form of DRY","title":"Avoid code stutter"},{"location":"Coding_Style_Guide/#comments-and-docstrings","text":"","title":"Comments and docstrings"},{"location":"Coding_Style_Guide/#general-conventions","text":"Code needs to be properly commented We follow python standard PEP 257 for commenting PEP 257 standardizes what comments should express and how they should do it (e.g., use triple quotes for commenting a function), but does not specify what markup syntax should be used to describe comments Different conventions have been developed for documenting interfaces ReST Google (which is cross-language, e.g., C++, python, ...) Epytext Numpydoc","title":"General conventions"},{"location":"Coding_Style_Guide/#descriptive-vs-imperative-style","text":"We decided to use imperative style for our comments and docstrings Pylint and other python QA tools favor an imperative style From PEP 257 The docstring is a phrase ending in a period. It prescribes the function or method's effect as a command (\"Do this\", \"Return that\"), not as a description; e.g. don't write \"Returns the pathname ...\".","title":"Descriptive vs imperative style"},{"location":"Coding_Style_Guide/#docstrings-style","text":"We follow ReST (aka re-Structured Text) style for docstrings which is: The most widely supported in the python community Supported by all doc generation tools (e.g., epydoc, sphinx) Default in Pycharm Default in pyment Supported by pydocstyle (which does not support Google style as explained here ) Example of a function definition with ReST styled docstring: ```python def my_function(param1: str) -> str: \"\"\" A one-line description of what the function does. A longer description (possibly on multiple lines) with a more detailed explanation of what the function does, trying to not be redundant with the parameter / return description below. The focus is on the interface and what the user should know to use the function and not how the function is implemented. :param param1: this is a first param :return: this is a description of what is returned \"\"\" ``` We pick lowercase after :param XYZ: ... unless the first word is a proper noun or type A full ReST docstring styling also requires to specify params and return types, however type hinting makes it redundant so you should use only type hinting Put docstrings in triple quotation marks Bad python Generate \"random returns\". Good python \"\"\" Generate \"random returns\". \"\"\" Sometimes functions are small enough so we just use a 1-liner docstring without detailed params and return descriptions. Just do not put text and docstring brackets in one line Bad python \"\"\"This is not our approach.\"\"\" Good python \"\"\" This is our approach. \"\"\" More examples of and discussions on python docstrings","title":"Docstrings style"},{"location":"Coding_Style_Guide/#comments-style","text":"Comments follow the same style of docstrings, e.g., imperative style with period . at the end Bad python # This comment is not imperative and has no period at the end Good python # Make comments imperative and end them with a period. Always place comments above the lines that they are referring to. Avoid writing comments on the same line as code since they require extra maintenance (e.g., when the line becomes too long) Bad python print(\"hello world\") # Introduce yourself. Good python # Introduce yourself. print(\"hello world\") The only exception is commenting if-elif-else statments: we comment them underneath the each statement in order to explain the code that belongs to the each statement particularly Bad python # Set remapping based on the run type. if is_prod: ... else: ... Good python # if is_prod: # Set remapping for database data used in production. ... else: # Set remapping for file system data used in simulation. ... If you want to separate an if statement from a bunch of code preceeding it, you can leave an empty comment before it","title":"Comments style"},{"location":"Coding_Style_Guide/#replace-empty-lines-in-code-with-comments","text":"If you feel that you need an empty line in the code, it probably means that a specific chunk of code is a logical piece of code performing a cohesive function ```python ... end_y = end_dt.year paths = list() ... - Instead of putting an empty line, you should put a comment describing at high level what the code does. python ... end_y = end_dt.year # Generate a list of file paths for Parquet dataset. paths = list() ... - If you don't want to add comments, just comment the empty line. python ... end_y = end_dt.year # paths = list() ... ``` - The problem with empty lines is that they are visually confusing since one empty line is used also to separate functions. For this reason we suggest using an empty comment","title":"Replace empty lines in code with comments"},{"location":"Coding_Style_Guide/#referring-to-an-object-in-code-comments","text":"In general, avoid this whenever possible Code object names (e.g., function, class, params) are often subject to change, so we need to take care of them everywhere. It is very hard to track all of them in comments so replace the names with their actual meaning Bad python # Generate a list of file paths for `ParquetDataset`. Good python # Generate a list of file paths for Parquet dataset. However, sometimes it is necessary. In this case refer to objects in the code using Markdown. This is useful for distinguishing the object code from the real-life object Bad python # The dataframe df_tmp is used for ... Good python # The dataframe `df_tmp` is used for ...","title":"Referring to an object in code comments"},{"location":"Coding_Style_Guide/#avoid-distracting-comments","text":"Use comments to explain the high level logic / goal of a piece of code and not the details, e.g., do not comment things that are obvious Bad python # Print results. _LOG.info(\"Results are %s\", ...)","title":"Avoid distracting comments"},{"location":"Coding_Style_Guide/#commenting-out-code","text":"When we comment out code, we should explain why it is no longer relevant Bad python is_alive = pd.Series(True, index=metadata.index) # is_alive = kgutils.annotate_alive(metadata, self.alive_cutoff) Good python # TODO(*): As discussed in PTask5047 for now we set all timeseries to be alive. # is_alive = kgutils.annotate_alive(metadata, self.alive_cutoff) is_alive = pd.Series(True, index=metadata.index)","title":"Commenting out code"},{"location":"Coding_Style_Guide/#use-type-hints","text":"We expect new code to use type hints whenever possible See PEP 484 Type hints cheat sheet At some point we will start adding type hints to old code We plan to start using static analyzers (e.g., mypy ) to check for bugs from type mistakes and to enforce type hints at run-time, whenever possible","title":"Use type hints"},{"location":"Coding_Style_Guide/#interval-notation","text":"Intervals are represented with [a, b), (a, b], (a, b), [a, b] We don't use the other style [a, b[","title":"Interval notation"},{"location":"Coding_Style_Guide/#if-you-find-a-bug-or-obsolete-docstringtodo-in-the-code","text":"The process is: Do a git blame to find who wrote the code If it's an easy bug, you can fix it and ask for a review from the author You can comment on a PR (if there is one) You can file a bug on Github with Clear info on the problem How to reproduce it, ideally a unit test Stacktrace","title":"If you find a bug or obsolete docstring/TODO in the code"},{"location":"Coding_Style_Guide/#linter","text":"The linter is in charge of reformatting the code according to our conventions and reporting potential problems You can find instructions on how to run linter at the First review process doc","title":"Linter"},{"location":"Coding_Style_Guide/#remove-linter-messages","text":"When the linter reports a problem: We assume that linter messages are correct, until the linter is proven wrong We try to understand what is the rationale for the linter's complaints We then change the code to follow the linter's suggestion and remove the lint If you think a message is too pedantic, please file a bug with the example and as a team we will consider whether to exclude that message from our list of linter suggestions If you think the message is a false positive, then try to change the code to make the linter happy E.g., if the code depends on some run-time behavior that the linter can't infer, then you should question whether that behavior is really needed A human reader would probably be as confused as the linter is","title":"Remove linter messages"},{"location":"Coding_Style_Guide/#when-to-disable-linter-messages","text":"If you really believe you should override the linter in this particular case, then use something like: python # pylint: disable=some-message,another-one You then need to explain in a comment why you are overriding the linter. Don't use linter code numbers, but the symbolic name whenever possible: Bad python # pylint: disable=W0611 import config.logging_settings Good python # pylint: disable=unused-import # This is needed when evaluating code at run-time that depends from # this import. import config.logging_settings","title":"When to disable linter messages"},{"location":"Coding_Style_Guide/#prefer-non-inlined-linter-comments","text":"As for the general comments, we prefer make linter comments non-inlined However, sometimes there is no other choice than an inlined comment to get the linter to understand which line we are referring to, so in rare cases it is OK: Bad but ok if needed python import config.logging_settings # pylint: disable=unused-import Good python # pylint: disable=line-too-long expected_df_as_str = \"\"\"# df= asset_id last_price start_datetime timestamp_db end_datetime 2000-01-01 09:31:00-05:00 1000 999.874540 2000-01-01 09:30:00-05:00 2000-01-01 09:31:00-05:00 2000-01-01 09:32:00-05:00 1000 1000.325254 2000-01-01 09:31:00-05:00 2000-01-01 09:32:00-05:00 2000-01-01 09:33:00-05:00 1000 1000.557248 2000-01-01 09:32:00-05:00 2000-01-01 09:33:00-05:00\"\"\" # pylint: enable=line-too-long","title":"Prefer non-inlined linter comments"},{"location":"Coding_Style_Guide/#dont-mix-real-changes-with-linter-changes","text":"We don't commit changes that modify the code together with linter reformatting, unless the linting is applied to the changes we just made The reason for not mixing real and linter changes is that for a PR or to just read the code it is difficult to understand what really changed vs what was just a cosmetic modification If you are worried the linter might change your code in a way you don't like, e.g., Screwing up some formatting you care about for some reason, or Suggesting changes that you are worried might introduce bugs you can commit your code and then do a \"lint commit\" with a message \"CMTaskXYZ: Lint\" In this way you have a backup state that you can rollback to, if you want If you run the linter and see that the linter is reformatting / modifying pieces of code you din't change, it means that our team mate forgot to lint their code git blame can figure out the culprit You can send him / her a ping to remind her to lint, so you don't have to clean after him / her In this case, the suggested approach is: Commit your change to a branch / stash Run the linter by itself on the files that need to be cleaned, without any change Run the unit tests to make sure nothing is breaking You can fix lints or just do formatting: it's up to you You can make this change directly on master or do a PR if you want to be extra sure: your call","title":"Don't mix real changes with linter changes"},{"location":"Coding_Style_Guide/#logging","text":"","title":"Logging"},{"location":"Coding_Style_Guide/#always-use-logging-instead-of-prints","text":"Always use logging and never print() to monitor the execution","title":"Always use logging instead of prints"},{"location":"Coding_Style_Guide/#our-logging-idiom","text":"In order to use our logging framework (e.g., -v from command lines, and much more) use: ```python import helpers.hdbg as hdbg _LOG = logging.getLogger( name ) hdbg.init_logger(verbosity=logging.DEBUG) _LOG.debug(\"I am a debug function about %s\", a) ``` In this way one can decide how much debug info is needed (see Unix rule of silence) E.g., when there is a bug one can run with -v DEBUG and see what's happening right before the bug","title":"Our logging idiom"},{"location":"Coding_Style_Guide/#logging-level","text":"Use _LOG.warning for messages to the final user related to something unexpected where the code is making a decision that might be controversial E.g., processing a dir that is supposed to contain only .csv files the code finds a non- .csv file and decides to skip it, instead of breaking Use _LOG.info to communicate to the final user, e.g., When the script is started Where the script is saving its results A progress bar indicating the amount of work completed Use _LOG.debug to communicate information related to the internal behavior of code Do not pollute the output with information a regular user does not care about Make sure the script prints when the work is terminated, e.g., \"DONE\" or \"Results written to ...\" This is useful to indicate that the script did not die in the middle: sometimes this happens silently and it is reported only from the OS return code","title":"Logging level"},{"location":"Coding_Style_Guide/#use-positional-args-when-logging","text":"Bad python _LOG.debug(\"cmd=%s %s %s\" % (cmd1, cmd2, cmd3)) _LOG.debug(\"cmd=%s %s %s\".format(cmd1, cmd2, cmd3)) _LOG.debug(\"cmd={cmd1} {cmd2} {cmd3}\") Good python _LOG.debug(\"cmd=%s %s %s\", cmd1, cmd2, cmd3) All the statements are equivalent from the functional point of view The reason is that in the second case the string is not built unless the logging is actually performed, which limits time overhead from logging","title":"Use positional args when logging"},{"location":"Coding_Style_Guide/#exceptions-dont-allow-positional-args","text":"For some reason people tend to believe that using the logging / dassert approach of positional param to exceptions Bad (use positional args) python raise ValueError(\"Invalid server_name='%s'\", server_name) Good (use string interpolation) python raise ValueError(\"Invalid server_name='%s'\" % server_name) Best (use string format) python raise ValueError(f\"Invalid server_name='{server_name}'\") The constructor of an exception accepts a string Using the string f-format is best since It's more readable There is little time overhead since if you get to the exception probably the code is going to terminate, and it's not in a hot loop","title":"Exceptions don't allow positional args"},{"location":"Coding_Style_Guide/#report-warnings","text":"If there is a something that is suspicious but you don't feel like it's worthwhile to assert, report a warning with: python _LOG.warning(...) If you know that if there is a warning then there are going to be many many warnings Print the first warning Send the rest to warnings.log At the end of the run, reports \"there are warnings in warnings.log\"","title":"Report warnings"},{"location":"Coding_Style_Guide/#assertions","text":"","title":"Assertions"},{"location":"Coding_Style_Guide/#validate-values-before-an-assignment","text":"We consider this as an extension of a pre-condition (\"only assign values that are correct\") rather than a postcondition Often is more compact since it doesn't have reference to self Bad python self._tau = tau hdbg.dassert_lte(self._tau, 0) Good python hdbg.dassert_lte(tau, 0) self._tau = tau Exceptions When we handle a default assignment, it's more natural to implement a post-condition: python col_rename_func = col_rename_func or (lambda x: x) hdbg.dassert_isinstance(col_rename_func, collections.Callable)","title":"Validate values before an assignment"},{"location":"Coding_Style_Guide/#encode-the-assumptions-using-assertions","text":"If your code makes an assumption don\u2019t just write a comment, but implement an assertion so the code can\u2019t be executed if the assertion is not verified (instead of failing silently) python hdbg.dassert_lt(start_date, end_date)","title":"Encode the assumptions using assertions"},{"location":"Coding_Style_Guide/#use-positional-args-when-asserting","text":"dassert_* is modeled after logging so for the same reasons one should use positional args Bad python hdbg.dassert_eq(a, 1, \"No info for %s\" % method) Good python hdbg.dassert_eq(a, 1, \"No info for %s\", method)","title":"Use positional args when asserting"},{"location":"Coding_Style_Guide/#report-as-much-information-as-possible-in-an-assertion","text":"When using a dassert_* you want to give to the user as much information as possible to fix the problem E.g., if you get an assertion after 8 hours of computation you don't want to have to add some logging and run for 8 hours to just know what happened A dassert_* typically prints as much info as possible, but it can't report information that is not visible to it: Bad python hdbg.dassert(string.startswith(\"hello\")) You don't know what is value of string is Good python hdbg.dassert(string.startswith(\"hello\"), \"string='%s'\", string) Note that often is useful to add ' (single quotation mark) to fight pesky spaces that make the value unclear, or to make the error as readable as possible","title":"Report as much information as possible in an assertion"},{"location":"Coding_Style_Guide/#imports","text":"","title":"Imports"},{"location":"Coding_Style_Guide/#dont-use-evil-import","text":"Do not use in notebooks or code the evil import * Bad python from helpers.sql import * Good python import helpers.sql as hsql The from ... import * : Pollutes the namespace with the symbols and spreads over everywhere, making it painful to clean up Obscures where each function is coming from, removing the context that comes with the namespace Is evil in many other ways","title":"Don't use evil import *"},{"location":"Coding_Style_Guide/#cleaning-up-the-evil-import","text":"To clean up the mess you can: For notebooks Find & replace (e.g., using jupytext and Pycharm) Change the import and run one cell at the time For code Change the import and use linter on file to find all the problematic spots One of the few spots where the evil import * is ok is in the __init__.py to tweak the path of symbols exported by a library This is an advanced topic and you should rarely use it","title":"Cleaning up the evil import *"},{"location":"Coding_Style_Guide/#avoid-from-import","text":"Import should always start from import : python import library as short_name import library.sublibrary as short_name This rule applies to imports of third party libraries and our library Because of this rule we have to always specify a short import of a parent lib before every code object that does not belong to the file: Bad python from helpers.sql import get_connection, get_connection_from_env_vars, \\ DBConnection, wait_connection_from_db, execute_insert_query Good python import helpers.sql as hsql ... ... hsql.get_connection() The problem with the from ... import ... is that it: Creates lots of maintenance effort E.g., anytime you want a new function you need to update the import statement Creates potential collisions of the same name E.g., lots of modules have a read_data() function Impairs debugging Importing directly in the namespace loses information about the module E.g., read_documents() is not clear: what documents? np.read_documents() at least gives information of which packages is it coming from and enables us to track it down to the code","title":"Avoid from ... import ..."},{"location":"Coding_Style_Guide/#exceptions-to-the-import-style","text":"We try to minimize the exceptions to this rule to avoid to keep this rule simple, rather than discussing about The current agreed upon exceptions are: For typing it is ok to do: python from typing import Iterable, List in order to avoid typing everywhere, since we want to use type hints as much as possible","title":"Exceptions to the import style"},{"location":"Coding_Style_Guide/#always-import-with-a-full-path-from-the-root-of-the-repo-submodule","text":"Bad python import exchange_class Good python import im_v2.ccxt.data.extract.exchange_class In this way your code can run without depending upon your current dir","title":"Always import with a full path from the root of the repo / submodule"},{"location":"Coding_Style_Guide/#baptizing-module-import","text":"Each module that can be imported should have a docstring at the very beginning (before any code) describing how it should be imported ```python \"\"\" Import as: import im_v2.ccxt.data.client.ccxt_clients as imvcdccccl \"\"\" ``` The import abbreviations are called 'short imports' and usually consist of 7-9 first letters of all the words that comprise path to file DO NOT simply give a random short import name Run linter to generate short import for a file automatically For some most files we specify short imorts by hand so thay may contain less symbols, e.g., hdbg The goal is to have always the same imports so it's easy to move code around, without collisions","title":"Baptizing module import"},{"location":"Coding_Style_Guide/#examples-of-imports","text":"Example 1 Bad python from im_v2.ccxt.data.client import ccxt_clients as ccxtcl Good python import im_v2.ccxt.data.client.ccxt_clients as imvcdccccl Example 2 Bad python from edgar.shared import headers_extractor as he Good python import edgar.shared.headers_extractor as eshheext Example 3 Bad python from helpers import hdbg Good python import helpers.hdbg as hdbg","title":"Examples of imports"},{"location":"Coding_Style_Guide/#scripts","text":"","title":"Scripts"},{"location":"Coding_Style_Guide/#use-python-and-not-bash-for-scripting","text":"We prefer to use python instead of bash scripts with very few exceptions E.g., scripts that need to modify the environment by setting env vars, like setenv.sh The problem with bash scripts is that it's too easy to put together a sequence of commands to automate a workflow Quickly things always become more complicated than what you thought, e.g., You might want to interrupt if one command in the script fails You want to use command line options You want to use logging to see what's going on inside the script You want to do a loop with a regex check inside Thus you need to use the more complex features of bash scripting and bash scripting is absolutely horrible, much worse than perl (e.g., just think of if [ ... ] vs if [[ ... ]] ) Our approach is to make simple to create scripts in python that are equivalent to sequencing shell commands, so that can evolve in complex scripts","title":"Use Python and not bash for scripting"},{"location":"Coding_Style_Guide/#skeleton-for-a-script","text":"The ingredients are: dev_scripts/script_skeleton.py : a template to write simple scripts you can copy and modify it helpers/hsystem.py : a set of utilities that make simple to run shell commands (e.g., capturing their output, breaking on error or not, tee-ing to file, logging, ...) helpers has lots of useful libraries The official reference for a script is dev_scripts/script_skeleton.py You can copy this file and change it A simple example is: dev_scripts/git/gup.py A complex example is: dev_scripts/replace_text.py","title":"Skeleton for a script"},{"location":"Coding_Style_Guide/#some-useful-patterns","text":"Some useful patterns / idioms that are supported by the framework are: Incremental mode: you skip an action if its outcome is already present (e.g., skipping creating a dir, if it already exists and it contains all the results) Non-incremental mode: clean and execute everything from scratch Dry-run mode: the commands are written to screen instead of being executed","title":"Some useful patterns"},{"location":"Coding_Style_Guide/#use-scripts-and-not-notebooks-for-long-running-jobs","text":"We prefer to use scripts to execute code that might take long time (e.g., hours) to run, instead of notebooks Pros of script All the parameters are completely specified by a command line Reproducible and re-runnable Cons of notebooks Tend to crash / hang for long jobs Not easy to understand if the notebook is doing progress Not easy to get debug output Notebooks are designed for interactive computing / debugging and not batch jobs You can experiment with notebooks, move the code into a library, and wrap it in a script","title":"Use scripts and not notebooks for long-running jobs"},{"location":"Coding_Style_Guide/#follow-the-same-structure","text":"All python scripts that are meant to be executed directly should: Be marked as executable files with: > chmod +x foo_bar.py Have the python code should start with the standard Unix shebang notation: python #!/usr/bin/env python This line tells the shell to use the python defined in the environment In this way you can execute directly without prepending with python Have a: python if __name__ == \"__main__\": ... Ideally use argparse to have a minimum of customization","title":"Follow the same structure"},{"location":"Coding_Style_Guide/#use-clear-names-for-the-scripts","text":"In general scripts (like functions) should have a name like \"action_verb\". Bad Examples of bad script names are timestamp_extractor.py and timestamp_extractor_v2.py Which timestamp data set are we talking about? What type of timestamps are we extracting? What is the difference about these two scripts? We need to give names to scripts that help people understand what they do and the context in which they operate We can add a reference to the task that originated the work (to give more context) Good E.g., for a script generating a dataset there should be an (umbrella) bug for this dataset, that we refer in the bug name, e.g., TaskXYZ_edgar_timestamp_dataset_extractor.py Also where the script is located should give some clue of what is related to","title":"Use clear names for the scripts"},{"location":"Coding_Style_Guide/#functions","text":"","title":"Functions"},{"location":"Coding_Style_Guide/#avoid-using-non-exclusive-bool-arguments","text":"While a simple True / False switch may suffice for today's needs, very often more flexibility is eventually needed If more flexibility is needed for a bool argument, you are faced with the choice: Adding another parameter (then parameter combinations grow exponentially and may not all make sense) Changing the parameter type to something else Either way, you have to change the function interface To maintain flexibility from the start, opt for a str parameter \"mode\", which is allowed to take a small well-defined set of values. If an implicit default is desirable, consider making the default value of the parameter None . This is only a good route if the default operation is non-controversial / intuitively obvious.","title":"Avoid using non-exclusive bool arguments"},{"location":"Coding_Style_Guide/#try-to-make-functions-work-on-multiple-types","text":"We encourage implementing functions that can work on multiple related types: Bad : implement demean_series() , demean_dataframe() Good : implement a function demean(obj) that can work with pd.Series and pd.DataFrame One convention is to call obj the variable whose type is not known until run-time In this way we take full advantage of duck typing to achieve something similar to C++ function overloading (actually even more expressive) Try to return the same type of the input, if possible E.g., the function called on a pd.Series returns a pd.Series","title":"Try to make functions work on multiple types"},{"location":"Coding_Style_Guide/#avoid-hard-wired-column-name-dependencies","text":"When working with dataframes, we often want need handle certain columns differently, or perform an operation on a strict subset of columns In these cases, it is tempting to assume that the special columns will have specific names, e.g., \"datetime\" The problem is that column names are Rarely obvious (e.g., compare \"datetime\" vs \"timestamp\" vs \"Datetime\" ) Tied to specific use cases The function you are writing may be written for a specific use case today, but what if it is more general If someone wants to reuse your function in a different setting where different column names make sense, why should they have to conform to your specific use case's needs? May overwrite existing column names For example, you may decided to call a column \"output\" , but what if the dataframe already has a column with that name? To get around this, allow the caller to communicate to the function the names of any special columns Good python def func(datetime_col: str): ... Make sure that you require column names only if they are actually used by the function If you must use hard-write column names internally or for some application, define the column name in the library file as a global variable, like python DATETIME_COL = \"datetime\" Users of the library can now access the column name through imports This prevents hidden column name dependencies from spreading like a virus throughout the codebase","title":"Avoid hard-wired column name dependencies"},{"location":"Coding_Style_Guide/#single-exit-point-from-a-function","text":"Consider the following Bad function python def _get_zero_element(list_: List): if not list_: return None else: return list_[0] Linter message is im.kibot/utils.py:394: [R1705(no-else-return), ExpiryContractMapper.extract_contract_expiry] Unnecessary \"else\" after \"return\" [pylint] Try to have a single exit point from a function, since this guarantees that the return value is always the same In general returning different data structures from the same function (e.g., a list in one case and a float in another) is indication of bad design There are exceptions like a function that works on different types (e.g., accepts a dataframe or a series and then returns a dataframe or a series, but the input and output is the same) Returning different types (e.g., float and string) is also bad Returning a type or None is typically ok Try to return values that are consistent so that the client doesn't have to switch statement, using isinstance(...) E.g., return a float and if the value can't be computed return np.nan (instead of None ) so that the client can use the return value in a uniform way Function examples with single exit point python def _get_zero_element(list_: List): if not list_: ret = np.nan else: ret = list_[0] return ret or python def _get_zero_element(list_: List): ret = np.nan if not list_ else list_[0] return ret However in rare cases it is OK to have functions like: python def ...(...): # Handle simple cases. ... if ...: return # lots of code ... return","title":"Single exit point from a function"},{"location":"Coding_Style_Guide/#order-of-function-parameters","text":"","title":"Order of function parameters"},{"location":"Coding_Style_Guide/#problem","text":"We want to have a standard, simple, and logical order for specifying the arguments of a function","title":"Problem"},{"location":"Coding_Style_Guide/#decision","text":"The preferred order is: Input parameters Output parameters In-out parameters Default parameters","title":"Decision"},{"location":"Coding_Style_Guide/#consistency-of-ordering-of-function-parameters","text":"Try to: Keep related variables close to each other Keep the order of parameters similar across functions that have similar interface Enforcing these rules is based on best effort Pycharm is helpful when changing order of parameters Use linter to check consistency of types between function definition and invocation","title":"Consistency of ordering of function parameters"},{"location":"Coding_Style_Guide/#style-for-default-parameter","text":"","title":"Style for default parameter"},{"location":"Coding_Style_Guide/#problem_1","text":"How to assign default parameters in a function to make them clear and distinguishable?","title":"Problem"},{"location":"Coding_Style_Guide/#decision_1","text":"We make all the default parameters keyword-only This means that we should always specify default parameters using a keyword When building a function, always put default parameters after * It's ok to use a default parameter in the interface as long as it is a Python scalar (which is immutable by definition) Good python def function( value: int = 5, *, dir_name: str = \"hello_world\", ): You should not use list, maps, objects, etc. as the default value but pass None and then initialize the default param inside the function Bad python def function( *, obj: Object = Object(), list_: List[int] = [], ): Good python def function( *, obj: Optional[Object] = None, list_: Optional[List[int]] = None, ): if obj is None: obj = Object() if list_ is None: list_ = [] We use a None default value when a function needs to be wrapped and the default parameter needs to be propagated Good ```python def function1( ..., *, dir_name: Optional[str] = None, ): dir_name = dir_name or \"/very_long_path\" def function2( ..., *, dir_name: Optional[str] = None, ): function1(..., dir_name=dir_name) ```","title":"Decision"},{"location":"Coding_Style_Guide/#rationale","text":"Pros of the Good vs Bad style When you wrap multiple functions, each function needs to propagate the default parameters, which: - violates DRY; and - adds maintenance burden (if you change the innermost default parameter, you need to change all of them!) With the proposed approach, all the functions use None , until the innermost function resolves the parameters to the default values The interface is cleaner Implementation details are hidden (e.g., why should the caller know what is the default path?) Mutable parameters can not be passed through (see here )) Cons: One needs to add Optional to the type hint","title":"Rationale"},{"location":"Coding_Style_Guide/#calling-functions-with-default-parameters","text":"","title":"Calling functions with default parameters"},{"location":"Coding_Style_Guide/#problem_2","text":"You have a function python def func( task_name : str, dataset_dir : str, *, clobber : bool = clobber, ): ... How should it be invoked?","title":"Problem"},{"location":"Coding_Style_Guide/#decision_2","text":"We prefer to Assign directly the positional parameters Bind explicitly the parameters with a default value using their name Do not put actual parameter values to the function call but specify them right before Bad python func(\"some_task_name\", \"/dir/subdir\", clobber=False) Good python task_name = \"some_task_name\" dataset_dir = \"/dir/subdir\" clobber = False func(task_name, dataset_dir, clobber=clobber)","title":"Decision"},{"location":"Coding_Style_Guide/#rationale_1","text":"Pros of Good vs Bad style If a new parameter with a default value is added to the function func before clobber : The Good idiom doesn't need to be changed All instances of the Bad idiom need to be updated The Bad idiom might keep working but with silent failures Of course mypy and Pycharm might point this out The Good style highlights which default parameters are being overwritten, by using the name of the parameter Overwriting a default parameter is an exceptional situation that should be explicitly commented Cons: None","title":"Rationale"},{"location":"Coding_Style_Guide/#dont-repeat-non-default-parameters","text":"","title":"Don't repeat non-default parameters"},{"location":"Coding_Style_Guide/#problem_3","text":"Given a function with the following interface: python def mult_and_sum(multiplier_1, multiplier_2, sum_): return multiplier_1 * multiplier_2 + sum_ how to invoke it?","title":"Problem"},{"location":"Coding_Style_Guide/#decision_3","text":"Positional arguments are not default, so not keyword-only for consistency Bad python a = 1 b = 2 c = 3 mult_and_sum(multiplier_1=a, multiplier_2=b, sum_=c) Good python a = 1 b = 2 c = 3 mult_and_sum(a, b, c)","title":"Decision"},{"location":"Coding_Style_Guide/#rationale_2","text":"Pros of Good vs Bad Non-default parameters in Python require all the successive parameters to be name-assigned This causes maintenance burden The Bad approach is in contrast with our rule for the default parameters We want to highlight which parameters are overriding the default The Bad approach in practice requires all positional parameters to be assigned explicitly causing: Repetition in violation of DRY (e.g., you need to repeat the same parameter everywhere); and Maintainance burden (e.g., if you change the name of a function parameter you need to change all the invocations) The Bad style is a convention used in no language (e.g., C, C++, Java) All languages allow binding by parameter position Only some languages allow binding by parameter name The Bad makes the code very wide, creating problems with our 80 columns rule Cons of Good vs Bad One could argue that the Bad form is clearer IMO the problem is in the names of the variables, which are uninformative, e.g., a better naming achieves the same goal of clarity python mul1 = 1 mul2 = 2 sum_ = 3 mult_and_sum(mul1, mul2, sum_)","title":"Rationale"},{"location":"Coding_Style_Guide/#writing-clear-beautiful-code","text":"","title":"Writing clear beautiful code"},{"location":"Coding_Style_Guide/#keep-related-code-close","text":"E.g., keep code that computes data close to the code that uses it. This holds also for notebooks: do not compute all the data structure and then analyze them. It\u2019s better to keep the section that \u201creads data\u201d close to the section that \u201cprocesses it\u201d. In this way it\u2019s easier to see \u201cblocks\u201d of code that are dependent from each other, and run only a cluster of cells.","title":"Keep related code close"},{"location":"Coding_Style_Guide/#order-functions-in-topological-order","text":"Order functions / classes in topological order so that the ones at the top of the files are the \"innermost\" and the ones at the end of the files are the \"outermost\" In this way, reading the code top to bottom one should not find a forward reference that requires skipping back and forth Linter reorders functions and classes in the topological order so make sure you run it after adding new ones","title":"Order functions in topological order"},{"location":"Coding_Style_Guide/#distinguish-public-and-private-functions","text":"The public functions foo_bar() (not starting with _ ) are the ones that make up the interface of a module and that are called from other modules and from notebooks Use private functions like _foo_bar() when a function is a helper of another private or public function Also follow the \u201ckeep related code close\u201d close by keeping the private functions close to the functions (private or public) that are using them Some references: StackOverflow","title":"Distinguish public and private functions"},{"location":"Coding_Style_Guide/#keep-public-functions-organized-in-a-logical-order","text":"Keep the public functions in an order related to the use representing the typical flow of use, e.g., Common functions, used by all other functions Read data Process data Save data You can use banners to separate layers of the code. Use the banner long 80 cols (e.g., I have a vim macro to create banners that always look the same) and be consistent with empty lines before / empty and so on. The banner is a way of saying \u201call these functions belong together\u201d. ```python # ############################################################################# # Read data. # ############################################################################# def _helper1_to_func1(): ... def _helper2_to_func1(): ... def func1_read_data1(): _helper1_to_func1() ... _helper2_to_func2() # ############################################################################# # Process data. # ############################################################################# ... # ############################################################################# # Save data. # ############################################################################# ... ``` Ideally each section of code should use only sections above, and be used by sections below (aka \u201cUnix layer approach\u201d). If you find yourself using too many banners this is in indication that code might need to be split into different classes or files Although we don\u2019t have agreed upon rules, it might be ok to have large files as long as they are well organized. E.g., in pandas code base, all the code for DataFrame is in a single file long many thousands of lines (!), but it is nicely separated in sections that make easy to navigate the code Too many files can become problematic, since one needs to start jumping across many files: in other words it is possible to organize the code too much (e.g. what if each function is in a single module?) Let\u2019s try to find the right balance. It might be a good idea to use classes to split the code, but also OOP can have a dark side E.g., using OOP only to reorganize the code instead of introducing \u201cconcepts\u201d IMO the worst issue is that they don\u2019t play super-well with Jupyter autoreload","title":"Keep public functions organized in a logical order"},{"location":"Coding_Style_Guide/#do-not-make-tiny-wrappers","text":"Examples of horrible functions: How many characters do we really saved? If typing is a problem, learn to touch type. python def is_exists(path: str) -> None: return os.path.exists(path) or python def make_dirs(path: str) -> List[str]: os.makedirs(path) This one can be simply replaced by os.path.dirname python def folder_name(f_name: str) -> str: if f_name[-1] != \"/\": return f_name + \"/\" return f_name","title":"Do not make tiny wrappers"},{"location":"Coding_Style_Guide/#regex","text":"The rule of thumb is to compile a regex expression, e.g., python backslash_regex = re.compile(r\"\\\\\") only if it's called more than once, otherwise the overhead of compilation and creating another var is not justified","title":"Regex"},{"location":"Coding_Style_Guide/#do-not-introduce-another-concept-unless-really-needed","text":"We want to introduce degrees of freedom and indirection only when we think this can be useful to make the code easy to maintain, read, and expand. If we add degrees of freedom everywhere just because we think that at some point in the future this might be useful, then there is very little advantage and large overhead. Introducing a new variable, function, class introduces a new concept that one needs to keep in mind. People that read the code, needs to go back and forth in the code to see what each concept means. Think about the trade-offs and be consistent. Example 1 python def fancy_print(txt): print \"fancy: \", txt Then people that change the code need to be aware that there is a function that prints in a special way. The only reason to add this shallow wrapper is that, in the future, we believe we want to change all these calls in the code. Example 2 python SNAPSHOT_ID = \"SnapshotId\" Another example is parametrizing a value used in a single function. If multiple functions need to use the same value, then this practice can be a good idea. If there is a single function using this, one should at least keep it local to the function. Still note that introducing a new concept can also create confusion. What if we need to change the code to: python SNAPSHOT_ID = \"TigerId\" then the variable and its value are in contrast.","title":"Do not introduce another \u201cconcept\u201d unless really needed"},{"location":"Coding_Style_Guide/#return-none-or-keep-one-type","text":"Functions that return different types can make things complicated downstream, since the callers need to be aware of all of it and handle different cases. This also complicates the docstring, since one needs to explicitly explain what the special values mean, all the types and so on. In general returning multiple types is an indication that there is a problem. Of course this is a trade-off between flexibility and making the code robust and easy to understand, e.g., In the following example it is better to either return None (to clarify that something special happened) or an empty dataframe pd.DataFrame(None) to allow the caller code being indifferent to what is returned. Bad python if \"Tags\" not in df.columns: df[\"Name\"] = np.nan else: df[\"Name\"] = df[\"Tags\"].apply(extract_name) Good python if \"Tags\" not in df.columns: df[\"Name\"] = None else: df[\"Name\"] = df[\"Tags\"].apply(extract_name)","title":"Return None or keep one type"},{"location":"Coding_Style_Guide/#avoid-wall-of-text-functions","text":"Bad python def get_timestamp_data(raw_df: pd.DataFrame) -> pd.DataFrame: timestamp_df = get_raw_timestamp(raw_df) documents_series = raw_df.progress_apply(he.extract_documents_v2, axis=1) documents_df = pd.concat(documents_series.values.tolist()) documents_df.set_index(api.cfg.INDEX_COLUMN, drop=True, inplace=True) documents_df = documents_df[ documents_df[api.cfg.DOCUMENTS_DOC_TYPE_COL] != \"\"] types = documents_df.groupby( api.cfg.DOCUMENTS_IDX_COL)[api.cfg.DOCUMENTS_DOC_TYPE_COL].unique() timestamp_df[api.cfg.TIMESTAMP_DOC_TYPES_COL] = types is_xbrl_series = raw_df[api.cfg.DATA_COLUMN].apply(he.check_xbrl) timestamp_df[api.cfg.TIMESTAMP_DOC_ISXBRL_COL] = is_xbrl_series timestamp_df[api.cfg.TIMESTAMP_DOC_EX99_COL] = timestamp_df[ api.cfg.TIMESTAMP_DOC_TYPES_COL].apply( lambda x: any(['ex-99' in t.lower() for t in x])) timestamp_df = timestamp_df.rename(columns=api.cfg.TIMESTAMP_COLUMN_RENAMES) return timestamp_df This function is correct but it has few problems (e.g., lack of a docstring, lots of unclear concepts, abuse of constants). Good ```python def get_timestamp_data(raw_df: pd.DataFrame) -> pd.DataFrame: \"\"\" Get data containing timestamp information. :param raw_df: input non-processed data :return: timestamp data \"\"\" # Get data containing raw timestamp information. timestamp_df = get_raw_timestamp(raw_df) # Extract the documents with data type information. documents_series = raw_df.progress_apply(he.extract_documents_v2, axis=1) documents_df = pd.concat(documents_series.values.tolist()) documents_df.set_index(api.cfg.INDEX_COLUMN, drop=True, inplace=True) documents_df = documents_df[ documents_df[api.cfg.DOCUMENTS_DOC_TYPE_COL] != \"\" ] types = documents_df.groupby( api.cfg.DOCUMENTS_IDX_COL )[api.cfg.DOCUMENTS_DOC_TYPE_COL].unique() # Set columns about types of information contained. timestamp_df[api.cfg.TIMESTAMP_DOC_TYPES_COL] = types is_xbrl_series = raw_df[api.cfg.DATA_COLUMN].apply(he.check_xbrl) timestamp_df[api.cfg.TIMESTAMP_DOC_ISXBRL_COL] = is_xbrl_series timestamp_df[api.cfg.TIMESTAMP_DOC_EX99_COL] = timestamp_df[ api.cfg.TIMESTAMP_DOC_TYPES_COL ].apply(lambda x: any([\"ex-99\" in t.lower() for t in x])) # Rename columns to canonical representation. timestamp_df = timestamp_df.rename(columns=api.cfg.TIMESTAMP_COLUMN_RENAMES) return timestamp_df ``` You should at least split the functions in chunks using # or even better comment what each chunk of code does.","title":"Avoid wall-of-text functions"},{"location":"Coding_Style_Guide/#writing-robust-code","text":"","title":"Writing robust code"},{"location":"Coding_Style_Guide/#dont-let-your-functions-catch-the-default-itis","text":"Default-itis is a disease of a function that manifests itself by getting too many default parameters. Default params should be used only for parameters that 99% of the time are constant. In general we require the caller to be clear and specify all the params. Functions catch defaultitis when the programmer is lazy and wants to change the behavior of a function without changing all the callers and unit tests. Resist this urge! grep is friend. Pycharm does this refactoring automatically.","title":"Don\u2019t let your functions catch the default-itis"},{"location":"Coding_Style_Guide/#explicitly-bind-default-parameters","text":"It\u2019s best to explicitly bind functions with the default params so that if the function signature changes, your functions doesn\u2019t confuse a default param was a positional one. Bad python hdbg.dassert( args.form or args.form_list, \"You must specify one of the parameters: --form or --form_list\", ) Good python hdbg.dassert( args.form or args.form_list, msg=\"You must specify one of the parameters: --form or --form_list\", )","title":"Explicitly bind default parameters"},{"location":"Coding_Style_Guide/#dont-hardwire-params-in-a-function-call","text":"Bad python esa_df = universe.get_esa_universe_mapped(False, True) It is difficult to read and understand without looking for the invoked function (aka write-only code) and it\u2019s brittle since a change in the function params goes unnoticed. Good python gvkey = False cik = True esa_df = universe.get_esa_universe_mapped(gvkey, cik) It\u2019s better to be explicit (as usual) This solution is robust since it will work as long as gvkey and cik are the only needed params, which is as much as we can require from the called function.","title":"Don\u2019t hardwire params in a function call"},{"location":"Coding_Style_Guide/#make-if-elif-else-complete","text":"In general all the if-elif-else statements should to be complete, so that the code is robust. Bad python hdbg.dassert_in( frequency, [\"D\", \"T\"] \"Only daily ('D') and minutely ('T') frequencies are supported.\", ) if frequency == \"T\": ... if frequency == \"D\": ... Good python if frequency == \"T\": ... elif frequency == \"D\": ... else: raise ValueError(\"The %s frequency is not supported\" % frequency) This code is robust and correct Still the if-elif-else is enough and the assertion is not needed DRY here wins: you don't want to have to keep two pieces of code in sync The last line is a catch-all that makes sure even if we modify the previous It makes sense to check early only when you want to fail before doing more work E.g., sanity checking the parameters of a long running function, so that it doesn't run for 1 hr and then crash because the name of the file is incorrect","title":"Make if-elif-else complete"},{"location":"Coding_Style_Guide/#add-todos-when-needed","text":"When there is something that you know you should have done, but you didn\u2019t have time to do, add a TODO, possibly using your github name e.g., python # TODO(gp): \u2026 In this way it\u2019s easy to grep for your TODOs, which becomes complicated when using different names. Be clear on the meaning of TODO A TODO(Batman): clean this up can be interpreted as \"Batman suggested to clean this up\" \"Batman should clean this up\" \"Batman has the most context to explain this problem or fix it\" On the one hand, git blame will report who created the TODO, so the first meaning is redundant. On the other hand, since we follow a shared ownership of the code, the second meaning should be quite infrequent. In fact the code has mostly TODO(*) todos, where * relates to all the team members Given pros and cons, the proposal is to use the first meaning. This is also what Google style guide suggests here If the TODO is associated with a Github issue, you can simply put the issue number and description inside the TODO, e.g., python # TODO(Grisha): \"Handle missing tiles\" CmTask #1775. You can create a TODO for somebody else, or you can create a Upsource comment / review or Github bug, depending on how important the issue is If the TODO is general, e.g., anybody can fix it, then you can avoid to put a name. This should not be abused since it creates a culture when people don\u2019t take responsibility for their mistakes. You can use P1, P2 to indicate if the issue is critical or not. E.g., P0 is the default for saying this is important, P1 is more of a \u201cnice to have\u201d. python # TODO(Sergey): P1 This can be implemented in pandas using a range generation.","title":"Add TODOs when needed"},{"location":"Coding_Style_Guide/#common-python-mistakes","text":"","title":"Common Python mistakes"},{"location":"Coding_Style_Guide/#vs-is","text":"is checks whether two variables point to the same object (aka reference equality), while == checks if the two pointed objects are equivalent (value equality). For checking against types like None we want to use is , is not Bad python if var == None: _Good__ python if var is None: For checking against values we want to use == Bad python if unit is \"minute\": _Good__ python if unit == \"minute\": For more info checks here","title":"== vs is"},{"location":"Coding_Style_Guide/#type-vs-isinstance","text":"type(obj) == list is worse since we want to test for reference equality (the type of object is a list) and not the type of obj is equivalent to a list. isinstance caters for inheritance (an instance of a derived class is an instance of a base class, too), while checking for equality of type does not (it demands identity of types and rejects instances of subtypes, AKA subclasses). Bad python if type(obj) is list: _Good__ python if isinstance(obj, list): For more info check here","title":"type() vs isinstance()"},{"location":"Coding_Style_Guide/#unit-tests","text":"Provide a minimal end-to-end unit testing (which creates a conda environment and then run a few unit tests) Use Pytest https://docs.pytest.org/en/latest/ unittest library Usually we are happy with Lightly testing the tricky functions Some end-to-end test to make sure the code is working Use your common sense E.g., no reason to test code that will be used only once To run unit tests in a single file ``` pytest datetime_utils_test.py -x -s ``` TODO(Dan/Samarth): Add a link to unit test doc when it is converted to md.","title":"Unit tests"},{"location":"Coding_Style_Guide/#convention-for-naming-tests","text":"According to PEP8 names of classes should always be camel case. On the other hand, if we are testing a function foo_bar() we prefer to call the testing code Test_foo_bar instead of TestFooBar . We suggest to name the class / method in the same way as the object we are testing, e.g.,: For testing a class FooBar we use a test class TestFooBar For testing methods of the class FooBar , e.g., FooBar.baz() , we use a test method TestFooBar.test_baz() For testing a protected method _gozilla() of FooBar we use test methods test__gozilla (note the double underscore). This is needed to distinguish testing the public method FooBar.gozilla() and FooBar._gozilla() We are ok with mixing camel case and snake case to mirror the code being tested. We prefer to name classes TestFooBar1 and methods TestFooBar1.test1() , even if there is a single class / method, to make it easier to add another test class, without having to rename class and check_string files. We are ok with using suffixes like 01 , 02 , \u2026 , when we believe it's important that methods are tested in a certain order (e.g., from the simplest to the most complex)","title":"Convention for naming tests"},{"location":"Coding_Style_Guide/#refactoring","text":"","title":"Refactoring"},{"location":"Coding_Style_Guide/#when-moving-refactoring-code","text":"If you move files, refactor code, move functions around make sure that: Code and notebook work (e.g., imports and caller of the functions) Documentation is updated (this is difficult, so best effort is enough) For code find all the places that have been modified ``` grep -r \"create_dataframe\" * edgar/form_4/notebooks/Task252_EDG4_Coverage_of_our_universe_from_Forms4.ipynb: \"documents, transactions = edu.create_dataframes(\\n\", edgar/form_4/notebooks/Task313_EDG4_Understand_Form_4_amendments.ipynb: \"documents, transactions = edu.create_dataframes(\\n\", edgar/form_4/notebooks/Task193_EDG4_Compare_form4_against_Whale_Wisdom_and_TR.ipynb: \"documents, transactions, owners, footnotes = edu.create_dataframes(\\n\", ``` Or if you use mighty Pycharm, Ctrl + Mouse Left Click (Shows you all places where this function or variable was used) and try to fix them, at least to give your best shot at making things work You can edit directly the notebooks without opening, or open and fix it. Good examples how you can safely rename anything for Pycharm users: https://www.jetbrains.com/help/Pycharm/rename-refactorings.html But remember, you must know how to do it without fancy IDE like Pycharm. If it\u2019s important code: Run unit tests Run notebooks (see here )","title":"When moving / refactoring code"},{"location":"Coding_Style_Guide/#write-script-for-renamings","text":"When you need to rename any code object that is being used in many files, use dev_scripts/replace_text.py to write a script that will implement your task Read the script docstring for detailed information about how to use it You DO NOT use replace_text.py directly. Instead, create an executable .sh script that uses replace_text.py Look for examples at dev_scripts/cleanup_scripts Commit the created script to the mentioned folder so then your team members can use it to implement renaming in other libs","title":"Write script for renamings"},{"location":"Coding_Style_Guide/#architectural-and-design-pattern","text":"","title":"Architectural and design pattern"},{"location":"Coding_Style_Guide/#research-quality-vs-production-quality","text":"Code belonging to top level libraries (e.g., //amp/core , //amp/helpers ) and production (e.g., //.../db , vendors ) needs to meet high quality standards, e.g., Well commented Following our style guide Thoroughly reviewed Good design Comprehensive unit tests Research code in notebook and python can follow slightly looser standards, e.g., Sprinkled with some TODOs Not perfectly general The reason is that: Research code is still evolving and we want to keep the structure flexible We don't want to invest the time in making it perfect if the research doesn't pan out Note that research code still needs to be: Understandable / usable by not authors Well commented Follow the style guide Somehow unit tested We should be able to raise the quality of a piece of research code to production quality when that research goes into production","title":"Research quality vs production quality"},{"location":"Coding_Style_Guide/#always-separate-what-changes-from-what-stays-the-same","text":"In both main code and unit test it's not a good idea to repeat the same code Bad Copy-paste-modify Good Refactor the common part in a function and then change the parameters used to call the function Example: What code is clearer to you, VersionA or VersionB? Can you spot the difference between the 2 pieces of code? Version A ```python stopwords = nlp_ut.read_stopwords_json(_STOPWORDS_PATH) texts = [\"a\", \"an\", \"the\"] stop_words = nlp_ut.get_stopwords( categories=[\"articles\"], stopwords=stopwords ) actual_result = nlp_ut.remove_tokens(texts, stop_words=stop_words) expected_result = [] self.assertEqual(actual_result, expected_result) ... texts = [\"do\", \"does\", \"is\", \"am\", \"are\", \"be\", \"been\", \"'s\", \"'m\", \"'re\"] stop_words = nlp_ut.get_stopwords( categories=[\"auxiliary_verbs\"], stopwords=stopwords, ) actual_result = nlp_ut.remove_tokens(texts, stop_words=stop_words) expected_result = [] self.assertEqual(actual_result, expected_result) ``` Version B ```python def _helper(texts, categories, expected_result): stopwords = nlp_ut.read_stopwords_json(_STOPWORDS_PATH) stop_words = nlp_ut.get_stopwords( categories=categories, stopwords=stopwords ) actual_result = nlp_ut.remove_tokens(texts, stop_words=stop_words) expected_result = [] self.assertEqual(actual_result, expected_result) texts = [\"a\", \"an\", \"the\"] categories = [\"articles\"] expected_result = [] _helper(texts, categories, expected_result) ... texts = [\"do\", \"does\", \"is\", \"am\", \"are\", \"be\", \"been\", \"'s\", \"'m\", \"'re\"] categories = [\"auxiliary_verbs\"] expected_result = [] _helper(texts, categories, expected_result) ``` Yes, Version A is Bad and Version B is Good","title":"Always separate what changes from what stays the same"},{"location":"Coding_Style_Guide/#organize-scripts-as-pipelines","text":"One can organize complex computations in stages of a pipeline E.g., to parse EDGAR forms Download -> (raw data) -> header parser -> (pq data) -> XBLR / XML / XLS parser -> (pq data) -> custom transformation One should be able to run the entire pipeline or just a piece E.g., one can run the header parser from the raw data, save the result to file, then read this file back, and run the XBLR parser Ideally one would always prefer to run the pipeline from scratch, but sometimes the stages are too expensive to compute over and over, so using chunks of the pipeline is better This can also mixed with the \u201cincremental mode\u201d, so that if one stage has already been run and the intermediate data has been generated, that stage is skipped Each stage can save files in a tmp_dir/stage_name The code should be organized to allow these different modes of operations, but there is not always need to be super exhaustive in terms of command line options E.g., I implement the various chunks of the pipeline in a library, separating functions that read / save data after a stage and then assemble the pieces into a throw-away script where I hardwire the file names and so on","title":"Organize scripts as pipelines"},{"location":"Coding_Style_Guide/#make-filename-unique","text":"Problem We have a lot of structure / boilerplate in our project around RH hypotheses. E.g., there are corresponding files for all the RH like: RHxyz/configs.py RHxyz/pipeline.py It is not clear if it's better to make filenames completely unique by repeating the RH , e.g., RH1E_configs.py , or let the directories disambiguate. Note that we are not referring to other common files like utils.py , which are made unique by their position in the file system and by the automatic shortening of the imports. Decision Invoking the principle of 'explicit is better than implicit', the proposal is to repeat the prefix. Bad : RH1E/configs.py Good : RH1E/RH1E_configs.py Rationale Pros of the repetition (e.g., RH1E/RH1E_configs.py ): The filename is unique so there is no dependency on where you are Since pytest requires all files to be unique, we need to repeat the prefix for the test names and the rule is \"always make the names of the files unique\" We are going to have lots of these files and we want to minimize the risk of making mistakes Cons of the repetition: Stuttering What happens if there are multiple nested dirs? Do we repeat all the prefixes? This seems to be an infrequent case","title":"Make filename unique"},{"location":"Coding_Style_Guide/#incremental-behavior","text":"Often we need to run the same code over and over E.g., because the code fails on an unexpected point and then we need to re-run from the beginning We use options like: --incremental --force --start_date --end_date --output_file Check existence output file before start function (or a thread when using parallelism) which handle data of the corresponding period If --incremental is set and output file already exists then skip the computation and report Log.info(\u201cSkipping processing file %s as requested\u201d, \u2026) If --incremental is not set If output file exists then we issue a log.warn and abort the process If output file exists and param --force , then report a log.warn and rewrite output file","title":"Incremental behavior"},{"location":"Coding_Style_Guide/#run-end-to-end","text":"Try to run things end-to-end (and from scratch) so we can catch these unexpected issues and code defensively E.g., we found out that TR data is malformed sometimes and only running end-to-end we can catch all the weird cases This also helps with scalability issues, since if takes 1 hr for 1 month of data and we have 10 years of data is going to take 120 hours (=5 days) to run on the entire data set","title":"Run end-to-end"},{"location":"Coding_Style_Guide/#think-about-scalability","text":"Do experiments to try to understand if a code solution can scale to the dimension of the data we have to deal with E.g., inserting data by doing SQL inserts of single rows are not scalable for pushing 100GB of data Remember that typically we need to run the same scripts multiple times (e.g., for debug and / or production)","title":"Think about scalability"},{"location":"Coding_Style_Guide/#use-command-line-for-reproducibility","text":"Try to pass params through command line options when possible In this way a command line contains all the set-up to run an experiment","title":"Use command line for reproducibility"},{"location":"Coding_Style_Guide/#structure-the-code-in-terms-of-filters","text":"Focus on build a set of \"filters\" split into different functions, rather than a monolithic flow Organize the code in terms of a sequence of transformations that can be run in sequence, e.g., Create SQL tables Convert json data to csv Normalize tables Load csv files into SQL Sanity check the SQL (e.g., mismatching TR codes, missing dates) Patch up SQL (e.g., inserting missing TR codes and reporting them to us so we can check with TR)","title":"Structure the code in terms of filters"},{"location":"Coding_Style_Guide/#code-style-for-different-languages","text":"","title":"Code style for different languages"},{"location":"Coding_Style_Guide/#sql","text":"You can use the package https://github.com/andialbrecht/sqlparse to format SQL queries There is also an on-line version of the same formatter at https://sqlformat.org","title":"SQL"},{"location":"Coding_Style_Guide/#conventions-addendum","text":"","title":"Conventions (Addendum)"},{"location":"Coding_Style_Guide/#be-patient","text":"For some reason talking about conventions makes people defensive and uncomfortable, sometimes. Conventions are not a matter of being right or wrong, but to consider pros and cons of different approaches, and make the decision only once instead of discussing the same problem every time. In this way we can focus on achieving the Ultimate Goal. If you are unsure or indifferent to a choice, be flexible and let other persons that seem to be less flexible decide.","title":"Be patient"},{"location":"Coding_Style_Guide/#goal","text":"The goal of the conventions is to simplify our job by removing ambiguity There is no right or wrong: that's why it's a convention and not a law of nature On the flip-side, if there is a right and wrong, then what we are discussing probably shouldn\u2019t be considered as a convention We don't want to spend time discussing inconsequential points We don't want reviewers to pick lints instead of focusing on architectural issues and potential bugs Remove cognitive burden of being distracted by \"this is an annoying lint\" (or at least perceived lint) Once a convention is stable, we would like to automate enforcing it by the linter Ideally the linter should fix our mistakes so we don't even have to think about them, and reviewers don't have to be distracted with pointing out the lints","title":"Goal"},{"location":"Coding_Style_Guide/#keep-the-rules-simple","text":"E.g., assume that we accepted the following rules: Git is capitalized if it refers to the tool and it's not capitalized when it refers to the command (this is what Git documentation suggests) Python is written capitalized (this is what Python documentation suggests) pandas is written lowercase, unless it is a beginning of the line in which case it's capitalized, but it's better to try to avoid to start a sentence with it (this is what pandas + English convention seems to suggest) Any other library could suggest a different convention based on the preference of its author, who tries to finally force people to follow his / her convention \u2026) All these rules require mental energy to be followed and readers will spend time checking that these rules are enforced, rather than focusing on bugs and architecture. In this case we want to leverage the ambiguity of \"it's unclear what is the correct approach\" by simplifying the rule E.g., every name of tools or library is always capitalized This is simple to remember and automatically enforce","title":"Keep the rules simple"},{"location":"Coding_Style_Guide/#allow-turning-off-the-automatic-tools","text":"We understand that tools can't always understand the context and the subtleties of human thoughts, and therefore they yield inevitably to false positives. Then we always want to permit disabling the automatic checks / fixes e.g., by using directives in comments or special syntax (e.g., anything in a ... or \u2026 block should be leaved untouched) It can be tricky determining when an exception is really needed and when overriding the tool becomes a slippery slope for ignoring the rules. Patience and flexibility is advised here.","title":"Allow turning off the automatic tools"},{"location":"Coding_Style_Guide/#make-the-spell-checker-happy","text":"The spell-checker is not always right: false positives are often very annoying We prefer to find a way to make the spell-checker happy rather than argue that the spell-checker is wrong and ignore it The risk with overriding the spell-checker (and any other tool) is that the decision is not binary anymore correct / not-correct and can't be automated and requires mental energy to see if the flagged error is real or not. E.g., insample is flagged as erroneous, so we convert it into in-sample . The solution for the obvious cases of missing a word (e.g., a technical word) is to add words to the vocabulary. This still needs to be done by everyone, until we find a way to centralize the vocabulary. E.g., untradable is a valid English word, but Pycharm's spell-checker doesn't recognize it. TODO(*): Should we add it to the dictionary or write it as \"un-tradable\"? Still we don't want to override the spell-checker when an alternative lower-cost solution is available. E.g., in-sample instead of insample out-of-sample instead of oos We decided that hyper-parameter can be written without hyphen: hyperparameter","title":"Make the spell-checker happy"},{"location":"Contributor_Scorecard/","text":"Contributor Scorecard General Some reminders Grades General We give feedback about every 2 weeks The scores are published here Some reminders The goal of this internship is to bridge the gap between college and the workplace. We try to give you a preview of how your skills will be evaluated and prepare you for being hired. As you have seen, we spend a lot of time helping you improve your skills through code reviews, writing documentation, suggesting resources to read ... etc. The purpose of these evaluations is to support your professional development. Embracing feedback positively and actively working on areas that need improvement will not only benefit your internship but also contribute to your overall growth as a professional Multiple people review your work and assess it as objectively as possible, so please don't question your scores, but rather try to understand why you receive the feedback and focus on how to improve the scores We are a non-profit open source project so we have limited capacity and time. We give an opportunity to contribute to everyone, but we can't support contributors that don't contribute enough to the project Grades A score of 4 or higher is considered contributing, indicating a good performance If your score falls below 4, please focus on improving in areas you are lacking. Your mentor (indicated in the Google Sheet) can provide you guidance for improving on Telegram. Feel free to have a short discussion with them Contributors that have a score less than 3 will be given a warning and if they can't increase their score, they will be dropped from the project. Our decision is final, everybody had a fair shot, but we can't have dragged on discussions. In life, you have only one chance to give a first impression. After you are dropped, you can take 6 months off to improve your skills and then apply to contribute for the project again","title":"Contributor Scorecard"},{"location":"Contributor_Scorecard/#contributor-scorecard","text":"General Some reminders Grades","title":"Contributor Scorecard"},{"location":"Contributor_Scorecard/#general","text":"We give feedback about every 2 weeks The scores are published here","title":"General"},{"location":"Contributor_Scorecard/#some-reminders","text":"The goal of this internship is to bridge the gap between college and the workplace. We try to give you a preview of how your skills will be evaluated and prepare you for being hired. As you have seen, we spend a lot of time helping you improve your skills through code reviews, writing documentation, suggesting resources to read ... etc. The purpose of these evaluations is to support your professional development. Embracing feedback positively and actively working on areas that need improvement will not only benefit your internship but also contribute to your overall growth as a professional Multiple people review your work and assess it as objectively as possible, so please don't question your scores, but rather try to understand why you receive the feedback and focus on how to improve the scores We are a non-profit open source project so we have limited capacity and time. We give an opportunity to contribute to everyone, but we can't support contributors that don't contribute enough to the project","title":"Some reminders"},{"location":"Contributor_Scorecard/#grades","text":"A score of 4 or higher is considered contributing, indicating a good performance If your score falls below 4, please focus on improving in areas you are lacking. Your mentor (indicated in the Google Sheet) can provide you guidance for improving on Telegram. Feel free to have a short discussion with them Contributors that have a score less than 3 will be given a warning and if they can't increase their score, they will be dropped from the project. Our decision is final, everybody had a fair shot, but we can't have dragged on discussions. In life, you have only one chance to give a first impression. After you are dropped, you can take 6 months off to improve your skills and then apply to contribute for the project again","title":"Grades"},{"location":"DataFlow/","text":"DataFlow specification Config Config . A Config is a dictionary-like object that represents parameters used to build and configure other objects (e.g., a DAG or a System). Each config is a hierarchical structure which consists of Subconfigs and Leaves . Subconfig is a nested object which represents a Config inside another config. A Subconfig of a Subconfig is a Subconfig of a Config, i.e. the relation is transitive. Leaf is any object inside a Config that is used to build another object that is not in itself a Config. Note that a dictionary or other mapping objects are not permitted inside a Config: each dictionary-like object should be converted to a Config and become a Subconfig. Config representation and properties A Config can be represented as a dictionary or a string. Example of a dictionary representation: config1={ \"resample_1min\": False, \"client_config\": { \"universe\": { \"full_symbols\": [\"binance::ADA_USDT\"], \"universe_version\": \"v3\", }, }, \"market_data_config\": {\"start_ts\": start_ts, \"end_ts\": end_ts}, } In the example above: - \"resample_1min\" is a leaf of the config1 - \"client_config\" is a subconfig of config1 - \"universe\" is a subconfig of \"client_config\" - \"market_data\" config is a subconfig of \"config1\" - \"start_ts\" and \"end_ts\" are leaves of \"market_data_config\" and config1 Example of a string representation: {width=\"6.854779090113736in\" height=\"1.2303444881889765in\"} The same values are annotated with marked_as_used , writer and val_type marked_as_used determines whether the object was used to construct another object writer provides a stacktrace of the piece of code which marked the object as used val_type is a type of the object Assigning and getting Config items Config object has its own implementations of __setitem__ and __getitem__ A new value can be set freely like in a python Dict object Overwriting the value is prohibited if the value has already been used Since Config is used to guarantee that the construction of any objects is reproducible, there are 2 methods to get the value. get_and_mark_as_used is utilized when a leaf of the config is used to construct another object When the value is used inside a constructor When the value is used as a parameter in a function Note that when selecting a subconfig the subconfig itself is not marked as used, but its leaves are. For this reason, the user should avoid marking subconfigs as used and instead select leaves separately. Example of marking the subconfig as used: _ = config.get_and_mark_as_used(\"market_data_config\") {width=\"6.5in\" height=\"1.1944444444444444in\"} Example of marking the leaf as used: _ = config.get_and_mark_as_used((\"market_data_config\", \"end_ts\")) {width=\"6.5in\" height=\"1.1388888888888888in\"} __getitem__ is used to select items for uses which do not affect the construction of other objects: Logging, debugging and printing Time semantics Time semantics . A DataFlow component can be executed or simulated accounting for different ways to represent the passing of time. E.g., it can be simulated in a timed or non-timed simulation, depending on how data is delivered to the system (as it is generated or in bulk with knowledge time). Clock. A function that reports the current timestamp. There are 3 versions of clock: Static clock. A clock that remains the same during a system run. a. Future peeking is allowed Replayed clock. A moving clock that can be in the past or future with respect to a real clock a. Use time passing at the same pace of real-time wall-clock or b. Simulate time based on events, e.g., as soon as the workload corresponding to one timestamp is complete we move to the next timestamp, without waiting for the actual time to pass c. Future peeking is technically possible but is prohibited Real clock. The wall-clock time matches what we observe in real-life, data is provided to processing units as it is produced by systems. a. Future peeking is not possible in principle Knowledge time. It is the time when data becomes available (e.g., downloaded or computed) to a system. Each row of data is tagged with the corresponding knowledge time. Data with knowledge time after the current clock time must not be observable in order to avoid future peeking. Timed simulation . Sometimes referred to as historical, vectorized, bulk, batch simulation. In a timed simulation the data is provided with a clock that reports the current timestamp. Data with knowledge time after the current timestamp must not be observable in order to avoid future peeking. TODO(gp): Add an example of df with forecasts explaining the timing Non-timed simulation . (Sometimes referred to as event-based, reactive simulation). Clock type is \"static clock\". Typically wall-clock time is a timestamp that corresponds to the latest knowledge time (or greater) in a dataframe. In this way all data in a dataframe is available because every row has a knowledge time that is less than or equal to the wall-clock time. Note that the clock is static, i.e. not moving. In a non-timed simulation, the data is provided in a dataframe for the entire period of interest. E.g., for a system predicting every 5 mins, all the input data are equally spaced on a 5-min grid and indexed with knowledge time. TODO(gp): Add an example of df with forecasts explaining the timing df[\"c\"] = (df[\"a\"] + df[\"b\"]).shift(1) Real-time execution . In real-time the clock type is \"real clock\". E.g., for a system predicting every 5 mins, one forecast is delivered every 5 mins of wall-clock. TODO(Grisha): add an example. Replayed simulation . In replayed simulation, the data is provided in the same \"format\" and with the same timing as it would be provided in real-time, but the clock type is \"replayed clock\". TODO(gp): Add an example of df with forecasts explaining the timing Different views of System components Different implementations of a component . A DataFlow component is described in terms of an interface and can have several implementations at different levels of detail. Reference implementation . A reference implementation is vendor-agnostic implementation of a component (e.g., DataFrameImClient, DataFrameBroker) Vendor implementation . A vendor implementation is a vendor-specific implementation of a component (e.g., CcxtImClient, CcxtBroker). Mocked implementation . A mocked implementation is a simulated version of a vendor-specific component (e.g., a DataFrameCcxtBroker). A mocked component can have the same timing semantics as the real-component (e.g., an asynchronous or reactive implementation) or not. Architecture In this section we summarize the responsibilities and the high level invariants of each component of a System . A System is represented in terms of a Config . - Each piece of a Config refers to and configures a specific part of the System - Each component should be completely configured in terms of a Config Component invariants All data in components should be indexed by the knowledge time (i.e., when the data became available to that component) in terms of current time. Each component has a way to know: - what is the current time (e.g., the real-time machine time or the simulated one) - the timestamp of the current data bar it's working on Each component - should print its state so that one can inspect how exactly it has been initialized - can be serialized and deserialized from disk - can be mocked for simulating - should save data in a directory as it executes to make the system observable Models are described in terms of DAGs using the DataFlow framework Misc . Models read data from historical and real-time data sets, typically not mixing these two styles. Raw data is typically stored in S3 bucket in the same format as it comes or in Parquet format. DataFlow computing DataFlow framework . DataFlow is a computing framework to implement machine learning models that can run with minimal changes in timed, non-timed, replayed simulation and real-time execution. The working principle underlying DataFlow is to run a model in terms of time slices of data so that both historical and real-time semantics can be accommodated without changing the model. Some of the advantages of the DataFlow approach are: Tiling to fit in memory Cached computation Adapt a procedural semantic to a reactive / streaming semantic Handle notion of time Control for future peeking Suite of tools to replay and debug executions from real-time Support for market data and other tabular data feeds Support for knowledge time TODO(gp): Explain the advantages Resampling VWAP (besides potential errors). This implies hardcoded formula in a mix with resampling functions. vwap_approach_2 = ( converted_data[\"close\"] * converted_data[\"volume\"]).resample(resampling_freq) ).mean() / converted_data[\"volume\"].resample(resampling_freq).sum() vwap_approach_2.head(3) TODO(gp): Explain this piece of code Dag Node . A Dag Node is a unit of computation of a DataFlow model. - A Dag Node has inputs, outputs, a unique node id (aka nid ), and a state - Typically, inputs and outputs to a Dag Node are dataframes - A Dag node stores a value for each output and method name (e.g., methods are fit , predict , save_state , load_state ) - The DataFlow time slice semantics is implemented in terms of Pandas and Sklearn libraries TODO(gp): Add picture. DataFlow model . A DataFlow model (aka DAG ) is a direct acyclic graph composed of DataFlow nodes. It allows to connect, query the structure Running a method on a Dag means running that method on all its nodes in topological order, propagating values through the Dag nodes. TODO(gp): Add picture. DagConfig . A Dag can be built assembling Nodes using a function representing the connectivity of the nodes and parameters contained in a Config (e.g., through a call to a builder DagBuilder.get_dag(config) ). A DagConfig is hierarchical and contains one subconfig per Dag node. It should only include Dag node configuration parameters, and not information about Dag connectivity, which is specified in the Dag builder part. Template configs Are incomplete configs, with some \"mandatory\" parameters unspecified but clearly identified with cconfig.DUMMY value Have reasonable defaults for specified parameters This facilitates config extension (e.g., if we add additional parameters / flexibility in the future, then we should not have to regenerate old configs) Leave dummy parameters for frequently-varying fields, such as ticker Should be completable and be completed before use Should be associated with a Dag builder DagBuilder . It is an object that builds a DAG and has a get_config_template() and a get_dag() method to keep the config and the Dag in sync. The client: - calls get_config_template() to receive the template config - fills / modifies the config - uses the final config to call get_dag(config) and get a fully built DAG A DagBuilder can be passed to other objects instead of Dag when the template config is fully specified and thus the Dag can be constructed from it. DagRunner . It is an object that allows to run a Dag . Different implementations of a DagRunner allow to run a Dag on data in different ways, e.g., FitPredictDagRunner : implements two methods fit / predict when we want to learn on in-sample data and predict on out-of-sample data RollingFitPredictDagRunner : allows to fit and predict on some data using a rolling pattern IncrementalDagRunner : allows to run one step at a time like in real-time RealTimeDagRunner : allows to run using nodes that have a real-time semantic DataFlow Computation Semantics Often raw data is available in a \"long format\", where the data is conditioned on the asset (e.g., full_symbol), e.g., {width=\"5.338542213473316in\" height=\"1.1036406386701663in\"} DataFlow represents data through multi-index dataframes, where the outermost index is the \"feature\" the innermost index is the asset, e.g., {width=\"6.5in\" height=\"1.0416666666666667in\"} The reason for this convention is that typically features are computed in a uni-variate fashion (e.g., asset by asset), and we can get vectorization over the assets by expressing operations in terms of the features. E.g., we can express a feature as (df[\"close\", \"open\"].max() - df[\"high\"]).shift(2) . Based on the example ./amp/dataflow/notebooks/gallery_dataflow_example.ipynb, one can work with DataFlow at 4 levels of abstraction: Pandas long-format (non multi-index) dataframes and for-loops We can do a group-by or filter by full_symbol Apply the transformation on each resulting df Merge the data back into a single dataframe with the long-format Pandas multiindex dataframes The data is in the DataFlow native format We can apply the transformation in a vectorized way This approach is best for performance and with compatibility with DataFlow point of view An alternative approach is to express multi-index transformations in terms of approach 1 (i.e., single asset transformations and then concatenation). This approach is functionally equivalent to a multi-index transformation, but typically slow and memory inefficient DataFlow nodes A node implements a certain transformations on DataFrames according to the DataFlow convention and interfaces Nodes operate on the multi-index representation by typically calling functions from level 2 above DAG A series of transformations in terms of DataFlow nodes Note that there are degrees of freedom in splitting the work between the various layers. E.g., code can be split in multiple functions at level 2) and then [http://172.30.2.136:10051/notebooks/dataflow_orange/pipelines/C1/notebooks/C1b_debugging.ipynb]{.underline} [http://172.30.2.136:10051/notebooks/dataflow_orange/pipelines/C1/notebooks/Implement_RH1E.ipynb]{.underline} Backtest and Experiment ConfigBuilder Generates a list of fully formed (not template) configs that can be then run These configs can correspond to one or multiple Experiments, tiled or not (see below) Config builder accepts BacktestConfig as an input Experiment in strict and loose sense Colloquially, we use experiment to mean different things, e.g., an experiment can consist in: a backtest where we run a single Dag with a single config (e.g., when we test the predictive power of a single model) running a Dag (e.g., E8d) through multiple configs (e.g., with longer / shorter history) to perform an \"A / B experiment\" running completely different Dags (e.g., E1 vs E8c) to compare their performance Strictly speaking, we refer to: The first one as a Backtest (which can be executed in terms of tiled configs or not) The second and the third as an Experiment In practice almost any experiment we run consists of one or more backtests Backtest In general a \"backtest\" is simply code that is configured by a *single* Config s The code contained in a backtest experiment can be anything Typically a backtest consists of: creating a Dag (e.g., through a DagBuilder ) or a System based on a config running it over a period of time (e.g., through a DagRunner ) saving the result into a directory BacktestConfig = a config that has multiple parts configuring both what to run (e.g., a Dag ) and how to run it (e.g., the universe, the period of time) It can correspond to multiple configs (e.g., when running a TiledBacktest ) The common pattern is <universe>-<top_n>.<trading_period>.<time_interval> , e.g., ccxt_v4-top3.5T.2019_2022 where ccxt_v4 is a specific version of universe top3 is top 3 assets, all means all assets in the universe 5T (5 minutes) is trading period 2019-2022 is timeframe, i.e. run the model using data from 2019 to 2022 Experiment A set of backtests to run, each of which corresponds to conceptually a single Config Each backtest can then be executed in a tiled fashion (e.g., by expressing it in terms of different configs, one per tile In order to create the list of fully built configs, both a Backtest and a Experiment need: an BacktestBuilder (what to run in a backtest) a ConfigBuilder (how to configure) dst_dir (destination dir of the entire experiment list, i.e., the one that the user passes to the command) Tiled backtest / experiment An experiment / backtest that is run through multiple tiles for time and assets In general this is just an implementation detail Tiled vs Tile We call \"tiled\" objects that are split in tiles (e.g., TiledBacktest ), and \"tile\" objects that refer to tiling (e.g., TileConfig ) Experiment (list) manager TODO(gp): experiment_list manager? Python code that runs experiments by: generating a list of Config object to run, based on a ConfigBuilder (i.e., run_experiment.py and run_notebook.py ) ExperimentBuilder TODO(gp): -> BacktestBuilder It is a function that: Creates a DAG from the passed config Runs the DAG Saves the results in a specified directory BacktestRunner A test case object that: runs a backtest (experiment) on a Dag and a Config processes its results (e.g., check that the output is readable, extract a PnL curve or other statistics) System An object representing a full trading system comprising of: MarketData HistoricalMarketData (ImClient) RealTimeMarketData Dag DagRunner Portfolio Optimizer Broker SystemRunner An object that allows to build and run a System TODO(gp): Not sure it's needed System_TestCase TODO(gp): IMO this is a TestCase + various helpers Data structures Fill Order Major software components {width=\"6.5in\" height=\"1.875in\"} [https://lucid.app/lucidchart/9ee80100-be76-42d6-ad80-531dcfee277e/edit?page=0_0&invitationId=inv_5777ae4b-d8f4-41c6-8901-cdfb93d98ca8#]{.underline} {width=\"6.5in\" height=\"4.319444444444445in\"} ImClient Responsibilities: Interactions: Main methods: MarketData Responsibilities: Interactions: Main methods: Forecaster. It is a DAG system that forecasts the value of the target economic quantities (e.g., for each asset in the target Responsibilities: Interactions: Main methods: process_forecasts. Interface to execute all the predictions in a Forecast dataframe through TargetPositionAndOrderGenerator. This is used as an interface to simulate the effect of given forecasts under different optimization conditions, spread, and restrictions, without running the Forecaster. TargetPositionAndOrderGenerator . Execute the forecasts by generating the optimal target positions according to the desired criteria and by generating the corresponding orders needed to get the system from the current to the desired state. TODO(gp): It also submits the orders so ForecastProcessor? Responsibilities: Retrieve the current holdings from Portfolio Perform optimization using forecasts and current holdings to compute the target position Generate the orders needed to achieve the target positions Submit orders to the Broker Interactions: Forecaster to receive the forecasts of returns for each asset Portfolio to recover the current holdings Main methods: compute_target_positions_and_generate_orders(): compute the target positions and generate the orders needed to reach _compute_target_holdings_shares(): call the Optimizer to compute the target holdings in shares Locates . Restrictions . Optimizer. Responsibilities: Interactions: Main methods: Portfolio . A Portfolio stores information about asset and cash holdings of a System over time. Responsibilities: hold the holdings in terms of shares of each asset id and cash available Interactions: MarketData to receive current prices to estimate the value of the holdings Accumulate statistics and Main methods: mark_to_market(): estimate the value of the current holdings using the current market prices ... DataFramePortfolio : an implementation of a Portfolio backed by a DataFrame. This is used to simulate a system on an order-by-order basis. This should be equivalent to using a DatabasePortfolio but without the complexity of querying a DB. DatabasePortfolio : an implementation of a Portfolio backed by an SQL Database to simulate systems where the Portfolio state is held in a database. This allows to simulate a system on an order-by-order basis. Broker. A Broker is an object to place orders to the market and to receive fills, adapting Order and Fill objects to the corresponding market-specific objects. In practice Broker adapts the internal representation of Order and Fills to the ones that are specific to the target market. Responsibilities: - Submit orders to MarketOms - Wait to ensure that orders were properly accepted by MarketOms - Execute complex orders (e.g., TWAP, VWAP, pegged orders) interacting with the target market - Receive fill information from the target market Interactions: - MarketData to receive prices and other information necessary to execute orders - MarketOms to place orders and receive fills Main methods: - submit_orders() - get_fills() MarketOms . MarketOms is the interface that allows to place orders and receive back fills to the specific target market. This is provided as-is and it's not under control of the user or of the protocol E.g., a specific exchange API interface OrderProcessor TODO(gp): Maybe MockedMarketOms since that's the actual function? OmsDb TO REORG From ./oms/architecture.md Invariants and conventions In this doc we use the new names for concepts and use \"aka\" to refer to the old name, if needed We refer to: The as-of-date for a query as as_of_timestamp The actual time from get_wall_clock_time() as wall_clock_timestamp Objects need to use get_wall_clock_time() to get the \"actual\" time We don't want to pass wall_clock_timestamp because this is dangerous It is difficult to enforce that there is no future peeking when one object tells another what time it is, since there is no way for the second object to double check that the wall clock time is accurate We pass wall_clock_timestamp only when one \"action\" happens atomically but it is split in multiple functions that need to all share this information. This approach should be the exception to the rule of calling get_wall_clock_time() It's ok to ask for a view of the world as of as_of_timestamp , but then the queried object needs to check that there is no future peeking by using get_wall_clock_time() Objects might need to get event_loop TODO(gp): Clean it up so that we pass event loop all the times and event loop has a reference to the global get_wall_clock_time() The Optimizer only thinks in terms of dollar Implementation process_forecasts() - Aka place_trades() - Act on the forecasts by: - Get the state of portfolio (by getting fills from previous clock) - Updating the portfolio holdings - Computing the optimal positions - Submitting the corresponding orders - optimize_positions() - Aka optimize_and_update() - Calls the Optimizer - compute_target_positions() - Aka compute_trades() - submit_orders() - Call Broker - get_fills() - Call Broker - For IS it is different - update_portfolio() - Call Portfolio - For IS it is different - It should not use any concrete implementation but only Abstract\\* Portfolio - get_holdings() - Abstract because IS, Mocked, Simulated have a different implementations - mark_to_market() Not abstract -> get_holdings() , PriceInterface update_state() Abstract Use abstract but make it NotImplemented (we will get some static checks and some other dynamic checks) We are trying not to mix static typing and duck typing CASH_ID, _compute_statistics() goes in Portolio Broker - submit_orders() - get_fills() Simulation DataFramePortfolio - This is what we call Portfolio - In RT we can run DataFramePortfolio and ImplementedPortfolio in parallel to collect real and simulated behavior - get_holdings() - Store the holdings in a df - update_state() - Update the holdings with fills -> SimulatedBroker.get_fills() - To make the simulated system closer to the implemented SimulatedBroker - submit_orders() - get_fills() Implemented system ImplementedPortfolio - get_holdings() - Check self-consistency and assumptions - Check that no order is in flight otherwise we should assert or log an error - Query the DB and gets you the answer - update_state() - No-op since the portfolio is updated automatically ImplementedBroker - submit_orders() - Save files in the proper location - Wait for orders to be accepted - get_fills - No-op since the portfolio is updated automatically Mocked system - Our implementation of the implemented system where we replace DB with a mock - The mocked DB should be as similar as possible to the implemented DB DatabasePortfolio - get_holdings() - Same behavior of ImplementedPortfolio but using OmsDb DatabaseBroker - submit_orders() - Same behavior of ImplementedBroker but using OmsDb OmsDb - submitted_orders table (mocks S3) - Contain the submitted orders - accepted_orders table - current_position table OrderProcessor - Monitor OmsDb.submitted_orders - Update OmsDb.accepted_orders - Update OmsDb.current_position using Fill and updating the Portfolio OMS High-level invariants The notion of parent vs child orders is only on our side, CCXT only understands orders We track data (e.g., orders, fills) into parallel OMS vs CCXT data structures OMS vs CCXT orders OMS vs CCXT fills CCXT trades vs fills Portfolio only cares about parent orders How orders are executed is an implementation detail Thus, we need to fold all child fills into an equivalent parent fill The data ccxt_order Every time we submit an order to CCXT (parent or child) we get back a ccxt_order object (aka ccxt_order_response ) It's mainly a confirmation of the order The format is https://docs.ccxt.com/#/?id=order-structure The most important info is the callback CCXT ID of the order (this is the only way to do it) ccxt_trade For each order (parent or child), we get back from CCXT 0 or more ccxt_trade , each representing a partial fill (e.g., price, number of shares, fees) E.g., If we walk the book, we obviously get multiple ccxt_trade If we match multiple trades even at the same price level, we might get different ccxt_trade ccxt_fill When an order closes, we can ask CCXT to summarize the results of that order in terms of the composing trades We can't use ccxt_fill to create an oms_fill because it doesn't contain enough info about prices and fees We get some information about quantity, but we don't get fees and other info (e.g., prices) We save this info to cross-check ccxt_fill vs ccxt_trade oms_fill It represents the fill of a parent order, since outside the execution system (e.g., in Portfolio ) we don't care about tracking individual fills For a parent order we need to convert multiple ccxt_trades into a single oms_fill TODO(gp): Unclear Get all the trades combined to the parent order to get a single OMS fill We use this v1 Another way is to query the state of an order We use this in v2 prototype, but it's not sure that it's the final approach","title":"DataFlow specification"},{"location":"DataFlow/#dataflow-specification","text":"","title":"DataFlow specification"},{"location":"DataFlow/#config","text":"Config . A Config is a dictionary-like object that represents parameters used to build and configure other objects (e.g., a DAG or a System). Each config is a hierarchical structure which consists of Subconfigs and Leaves . Subconfig is a nested object which represents a Config inside another config. A Subconfig of a Subconfig is a Subconfig of a Config, i.e. the relation is transitive. Leaf is any object inside a Config that is used to build another object that is not in itself a Config. Note that a dictionary or other mapping objects are not permitted inside a Config: each dictionary-like object should be converted to a Config and become a Subconfig.","title":"Config"},{"location":"DataFlow/#config-representation-and-properties","text":"A Config can be represented as a dictionary or a string. Example of a dictionary representation: config1={ \"resample_1min\": False, \"client_config\": { \"universe\": { \"full_symbols\": [\"binance::ADA_USDT\"], \"universe_version\": \"v3\", }, }, \"market_data_config\": {\"start_ts\": start_ts, \"end_ts\": end_ts}, } In the example above: - \"resample_1min\" is a leaf of the config1 - \"client_config\" is a subconfig of config1 - \"universe\" is a subconfig of \"client_config\" - \"market_data\" config is a subconfig of \"config1\" - \"start_ts\" and \"end_ts\" are leaves of \"market_data_config\" and config1 Example of a string representation: {width=\"6.854779090113736in\" height=\"1.2303444881889765in\"} The same values are annotated with marked_as_used , writer and val_type marked_as_used determines whether the object was used to construct another object writer provides a stacktrace of the piece of code which marked the object as used val_type is a type of the object","title":"Config representation and properties"},{"location":"DataFlow/#assigning-and-getting-config-items","text":"Config object has its own implementations of __setitem__ and __getitem__ A new value can be set freely like in a python Dict object Overwriting the value is prohibited if the value has already been used Since Config is used to guarantee that the construction of any objects is reproducible, there are 2 methods to get the value. get_and_mark_as_used is utilized when a leaf of the config is used to construct another object When the value is used inside a constructor When the value is used as a parameter in a function Note that when selecting a subconfig the subconfig itself is not marked as used, but its leaves are. For this reason, the user should avoid marking subconfigs as used and instead select leaves separately. Example of marking the subconfig as used: _ = config.get_and_mark_as_used(\"market_data_config\") {width=\"6.5in\" height=\"1.1944444444444444in\"} Example of marking the leaf as used: _ = config.get_and_mark_as_used((\"market_data_config\", \"end_ts\")) {width=\"6.5in\" height=\"1.1388888888888888in\"} __getitem__ is used to select items for uses which do not affect the construction of other objects: Logging, debugging and printing","title":"Assigning and getting Config items"},{"location":"DataFlow/#time-semantics","text":"Time semantics . A DataFlow component can be executed or simulated accounting for different ways to represent the passing of time. E.g., it can be simulated in a timed or non-timed simulation, depending on how data is delivered to the system (as it is generated or in bulk with knowledge time). Clock. A function that reports the current timestamp. There are 3 versions of clock: Static clock. A clock that remains the same during a system run. a. Future peeking is allowed Replayed clock. A moving clock that can be in the past or future with respect to a real clock a. Use time passing at the same pace of real-time wall-clock or b. Simulate time based on events, e.g., as soon as the workload corresponding to one timestamp is complete we move to the next timestamp, without waiting for the actual time to pass c. Future peeking is technically possible but is prohibited Real clock. The wall-clock time matches what we observe in real-life, data is provided to processing units as it is produced by systems. a. Future peeking is not possible in principle Knowledge time. It is the time when data becomes available (e.g., downloaded or computed) to a system. Each row of data is tagged with the corresponding knowledge time. Data with knowledge time after the current clock time must not be observable in order to avoid future peeking. Timed simulation . Sometimes referred to as historical, vectorized, bulk, batch simulation. In a timed simulation the data is provided with a clock that reports the current timestamp. Data with knowledge time after the current timestamp must not be observable in order to avoid future peeking. TODO(gp): Add an example of df with forecasts explaining the timing Non-timed simulation . (Sometimes referred to as event-based, reactive simulation). Clock type is \"static clock\". Typically wall-clock time is a timestamp that corresponds to the latest knowledge time (or greater) in a dataframe. In this way all data in a dataframe is available because every row has a knowledge time that is less than or equal to the wall-clock time. Note that the clock is static, i.e. not moving. In a non-timed simulation, the data is provided in a dataframe for the entire period of interest. E.g., for a system predicting every 5 mins, all the input data are equally spaced on a 5-min grid and indexed with knowledge time. TODO(gp): Add an example of df with forecasts explaining the timing df[\"c\"] = (df[\"a\"] + df[\"b\"]).shift(1) Real-time execution . In real-time the clock type is \"real clock\". E.g., for a system predicting every 5 mins, one forecast is delivered every 5 mins of wall-clock. TODO(Grisha): add an example. Replayed simulation . In replayed simulation, the data is provided in the same \"format\" and with the same timing as it would be provided in real-time, but the clock type is \"replayed clock\". TODO(gp): Add an example of df with forecasts explaining the timing","title":"Time semantics"},{"location":"DataFlow/#different-views-of-system-components","text":"Different implementations of a component . A DataFlow component is described in terms of an interface and can have several implementations at different levels of detail. Reference implementation . A reference implementation is vendor-agnostic implementation of a component (e.g., DataFrameImClient, DataFrameBroker) Vendor implementation . A vendor implementation is a vendor-specific implementation of a component (e.g., CcxtImClient, CcxtBroker). Mocked implementation . A mocked implementation is a simulated version of a vendor-specific component (e.g., a DataFrameCcxtBroker). A mocked component can have the same timing semantics as the real-component (e.g., an asynchronous or reactive implementation) or not.","title":"Different views of System components"},{"location":"DataFlow/#architecture","text":"In this section we summarize the responsibilities and the high level invariants of each component of a System . A System is represented in terms of a Config . - Each piece of a Config refers to and configures a specific part of the System - Each component should be completely configured in terms of a Config","title":"Architecture"},{"location":"DataFlow/#component-invariants","text":"All data in components should be indexed by the knowledge time (i.e., when the data became available to that component) in terms of current time. Each component has a way to know: - what is the current time (e.g., the real-time machine time or the simulated one) - the timestamp of the current data bar it's working on Each component - should print its state so that one can inspect how exactly it has been initialized - can be serialized and deserialized from disk - can be mocked for simulating - should save data in a directory as it executes to make the system observable Models are described in terms of DAGs using the DataFlow framework Misc . Models read data from historical and real-time data sets, typically not mixing these two styles. Raw data is typically stored in S3 bucket in the same format as it comes or in Parquet format.","title":"Component invariants"},{"location":"DataFlow/#dataflow-computing","text":"DataFlow framework . DataFlow is a computing framework to implement machine learning models that can run with minimal changes in timed, non-timed, replayed simulation and real-time execution. The working principle underlying DataFlow is to run a model in terms of time slices of data so that both historical and real-time semantics can be accommodated without changing the model. Some of the advantages of the DataFlow approach are: Tiling to fit in memory Cached computation Adapt a procedural semantic to a reactive / streaming semantic Handle notion of time Control for future peeking Suite of tools to replay and debug executions from real-time Support for market data and other tabular data feeds Support for knowledge time TODO(gp): Explain the advantages Resampling VWAP (besides potential errors). This implies hardcoded formula in a mix with resampling functions. vwap_approach_2 = ( converted_data[\"close\"] * converted_data[\"volume\"]).resample(resampling_freq) ).mean() / converted_data[\"volume\"].resample(resampling_freq).sum() vwap_approach_2.head(3) TODO(gp): Explain this piece of code Dag Node . A Dag Node is a unit of computation of a DataFlow model. - A Dag Node has inputs, outputs, a unique node id (aka nid ), and a state - Typically, inputs and outputs to a Dag Node are dataframes - A Dag node stores a value for each output and method name (e.g., methods are fit , predict , save_state , load_state ) - The DataFlow time slice semantics is implemented in terms of Pandas and Sklearn libraries TODO(gp): Add picture. DataFlow model . A DataFlow model (aka DAG ) is a direct acyclic graph composed of DataFlow nodes. It allows to connect, query the structure Running a method on a Dag means running that method on all its nodes in topological order, propagating values through the Dag nodes. TODO(gp): Add picture. DagConfig . A Dag can be built assembling Nodes using a function representing the connectivity of the nodes and parameters contained in a Config (e.g., through a call to a builder DagBuilder.get_dag(config) ). A DagConfig is hierarchical and contains one subconfig per Dag node. It should only include Dag node configuration parameters, and not information about Dag connectivity, which is specified in the Dag builder part.","title":"DataFlow computing"},{"location":"DataFlow/#template-configs","text":"Are incomplete configs, with some \"mandatory\" parameters unspecified but clearly identified with cconfig.DUMMY value Have reasonable defaults for specified parameters This facilitates config extension (e.g., if we add additional parameters / flexibility in the future, then we should not have to regenerate old configs) Leave dummy parameters for frequently-varying fields, such as ticker Should be completable and be completed before use Should be associated with a Dag builder DagBuilder . It is an object that builds a DAG and has a get_config_template() and a get_dag() method to keep the config and the Dag in sync. The client: - calls get_config_template() to receive the template config - fills / modifies the config - uses the final config to call get_dag(config) and get a fully built DAG A DagBuilder can be passed to other objects instead of Dag when the template config is fully specified and thus the Dag can be constructed from it. DagRunner . It is an object that allows to run a Dag . Different implementations of a DagRunner allow to run a Dag on data in different ways, e.g., FitPredictDagRunner : implements two methods fit / predict when we want to learn on in-sample data and predict on out-of-sample data RollingFitPredictDagRunner : allows to fit and predict on some data using a rolling pattern IncrementalDagRunner : allows to run one step at a time like in real-time RealTimeDagRunner : allows to run using nodes that have a real-time semantic","title":"Template configs"},{"location":"DataFlow/#dataflow-computation-semantics","text":"Often raw data is available in a \"long format\", where the data is conditioned on the asset (e.g., full_symbol), e.g., {width=\"5.338542213473316in\" height=\"1.1036406386701663in\"} DataFlow represents data through multi-index dataframes, where the outermost index is the \"feature\" the innermost index is the asset, e.g., {width=\"6.5in\" height=\"1.0416666666666667in\"} The reason for this convention is that typically features are computed in a uni-variate fashion (e.g., asset by asset), and we can get vectorization over the assets by expressing operations in terms of the features. E.g., we can express a feature as (df[\"close\", \"open\"].max() - df[\"high\"]).shift(2) . Based on the example ./amp/dataflow/notebooks/gallery_dataflow_example.ipynb, one can work with DataFlow at 4 levels of abstraction: Pandas long-format (non multi-index) dataframes and for-loops We can do a group-by or filter by full_symbol Apply the transformation on each resulting df Merge the data back into a single dataframe with the long-format Pandas multiindex dataframes The data is in the DataFlow native format We can apply the transformation in a vectorized way This approach is best for performance and with compatibility with DataFlow point of view An alternative approach is to express multi-index transformations in terms of approach 1 (i.e., single asset transformations and then concatenation). This approach is functionally equivalent to a multi-index transformation, but typically slow and memory inefficient DataFlow nodes A node implements a certain transformations on DataFrames according to the DataFlow convention and interfaces Nodes operate on the multi-index representation by typically calling functions from level 2 above DAG A series of transformations in terms of DataFlow nodes Note that there are degrees of freedom in splitting the work between the various layers. E.g., code can be split in multiple functions at level 2) and then [http://172.30.2.136:10051/notebooks/dataflow_orange/pipelines/C1/notebooks/C1b_debugging.ipynb]{.underline} [http://172.30.2.136:10051/notebooks/dataflow_orange/pipelines/C1/notebooks/Implement_RH1E.ipynb]{.underline}","title":"DataFlow Computation Semantics"},{"location":"DataFlow/#backtest-and-experiment","text":"","title":"Backtest and Experiment"},{"location":"DataFlow/#configbuilder","text":"Generates a list of fully formed (not template) configs that can be then run These configs can correspond to one or multiple Experiments, tiled or not (see below) Config builder accepts BacktestConfig as an input","title":"ConfigBuilder"},{"location":"DataFlow/#experiment-in-strict-and-loose-sense","text":"Colloquially, we use experiment to mean different things, e.g., an experiment can consist in: a backtest where we run a single Dag with a single config (e.g., when we test the predictive power of a single model) running a Dag (e.g., E8d) through multiple configs (e.g., with longer / shorter history) to perform an \"A / B experiment\" running completely different Dags (e.g., E1 vs E8c) to compare their performance Strictly speaking, we refer to: The first one as a Backtest (which can be executed in terms of tiled configs or not) The second and the third as an Experiment In practice almost any experiment we run consists of one or more backtests","title":"Experiment in strict and loose sense"},{"location":"DataFlow/#backtest","text":"In general a \"backtest\" is simply code that is configured by a *single* Config s The code contained in a backtest experiment can be anything Typically a backtest consists of: creating a Dag (e.g., through a DagBuilder ) or a System based on a config running it over a period of time (e.g., through a DagRunner ) saving the result into a directory","title":"Backtest"},{"location":"DataFlow/#backtestconfig","text":"= a config that has multiple parts configuring both what to run (e.g., a Dag ) and how to run it (e.g., the universe, the period of time) It can correspond to multiple configs (e.g., when running a TiledBacktest ) The common pattern is <universe>-<top_n>.<trading_period>.<time_interval> , e.g., ccxt_v4-top3.5T.2019_2022 where ccxt_v4 is a specific version of universe top3 is top 3 assets, all means all assets in the universe 5T (5 minutes) is trading period 2019-2022 is timeframe, i.e. run the model using data from 2019 to 2022","title":"BacktestConfig"},{"location":"DataFlow/#experiment","text":"A set of backtests to run, each of which corresponds to conceptually a single Config Each backtest can then be executed in a tiled fashion (e.g., by expressing it in terms of different configs, one per tile In order to create the list of fully built configs, both a Backtest and a Experiment need: an BacktestBuilder (what to run in a backtest) a ConfigBuilder (how to configure) dst_dir (destination dir of the entire experiment list, i.e., the one that the user passes to the command)","title":"Experiment"},{"location":"DataFlow/#tiled-backtest-experiment","text":"An experiment / backtest that is run through multiple tiles for time and assets In general this is just an implementation detail","title":"Tiled backtest / experiment"},{"location":"DataFlow/#tiled-vs-tile","text":"We call \"tiled\" objects that are split in tiles (e.g., TiledBacktest ), and \"tile\" objects that refer to tiling (e.g., TileConfig )","title":"Tiled vs Tile"},{"location":"DataFlow/#experiment-list-manager","text":"TODO(gp): experiment_list manager? Python code that runs experiments by: generating a list of Config object to run, based on a ConfigBuilder (i.e., run_experiment.py and run_notebook.py )","title":"Experiment (list) manager"},{"location":"DataFlow/#experimentbuilder","text":"TODO(gp): -> BacktestBuilder It is a function that: Creates a DAG from the passed config Runs the DAG Saves the results in a specified directory","title":"ExperimentBuilder"},{"location":"DataFlow/#backtestrunner","text":"A test case object that: runs a backtest (experiment) on a Dag and a Config processes its results (e.g., check that the output is readable, extract a PnL curve or other statistics)","title":"BacktestRunner"},{"location":"DataFlow/#system","text":"An object representing a full trading system comprising of: MarketData HistoricalMarketData (ImClient) RealTimeMarketData Dag DagRunner Portfolio Optimizer Broker","title":"System"},{"location":"DataFlow/#systemrunner","text":"An object that allows to build and run a System TODO(gp): Not sure it's needed","title":"SystemRunner"},{"location":"DataFlow/#system_testcase","text":"TODO(gp): IMO this is a TestCase + various helpers","title":"System_TestCase"},{"location":"DataFlow/#data-structures","text":"Fill Order","title":"Data structures"},{"location":"DataFlow/#major-software-components","text":"{width=\"6.5in\" height=\"1.875in\"} [https://lucid.app/lucidchart/9ee80100-be76-42d6-ad80-531dcfee277e/edit?page=0_0&invitationId=inv_5777ae4b-d8f4-41c6-8901-cdfb93d98ca8#]{.underline} {width=\"6.5in\" height=\"4.319444444444445in\"} ImClient Responsibilities: Interactions: Main methods: MarketData Responsibilities: Interactions: Main methods: Forecaster. It is a DAG system that forecasts the value of the target economic quantities (e.g., for each asset in the target Responsibilities: Interactions: Main methods: process_forecasts. Interface to execute all the predictions in a Forecast dataframe through TargetPositionAndOrderGenerator. This is used as an interface to simulate the effect of given forecasts under different optimization conditions, spread, and restrictions, without running the Forecaster. TargetPositionAndOrderGenerator . Execute the forecasts by generating the optimal target positions according to the desired criteria and by generating the corresponding orders needed to get the system from the current to the desired state. TODO(gp): It also submits the orders so ForecastProcessor? Responsibilities: Retrieve the current holdings from Portfolio Perform optimization using forecasts and current holdings to compute the target position Generate the orders needed to achieve the target positions Submit orders to the Broker Interactions: Forecaster to receive the forecasts of returns for each asset Portfolio to recover the current holdings Main methods: compute_target_positions_and_generate_orders(): compute the target positions and generate the orders needed to reach _compute_target_holdings_shares(): call the Optimizer to compute the target holdings in shares Locates . Restrictions . Optimizer. Responsibilities: Interactions: Main methods: Portfolio . A Portfolio stores information about asset and cash holdings of a System over time. Responsibilities: hold the holdings in terms of shares of each asset id and cash available Interactions: MarketData to receive current prices to estimate the value of the holdings Accumulate statistics and Main methods: mark_to_market(): estimate the value of the current holdings using the current market prices ... DataFramePortfolio : an implementation of a Portfolio backed by a DataFrame. This is used to simulate a system on an order-by-order basis. This should be equivalent to using a DatabasePortfolio but without the complexity of querying a DB. DatabasePortfolio : an implementation of a Portfolio backed by an SQL Database to simulate systems where the Portfolio state is held in a database. This allows to simulate a system on an order-by-order basis. Broker. A Broker is an object to place orders to the market and to receive fills, adapting Order and Fill objects to the corresponding market-specific objects. In practice Broker adapts the internal representation of Order and Fills to the ones that are specific to the target market. Responsibilities: - Submit orders to MarketOms - Wait to ensure that orders were properly accepted by MarketOms - Execute complex orders (e.g., TWAP, VWAP, pegged orders) interacting with the target market - Receive fill information from the target market Interactions: - MarketData to receive prices and other information necessary to execute orders - MarketOms to place orders and receive fills Main methods: - submit_orders() - get_fills() MarketOms . MarketOms is the interface that allows to place orders and receive back fills to the specific target market. This is provided as-is and it's not under control of the user or of the protocol E.g., a specific exchange API interface OrderProcessor TODO(gp): Maybe MockedMarketOms since that's the actual function? OmsDb TO REORG From ./oms/architecture.md Invariants and conventions In this doc we use the new names for concepts and use \"aka\" to refer to the old name, if needed We refer to: The as-of-date for a query as as_of_timestamp The actual time from get_wall_clock_time() as wall_clock_timestamp Objects need to use get_wall_clock_time() to get the \"actual\" time We don't want to pass wall_clock_timestamp because this is dangerous It is difficult to enforce that there is no future peeking when one object tells another what time it is, since there is no way for the second object to double check that the wall clock time is accurate We pass wall_clock_timestamp only when one \"action\" happens atomically but it is split in multiple functions that need to all share this information. This approach should be the exception to the rule of calling get_wall_clock_time() It's ok to ask for a view of the world as of as_of_timestamp , but then the queried object needs to check that there is no future peeking by using get_wall_clock_time() Objects might need to get event_loop TODO(gp): Clean it up so that we pass event loop all the times and event loop has a reference to the global get_wall_clock_time() The Optimizer only thinks in terms of dollar Implementation process_forecasts() - Aka place_trades() - Act on the forecasts by: - Get the state of portfolio (by getting fills from previous clock) - Updating the portfolio holdings - Computing the optimal positions - Submitting the corresponding orders - optimize_positions() - Aka optimize_and_update() - Calls the Optimizer - compute_target_positions() - Aka compute_trades() - submit_orders() - Call Broker - get_fills() - Call Broker - For IS it is different - update_portfolio() - Call Portfolio - For IS it is different - It should not use any concrete implementation but only Abstract\\* Portfolio - get_holdings() - Abstract because IS, Mocked, Simulated have a different implementations - mark_to_market() Not abstract -> get_holdings() , PriceInterface update_state() Abstract Use abstract but make it NotImplemented (we will get some static checks and some other dynamic checks) We are trying not to mix static typing and duck typing CASH_ID, _compute_statistics() goes in Portolio Broker - submit_orders() - get_fills() Simulation DataFramePortfolio - This is what we call Portfolio - In RT we can run DataFramePortfolio and ImplementedPortfolio in parallel to collect real and simulated behavior - get_holdings() - Store the holdings in a df - update_state() - Update the holdings with fills -> SimulatedBroker.get_fills() - To make the simulated system closer to the implemented SimulatedBroker - submit_orders() - get_fills() Implemented system ImplementedPortfolio - get_holdings() - Check self-consistency and assumptions - Check that no order is in flight otherwise we should assert or log an error - Query the DB and gets you the answer - update_state() - No-op since the portfolio is updated automatically ImplementedBroker - submit_orders() - Save files in the proper location - Wait for orders to be accepted - get_fills - No-op since the portfolio is updated automatically Mocked system - Our implementation of the implemented system where we replace DB with a mock - The mocked DB should be as similar as possible to the implemented DB DatabasePortfolio - get_holdings() - Same behavior of ImplementedPortfolio but using OmsDb DatabaseBroker - submit_orders() - Same behavior of ImplementedBroker but using OmsDb OmsDb - submitted_orders table (mocks S3) - Contain the submitted orders - accepted_orders table - current_position table OrderProcessor - Monitor OmsDb.submitted_orders - Update OmsDb.accepted_orders - Update OmsDb.current_position using Fill and updating the Portfolio","title":"Major software components"},{"location":"DataFlow/#oms","text":"","title":"OMS"},{"location":"DataFlow/#high-level-invariants","text":"The notion of parent vs child orders is only on our side, CCXT only understands orders We track data (e.g., orders, fills) into parallel OMS vs CCXT data structures OMS vs CCXT orders OMS vs CCXT fills CCXT trades vs fills Portfolio only cares about parent orders How orders are executed is an implementation detail Thus, we need to fold all child fills into an equivalent parent fill The data ccxt_order Every time we submit an order to CCXT (parent or child) we get back a ccxt_order object (aka ccxt_order_response ) It's mainly a confirmation of the order The format is https://docs.ccxt.com/#/?id=order-structure The most important info is the callback CCXT ID of the order (this is the only way to do it) ccxt_trade For each order (parent or child), we get back from CCXT 0 or more ccxt_trade , each representing a partial fill (e.g., price, number of shares, fees) E.g., If we walk the book, we obviously get multiple ccxt_trade If we match multiple trades even at the same price level, we might get different ccxt_trade ccxt_fill When an order closes, we can ask CCXT to summarize the results of that order in terms of the composing trades We can't use ccxt_fill to create an oms_fill because it doesn't contain enough info about prices and fees We get some information about quantity, but we don't get fees and other info (e.g., prices) We save this info to cross-check ccxt_fill vs ccxt_trade oms_fill It represents the fill of a parent order, since outside the execution system (e.g., in Portfolio ) we don't care about tracking individual fills For a parent order we need to convert multiple ccxt_trades into a single oms_fill TODO(gp): Unclear Get all the trades combined to the parent order to get a single OMS fill We use this v1 Another way is to query the state of an order We use this in v2 prototype, but it's not sure that it's the final approach","title":"High-level invariants"},{"location":"DataPull/","text":"DataPull Asset representation TODO(gp): Ideally we want to use a single schema like Vendor:ExchangeId:Asset \\Asset universe ETL We employ a variation of the ETL approach, called EtLT (i.e., extract, lightly transform, load, transform) for downloading both data and metadata. We can have a different pipeline for data and one metadata. Data is extracted from an external data source, lightly transformed, and then loaded into permanent storage. Then downstream data pipelines read the data with a standard client interface. Large variety of data. Data comes in a very large variety, for instance: Different vendor can provide the same data E.g., Kibot, Binance, CryptoDataDownload provide data for the Binance exchange Different time semantics, e.g., Intervals can be [a, b) or (a, b] A bar can be marked at the end or at the beginning of the interval Data and metadata Some vendors provide metadata, others don't Multiple asset classes (e.g., equities, futures, crypto) Data at different time resolutions, e.g., daily bars minute bars trades order book data Historical vs real-time Price data vs alternative data Storage backend . Data can be saved in multiple storage backends: - database (e.g., Postgres, MongoDB) - local filesystem - remote filesystems (e.g., AWS S3 bucket) Data can be saved on filesystems in different formats (e.g., CSV, JSON, Parquet). S3 vs local filesystem. Unfortunately it's not easy to abstract the differences between AWS S3 buckets and local filesystems, since the S3 interface is more along a key-value store rather than a filesystem (supporting permissions, deleting recursively a directory, moving, etc.). Solutions based on abstracting a filesystem on top of S3 (e.g., mounting S3 with Fuse filesystems) are not robust enough. Some backends (e.g., Parquet) allow handling an S3 bucket transparently. Our typical approach is: When writing to S3, use the local filesystem for staging the data in the desired structure and then copy all the data to S3 When reading from S3, read the data directly, use the functionalities supported by the backend, or copy the data locally and then read it from local disk Data formats . The main data formats that Sorrentum supports are: CSV Pros Easy to inspect Easy to load / save Everybody understands it Cons Data can't be easily sliced by asset ids / by time Large footprint (non binary), although it can be compressed (e.g., as .csv.gz on the fly) Parquet Pros Compressed AWS friendly Data can be easily sliced by asset ids and time Cons Not easy to inspect Solution: use wrapper to convert to CSV Difficult to append Solution: use chunking + defragmentation Cumbersome for real-time data database Pros Easy to inspect Support any access pattern Friendly for real-time data Cons Need devops to manage database instance Difficult to track lineage and version Unfortunately there is not an obvious best solution so we have to deal with multiple representations and transforming between them. In practice Parquet is better suited to store historical data and database to store real time data. Extract stage . The goal is to acquire raw data from an external source and archive it into a permanent storage backend (e.g., file-system and/or database). The data can be either historical or real-time. We typically don't process the data at all, but rather we prefer to save the data raw as it comes from the wire. Transform stage . Typically, we prefer to load the data in the backend with minor or no transformation. Specifically we allow changing the representation of the data / format (e.g., removing some totally useless redundancy, compressing the data, transforming from strings to datetimes). We don't allow changing the semantics or filter columns. This is done dynamically in the client stage Load stage. The load stage simply saves the data into one of the supported backends. Typically, we prefer to save Historical data into Parquet format since it supports more naturally the access patterns needed for long simulations Real-time data into a database since this makes it easy to append and retrieve data in real-time. Often we want to also append real-time data to Parquet Client stage. The client stage allows downstream pipelines to access data from the backend storage. The access pattern is always for a model is always \"give me the columns XYZ for assets ABC in the period [..., ...]\". We prefer to perform some transformations that are lightweight (e.g., converting Unix epochs in timestamps) or still evolving (e.g., understanding the timing semantic of the data) are performed inside this stage, rather than in the transform stage. ETL primitives . We implement basic primitives that can be combined in different ways to create various ETL pipelines. Extract: Read data from an external source to memory (typically in the form of Pandas data structures) E.g., downloading data from a REST or Websocket interface Load: Load data stored in memory -> permanent storage (e.g., save as CSV or as Parquet) E.g., pd.to_parquet() DbSave Save to DB Create schema Client: From a permanent storage (e.g., disk) -> Memory E.g., pd.from_parquet() ClientFromDb DB -> Memory Creates the SQL query to read the data Validator https://github.com/cryptokaizen/cmamp/pull/3386/files Transform Just a class Example of ETL pipelines. Download historical data and save it as CSV or PQ Download order book data, compress it, and save it on S3 Insert 1 minute worth of data in the DB Write data into DB One could argue that operations on the DB might not look like extract but rather load We treat any backend (S3, local, DB) in the same way and the DB is just a backend [More detailed description]{.underline} Examples : Transformations CSV -> Parquet <-> DB Convert the CSV data into Parquet using certain indices Convert Parquet by-date into Parquet by-asset Data pipelines Download data by asset (as a time series) asset.csv.gz or Parquet Historical format Download data by time, e.g., 20211105/... csv.gz or Parquet This is typical of real-time flow General conventions Data invariants . We use the following invariants when storing data during data on-boarding and processing: - Data quantities are associated to intervals are [a, b) (e.g., the return over an interval) or to a single point in time (e.g., the close price at 9am UTC) - Every piece of data is labeled with the end of the sampling interval or with the point-in-time - E.g., for a quantity computed in an interval [06:40:00, 06:41:00) the timestamp is 06:41:00 - Timestamps are always time-zone aware and use UTC timezone - Every piece of data has a knowledge timestamp (aka \"as-of-date\") which represent when we were aware of the data according to our wall-clock: - Multiple timestamps associated with different events can be tracked, e.g., start_download_timestamp , end_download_timestamp - No system should depend on data available strictly before the knowledge timestamp - Data is versioned: every time we modify the schema or the semantics of the data, we bump up the version using semantic versioning and update the changelog of what each version contains An example of tabular data is below: {width=\"6.5in\" height=\"1.0138888888888888in\"} Data organization . We keep data together by execution run instead of by data element. E.g., assume we run a flow called XYZ_sanity_check every day and the flow generates three pieces of data, one file output.txt and two directories logs , temp_data . We want to organize the data in a directory structure like: Better - XYZ_sanity_check/ - run.{date}/ - output.txt - logs/ - temp_data/ - run.{date}.manual/ - output.txt - logs/ - temp_data/ Worse - XYZ_sanity_check/ - output.{date}/ - output.txt - logs.{date}/ - temp_data.{date}/ The reasons why the first data layout is superior are: It's easier to delete a single run by deleting a single dir instead of deleting multiple files It allows the format of the data to evolve over time without having to change the schema of the data retroactively It allows scripts post-processing the data to point to a directory with a specific run and work out of the box it's easier to move the data for a single run from one dir (e.g., locally) to another (e.g., a central location) in one command there is redundancy and visual noise, e.g., the same data is everywhere We can tag directory by a run mode (e.g., manual vs scheduled ) by adding the proper suffix to a date-dir. Directory with one file . Having a directory containing one single file often creates redundancy. We prefer not to use directories unless they contain more than one file. We can use directories if we believe that it's highly likely that more files will be needed, but as often happens YANGI (you are not going to need it) applies. Naming convention . We use . to separate conceptually different pieces of a file or a directory. We don't allow white spaces since they are not Linux friendly and need to be escaped. We replace white spaces with _ . We prefer not to use - whenever possible, since they create issues with Linux auto-completion and need to be escaped. E.g., bulk.airflow.csv instead of bulk_airflow.csv Data pipeline classification . A data pipeline can be any of the following: a downloader External DB (e.g., data provider) -> Internal DB: the data flows from an external API to an internal DB It downloads historical or real-time data and saves the dataset in a location The name of the script and the location of the data downloaded follow the naming scheme described below It is typically implemented as a Python script a QA flow for a single or multiple datasets Internal DB -> Process It computes some statistics from one or more datasets (primary or derived) and throws an exception if the data is malformed It aborts if the data has data not compliant to certain QA metrics It is typically implemented as a Python notebook backed by a Python library a derived dataset flow Internal DB -> Process -> Internal DB It computes some data derived from an existing data set E.g., resampling, computing features It is typically implemented as a Python script a model flow Internal DB -> Process -> Outside DB (e.g., exchange) E.g., it runs a computation from internal data and places some trades It is typically implemented as a Python script Data classification . Data can be from market sources or from non-market (aka alternative) sources. Each data source can come with metadata, e.g., List of assets in the universe over time Attributes of assets (e.g., industry and other classification) Asset universe . Often the data relates to a set of assets, e.g., currency pairs on different exchanges. The support of the data is referred to as the \"data universe\". This metadata is versioned as any other piece of data. Data set downloading and handling Data set naming scheme . Each data set is stored in a data lake with a path and name that describe its metadata according to the following signature: dataset_signature={download_mode}.{downloading_entity}.{action_tag}.{data_format}.{data_type}.{asset_type}.{universe}.{vendor}.{exchange_id}.{version[-snapshot]}.{extension} TODO(gp): @juraj add a {backend} = s3, postgres, mongo, local_file The signature schema might be dependent on the backend E.g., bulk/airflow/downloaded_1min/csv/ohlcv/futures/universe_v1_0/ccxt/binance/v1_0-20220210/BTC_USD.csv.gz We use - to separate pieces of the same attribute (e.g., version and snapshot) and _ as replacements of a space character. The organization of files in directories should reflect the naming scheme. We always use one directory per attribute for files (e.g., bulk.airflow.csv/... or bulk/airflow/csv/... ). When the metadata is used not to identify a file in the filesystem (e.g., for a script or as a tag) then we use . as separators between the attributes. Data set attributes . There are several \"attributes\" of a data set: download_mode : the type of downloading mode bulk Aka \"one-shot\", \"one-off\", and improperly \"historical\" Data downloaded in bulk mode, as one-off documented operations Sometimes it's referred to as \"historical\", since one downloads the historical data in bulk before the real-time flow is deployed periodic Aka \"scheduled\", \"streaming\", \"continuous\", and improperly \"real-time\" Data is captured regularly and continuously Sometimes it's referred as to \"real-time\" since one capture this data It can contain information about the frequency of downloading (e.g., periodic-5mins , periodic-EOD ) if it needs to be identified with respect to others unit_test Data used for unit test (independently if it was downloaded automatically or created manually) downloading_entity : different data depending on whom downloaded it, e.g., airflow : data was downloaded as part of the automatic flow manual : data download was triggered manually (e.g., running the download script) action_tag : information about the downloading, e.g., downloaded_1min or downloaded_EOD data_format : the format of the data, e.g., csv (always csv.gz, there is no reason for not compressing the data) parquet data_type : what type of data is stored, e.g., ohlcv , bid_ask , market_depth (aka order_book ), bid_ask_market_data (if it includes both), trades asset_type : what is the asset class E.g., futures, spot, options universe : the name of the universe containing the possible assets Typically the universe can have further characteristics and it can be also versioned E.g., universe_v1_7 vendor : the source that provided the data Aka \"provider\" E.g., ccxt , crypto_chassis , cryptodata_download , talos , kaiko , Data can also be downloaded directly from an exchange (e.g., coinbase , binance ) exchange_id : which exchange the data refers to E.g., binance version : any data set needs to have a version Version is represented as major, minor, patch according to semantic versioning in the format v{a}_{b}_{c} (e.g., v1_0_0) If the schema of the data is changed the major version is increased If a bug is fixed in the downloader that improves the semantic of the data but it's not a backward incompatible change, the minor version is increased The same version can also include an optional snapshot which refers to the date when the data was downloaded (e.g., a specific date 20220210 to represent when the day on which the historical data was downloaded, i.e., the data was the historical data as-of 2022-02-10) Note that snapshot and version have an overlapping but not identical meaning. snapshot represents when the data was downloaded, while version refers to the evolution of the semantic of the data and of the downloader. E.g., the same data source can be downloaded manually on different days with the same downloader (and thus with the same version). asset_type : which cryptocurrency the data refers to: Typically, there is one file per asset (e.g., BTC_USDT.csv.gz ) Certain data formats can organize the data in a more complex way E.g., Parquet files save the data in a directory structure {asset}/{year}/{month}/data.parquet It is possible that a single data set covers multiple values of a specific attribute E.g., a data set storing data for both futures and spot, can have asset_type=futures_spot Not all the cross-products are possible, e.g. there is no data set with download_mode=periodic scheduled by Airflow and downloading_entity=manual We organize the schema in terms of access pattern for the modeling and analysis stage E.g., snapshot comes before vendor since in different snapshots we can have different universes E.g., snapshot -> dataset -> vendor -> exchange -> coin A universe is just a mapping of a tag (e.g., v5) to a set of directories Each data set has multiple columns. References . The list of data sources on CK S3 bucket is [Bucket data organization]{.underline} Useful notebooks for processing data is [Master notebooks]{.underline} CK data specs: [Data pipelines - Specs]{.underline} Data on-boarding flow Downloader types. For each data set, there are typically two types of downloaders: bulk and periodic. This step is often the equivalent of the Extract phase in an ETL / ELT / EtLT pipeline. E.g., an EtLT pipeline can consists of the following phases: E: extract 1 minute data from websocket, t: apply non-business logic related transformation from JSON to dataframe L: load into SQL T: transform the data resampling to 5 minute data Bulk downloaders . Download past data querying from an API. A characteristic of bulk downloaders is that the download is not scheduled to be repeated on a regular basis. It is mostly executed once (e.g., to get the historical data) or a few times (e.g., to catch up with an intermittent data feed). It is executed (or at least triggered) manually. The data is downloaded in bulk mode at $T_{dl,bulkhist}$ to catch up with the historical data up to the moment of the deployment of the periodic downloader scheduled every period $\\Delta t_{deploy,periodic}$. The bulk download flow is also needed any time we need to \"catch up\" with a missing periodic download, e.g., if the real-time capture system was down. Preferred bulk data format . Typically the data is saved in a format that allows data to be loaded depending on what's needed from downstream systems (e.g., Parquet using tiles on assets and period of times). Periodic downloaders . Download the data querying an API every period $\\Delta T_{dl,periodic}$, which depends on the application needs, e.g., every second, minute. Typically periodic downloaders are triggered automatically (e.g., by a workflow orchestrator like Apache Airflow). Another possible name is \"streaming\" data. Typical example is a websocket feed of continuous data. Preferred real-time data format . Typically we save data in a DB to be able to easily query the data from the model. Often we also save data to an historical-like format to have a backup copy of the data. Parquet format is not ideal since it's not easy to append data. TODO(gp): Add diagram Providers -> us It's the extract in ETL Downloader naming scheme. A downloader has a name that represents the characteristics of the data that is being downloaded in the format above. The downloaders usually don't encapsulate logic to download only a single dataset. This means that the naming conventions for downloaders are less strict than for the datasets themselves. More emphasis is put into providing a comprehensive docstring We can't use . in filenames as attribute separators because Python uses them to separate packages in import statements, so we replace them with _ in scripts The name should capture the most general use-case E.g. if a downloader can download both OHLCV and Bid/Ask data for given exchange in a given time interval and save to relational DB or S3 we can simply name it download_exchange_data.py TODO(Juraj): explain that Airflow DAG names follow similar naming conventions Notebooks and scripts follow the naming scheme using a description (e.g., resampler , notebook ) instead of downloader and a proper suffix (e.g., ipynb, py, sh) TODO(gp): The first cell of a notebook contains a description of the content, including which checks are performed Production notebooks decide what is an error, by asserting Idempotency and catch-up mode . TODO(gp): Add a paragraph on this. This is used for any data pipeline (both downloading and processing). Example . An example of system downloading price data has the following components +------+--------------+--------------+--------+------------+-------+ | ** | Dataset | _ | _ | Data | * | | Acti | signature | Frequency** | Dashb | location | *Acti | | on | | | oard | | ve?** | +======+==============+==============+========+============+=======+ | Hi | | - All of | ht | s3://... | Yes | | stor | | the past | tps:// | | | | ical | | day data | | | | | down | | | | | | | load | | - Once a | | | | | | | day at | | | | | | | 0:00:00 | | | | | | | UTC | | | | +------+--------------+--------------+--------+------------+-------+ | R | | - Last | ... | s3://... | Yes | | eal- | | minute | | | | | time | | data | | | | | down | | | | | | | load | | - Every | | | | | | | minute | | | | +------+--------------+--------------+--------+------------+-------+ [Airflow Active]{.underline} [Downloaders]{.underline} Describe ETL layer from [Design - Software components]{.underline} [OHLCV data pipeline]{.underline} Data QA workflows Quality-assurance metrics . Each data set has QA metrics associated with it to make sure the data has the minimum expected data quality. E.g., for 1-minute OHLCV data, the possible QA metrics are: missing bars (timestamp) missing / nan OHLCV values within an individual bar points with volume = 0 data points where OHLCV data is not in the correct relationship (e.g., H and L are not higher or lower than O and C), data points where OHLCV data are outliers (e.g., they are more than N standard deviations from the running mean) Bulk data single-dataset QA metrics . It is possible to run the QA flow to compute the quality of the historical data. This is done typically as a one-off operation right after the historical data is downloaded in bulk. This touches only one dataset, namely the one that was just downloaded. Periodic QA metrics . Every N minutes of downloading real-time data, the QA flow is run to generate statistics about the quality of the data. In case of low data quality data the system sends a notification. Cross-datasets QA metrics . There are QA workflows that compare different data sets that are related to each other, e.g., consider the case of downloading the same data (e.g., 1-minute OHLCV for spot BTC_USDT from Binance exchange) from different providers (e.g., Binance directly and a third-party provider) and wanting to compare the data under the assumption to be the same - consider the case where there is a REST API that allows to get data for a period of data, and a WebSocket that streams the data consider the case where one gets an historical dump of the data from a third party provider vs the data from the exchange real-time stream consider the case of NASDAQ streaming data vs TAQ data disseminated once the market is close Historical / real-time QA flow. Every period $T_{dl,hist}$, a QA flow is run where the real-time data is compared to the historical data to ensure that the historical view of the data matches the real-time one. This is necessary but not sufficient to guarantee that the bulk historical data can be reliably used as a proxy for the real-time data as-of, in fact this is simply a self-consistency check. We do not have any guarantee that the data source collected correct historical data. Data QA workflow naming scheme. A QA workflow has a name that represents its characteristics in the format: {qa_type}.{dataset_signature} E.g., production_qa.{download_mode}.{downloading_entity}.{action_tag}.{data_format}.{data_type}.{asset_type}.{universe}.{vendor}.{exchange}.{version[-snapshot]}.{asset}.{extension} E.g., research_cross_comparison.periodic.airflow.downloaded_1sec_1min.all.bid_ask.futures.all.ccxt_cryptochassis.all.v1_0_0 where: qa_type : the type of the QA flow, e.g., production_qa : perform a QA flow on historical and real-time data. The interface should be an IM client and thus it should be possible to run QA on both historical and real-time data research_analysis : perform a free-form analysis of the data. This can then be the basis for a qa analysis compare_historical_real_time : compare historical and real-time data coming from the same source of data compare_historical_cross_comparison : compare historical data from two different data sources The same rules apply as in downloader and derived dataset for the naming scheme. Since cross-comparison involves two (or more dataset) we use a short notation merging the attributes that differ. E.g., a comparison between the datasets periodic.1minute.postgres.ohlcv.futures.1minute.ccxt.binance periodic.1day.postgres.ohlcv.futures.1minute.ccxt.binance is called: compare_qa.periodic.1minute-1day.postgres.ohlcv.futures.1minute.ccxt.binance since the only difference is in the frequency of the data sampling. It is possible to use a long format {dataset_signature1}-vs-{dataset_signature2} . Examples . +------+---------+----------------+------------+------+-------+-----+ | ** | ** | * | F | * | * | ** | | Symb | Dataset | *Description | requency | Das | Data | Act | | olic | sign | | | hboa | locat | ive | | na | ature | | | rd | ion | ? | | me | | | | | | | +======+=========+================+============+======+=======+=====+ | hist | His | - | - All of | | s3:/ | Yes | | dl1 | torical | | the | | /... | | | | d | | past | | | | | | ownload | | day | | | | | | | | data | | | | | | | | | | | | | | | | - Once a | | | | | | | | day at | | | | | | | | | | | | | | | | 0:00:00 | | | | | | | | UTC | | | | +------+---------+----------------+------------+------+-------+-----+ | rt | Re | - | Every | | s3:/ | Yes | | _dl1 | al-time | | minute | | /... | | | | d | | | | | | | | ownload | | | | | | +------+---------+----------------+------------+------+-------+-----+ | rt | Re | Check QA | Every 5 | | s3:/ | Yes | | _dl1 | al-time | metrics for | minutes | | /... | | | .qa1 | QA | dl1 | | | | | | | check | | | | | | +------+---------+----------------+------------+------+-------+-----+ | h | Check | Check | Once a day | | | | | ist | of | consistency | at 0:15:00 | | | | | dl1. | his | between | UTC | | | | | rt_d | torical | historical and | | | | | | l1.c | vs | real-time CCXT | | | | | | heck | re | binance data | | | | | | | al-time | | | | | | +------+---------+----------------+------------+------+-------+-----+ | rt | Re | - vendor | Every | | s3:/ | Yes | | _dl2 | al-time | =CryptoChassis | minute | | /... | | | | d | | | | | | | | ownload | - ex | | | | | | | | change=Binance | | | | | | | | | | | | | | | | - data | | | | | | | | | | | | | | | | type=bid/ask | | | | | +------+---------+----------------+------------+------+-------+-----+ | rt | Re | Check QA | Every 5 | | s3:/ | Yes | | _dl2 | al-time | metrics for | minutes | | /... | | | .qa2 | QA | dl3 | | | | | | | check | | | | | | +------+---------+----------------+------------+------+-------+-----+ | rt_d | Cro | Compare data | Every 5 | | | | | l1_d | ss-data | from rt_dl1 | minutes | | | | | l2.c | QA | and rt_dl2 | | | | | | heck | check | | | | | | +------+---------+----------------+------------+------+-------+-----+ Derived data workflows Derived data workflows . Data workflows can transform datasets into other datasets E.g., resample 1 second data into 1 minute data The data is then written back to the same data source as the originating data (e.g., DB for period / real-time data, Parquet / csv / S3 for historical data). TODO(gp): Add a plot (we are the source of the provider) Derived data naming scheme . We use the same naming scheme as in downloaded data set {dataset_signature} but we encode the information about the content of the newly generated data in the action_tag attribute of the data, e.g., resample_1min to distinguish it from downloaded_1sec . We use this approach so that the scheme of the derived data is the same as a downloaded data set. Derived data research flow. The goal is to decide how to transform the raw data into derived data and come up with QA metrics to assess the quality of the transformation It can be cross-vendor or not E.g., sample 1sec data to 1min and compare to a reference. The sampling is done on the fly since the researcher is trying to understand how to resample (e.g., removing outliers) to get a match Derived data production flow. The research flow is frozen and put in production E.g., run the resample script to sample and write back to DB and historical data. This flow can be run in historical mode (to populate the backend with the production data) and in real-time mode (to compute the streaming data) Derived data QA flow . the goal is to monitor that the production flow is still performing properly with respect to the QA metrics E.g., the 1-sec to 1-min resampling is not performed on-the-fly, but it uses the data already computed by the script in the production flow. This flow is mainly run in real-time, but we might want to look at QA performance also historically This same distinction can also be applied to feature computation and to the machine learning flow. Provider -> data -> Us -> derived flow -> Us -> features -> Us -> ML -> Exchange Data formats Storage-invariance of data. Data should be independent from its storage format, e.g., CSV, Parquet, Relational DB. In other words, converting data from one format to another should not yield losing any information. Data stored by-date. The by-date representation means that there is a file for the data for all the assets: 1546871400 ... Bitcoin 1546871400 ... Ethereum 1546871401 ... 1546871402 ... Thus the same timestamp is repeated for all the assets The original format of the data is by-date and for a single day is \"20190107.pq\" like: vendor_date start_time end_time ticker open close volume id 2019-01-07 1546871400 1546871460 A 65.64 65.75 19809 16572.0 2019-01-07 1546871400 1546871460 AA 28.53 28.52 31835 1218568.0 2019-01-07 1546871400 1546871460 AAAU 12.92 12.92 11509 1428781.0 2019-01-07 1546871400 1546871460 AABA 58.90 58.91 7124 10846.0 {width=\"6.5in\" height=\"1.0416666666666667in\"} There are 2 special columns in the by-date file: one that represents the timestamp (\"start_time\" in the example). This is unique and monotonic (start_time, ticker) are unique one that represents the asset (\"ticker\" in the example) Data stored by-asset. By-asset data means that there is a single file for each single asset with the data for all the timestamps (no timestamp is repeated in a single file) Bitcoin.pq 1546871400 ... 1546871401 1546871402 Eth.pq 1546871400 ... 1546871401 1546871402 By-asset Parquet data. Data pipelines can transform by-asset data into Parquet data, preserving all the columns. Successive stages of the pipeline perform other data transformations. By-asset means that the asset that is in the innermost directory dst_dir/ year=2021/ month=12/ day=11/ asset=BTC_USDT/ data.parquet asset=ETH_USDT/ data.parquet Typically, the by-date format is just a format that we receive data from, and we don't want to transform data to. The name of the asset can depend on the data and it can be asset , currency_pair , ticker . By default we use names of columns from the data and we reindex the partitioned dataframe on datetime, so saved parquet will all have the same datetime index. Partitioning on year/month/day is optional and should be provided as a switch in partitioning function. Data stored by-tile. Data is organized by tile when the Parquet files are partitioned by asset, by year, and by month so that it's possible to read only a chunk of that asset=BTC_USDT/ year=2021/ month=12/ data.parquet year=2021/ month=11/ data.parquet ... asset=ETH_USDT/ year=2021/ month=12/ data.parquet ... Sandbox This paragraph describes an example of infrastructure that implements the Sorrentum Data Protocol. The It is a Docker Container containing the following services: Airflow Jupyter notebook Postgres MongoDB Notes for CK devs (TO REMOVE) It is a separated code base from Sorrentum and it shares only a few base library (e.g., helpers ) It is a scaled down version of CK production infrastructure (e.g., managed Airflow is replaced by a local Airflow instance) The system follows closely the production system described in [Data pipelines - Specs]{.underline} Data client stack Once the data is downloaded, it needs to be retrieved for processing in a common format (e.g., DataFlow format). We use a two layer approach to split the complexity / responsibilities of reading the data ImClient Data is downloaded and saved with minimal or no transformation Adapts from the vendor data to a standard internal \"MarketData\" format MarketData` implements \"behaviors\" that are orthogonal to vendors, e.g., RealTime or Historical Stitched (i.e.,\"overlap\" multiple data sources giving a single view of the data). E.g., the data from the last day comes from a real-time source while the data before that comes from an historical source Replayed = serialize the data to disk and read it back, implementing also knowledge time as-of-time semantic. This behavior is orthogonal to RealTime, Historical, Stitched, i.e., one can replay any MarketData , including an already replayed one Data format for ImClient / MarketData pipeline Both ImClient and MarketData have an output format that is enforced by the base and the derived classes together ImClient and MarketData have 3 interfaces each: an external \"input\" format for a class format of the data as input to a class derived from MarketData / ImClient an internal \"input\" format format that derived classes need to follow so that the corresponding base class can do its job, i.e., apply common transformations to all MarketData / ImClient classes an external \"output\" format format of the data outputted by any derived class from MarketData / ImClient - The chain of transformations is: - Class derived from ImClient - The transformations are vendor-specific ImClient The transformations are fixed Class derived from MarketData The transformations are specific to the MarketData concrete class :qa MarketData The transformations are fixed \\[Vendor\\] -> \\[DerivedImClient\\] \\[DerivedImClient\\] -> \\[AbstractImClient\\] \\[AbstractImClient\\] -> \\[DerivedMarketData\\] \\[DerivedMarketData\\] -> \\[AbstractMarketData\\] Transformations performed by classes derived from ImClient Whatever is needed to transform the vendor data into the internal format accepted by base ImClient Only derived classes ImClient knows what is exact semantic of the vendor-data Transformations performed by abstract class ImClient Implemented by ImClient._apply_im_normalization() Output format of ImClient TODO(*): Check the code in ImClient since that might be more up to date than this document and, if needed, update this doc The data in output of a class derived from ImClient is normalized so that: the index: represents the knowledge time is the end of the sampling interval is called timestamp is a tz-aware timestamp in UTC the data: is resampled on a 1 minute grid and filled with NaN values is sorted by index and full_symbol is guaranteed to have no duplicates belongs to intervals like [a, b] has a full_symbol column with a string representing the canonical name of the instrument TODO(gp): We are planning to use an ImClient data format closer to MarketData by using start_time , end_time , and knowledge_time since these can be inferred only from the vendor data semantic Transformations performed by classes derived from MarketData Classes derived from MarketData do whatever they need to do in _get_data() to get the data, but always pass back data that: is indexed with a progressive index has asset, start_time, end_time, knowledge_time start_time, end_time, knowledge_time are timezone aware E.g., asset_id start_time end_time close volume idx 0 17085 2021-07-26 13:41:00+00:00 2021-07-26 13:42:00+00:00 148.8600 400176 1 17085 2021-07-26 13:30:00+00:00 2021-07-26 13:31:00+00:00 148.5300 1407725 2 17085 2021-07-26 13:31:00+00:00 2021-07-26 13:32:00+00:00 148.0999 473869 Transformations performed by abstract class MarketData The transformations are done inside get_data_for_interval() , during normalization, and are: indexing by end_time converting end_time , start_time , knowledge_time to the desired timezone sorting by end_time and asset_id applying column remaps Output format of MarketData - The base MarketData normalizes the data by: - sorting by the columns that correspond to end_time and asset_id - indexing by the column that corresponds to end_time , so that it is suitable to DataFlow computation E.g., asset_id start_time close volume end_time 2021-07-20 09:31:00-04:00 17085 2021-07-20 09:30:00-04:00 143.990 1524506 2021-07-20 09:32:00-04:00 17085 2021-07-20 09:31:00-04:00 143.310 586654 2021-07-20 09:33:00-04:00 17085 2021-07-20 09:32:00-04:00 143.535 667639 Asset ids format ImClient uses assets encoded as full_symbols strings (e.g., binance::BTC_UTC ) There is a vendor-specific mapping: from full_symbols to corresponding data from asset_ids (ints) to full_symbols (strings) If the asset_ids -> full_symbols mapping is provided by the vendor, then we reuse it Otherwise, we build a mapping hashing full_symbols strings into numbers MarketData and everything downstream uses asset_ids that are encoded as ints This is because we want to use ints and not strings in dataframe Handling of asset_ids Different implementations of ImClient backing a MarketData are possible, e.g.: The caller needs to specify the requested asset_ids In this case the universe is provided by MarketData when calling the data access methods The reading backend is initialized with the desired universe of assets and then MarketData just uses or subsets that universe For these reasons, assets are selected at 3 different points: MarketData allows to specify or subset the assets through asset_ids through the constructor ImClient backends specify the assets returned E.g., a concrete implementation backed by a DB can stream the data for its entire available universe Certain class methods allow querying data for a specific asset or subset of assets For each stage, a value of None means no filtering Handling of filtering by time Clients of MarketData might want to query data by: using different interval types, namely `[a, b), [a, b], (a, b], (a, b)` filtering on either the start_ts or end_ts For this reason, this class supports all these different ways of providing data ImClient has a fixed semantic of the interval \\[a, b\\] MarketData adapts the fixed semantic to multiple ones Handling timezone ImClient always uses UTC as output MarketData adapts UTC to the desired timezone, as requested by the client Checklist for releasing a new data set Decide what to do exactly (e.g., do we download only bulk data or also real-time?) Review what code we have and what can be generalized to accomplish the task at hand Decide what's the name of the data set according to our convention Create DAGs for Airflow Update the Raw Data Gallery im_v2/common/notebooks/Master_raw_data_gallery.ipynb Quick exploratory analysis to make sure the data is not malformed Update the table in Data pipelines - Specs Exploratory analysis for a thorough QA analysis Add QA system to Airflow prod","title":"DataPull"},{"location":"DataPull/#datapull","text":"","title":"DataPull"},{"location":"DataPull/#asset-representation","text":"TODO(gp): Ideally we want to use a single schema like Vendor:ExchangeId:Asset \\Asset universe","title":"Asset representation"},{"location":"DataPull/#etl","text":"We employ a variation of the ETL approach, called EtLT (i.e., extract, lightly transform, load, transform) for downloading both data and metadata. We can have a different pipeline for data and one metadata. Data is extracted from an external data source, lightly transformed, and then loaded into permanent storage. Then downstream data pipelines read the data with a standard client interface. Large variety of data. Data comes in a very large variety, for instance: Different vendor can provide the same data E.g., Kibot, Binance, CryptoDataDownload provide data for the Binance exchange Different time semantics, e.g., Intervals can be [a, b) or (a, b] A bar can be marked at the end or at the beginning of the interval Data and metadata Some vendors provide metadata, others don't Multiple asset classes (e.g., equities, futures, crypto) Data at different time resolutions, e.g., daily bars minute bars trades order book data Historical vs real-time Price data vs alternative data Storage backend . Data can be saved in multiple storage backends: - database (e.g., Postgres, MongoDB) - local filesystem - remote filesystems (e.g., AWS S3 bucket) Data can be saved on filesystems in different formats (e.g., CSV, JSON, Parquet). S3 vs local filesystem. Unfortunately it's not easy to abstract the differences between AWS S3 buckets and local filesystems, since the S3 interface is more along a key-value store rather than a filesystem (supporting permissions, deleting recursively a directory, moving, etc.). Solutions based on abstracting a filesystem on top of S3 (e.g., mounting S3 with Fuse filesystems) are not robust enough. Some backends (e.g., Parquet) allow handling an S3 bucket transparently. Our typical approach is: When writing to S3, use the local filesystem for staging the data in the desired structure and then copy all the data to S3 When reading from S3, read the data directly, use the functionalities supported by the backend, or copy the data locally and then read it from local disk Data formats . The main data formats that Sorrentum supports are: CSV Pros Easy to inspect Easy to load / save Everybody understands it Cons Data can't be easily sliced by asset ids / by time Large footprint (non binary), although it can be compressed (e.g., as .csv.gz on the fly) Parquet Pros Compressed AWS friendly Data can be easily sliced by asset ids and time Cons Not easy to inspect Solution: use wrapper to convert to CSV Difficult to append Solution: use chunking + defragmentation Cumbersome for real-time data database Pros Easy to inspect Support any access pattern Friendly for real-time data Cons Need devops to manage database instance Difficult to track lineage and version Unfortunately there is not an obvious best solution so we have to deal with multiple representations and transforming between them. In practice Parquet is better suited to store historical data and database to store real time data. Extract stage . The goal is to acquire raw data from an external source and archive it into a permanent storage backend (e.g., file-system and/or database). The data can be either historical or real-time. We typically don't process the data at all, but rather we prefer to save the data raw as it comes from the wire. Transform stage . Typically, we prefer to load the data in the backend with minor or no transformation. Specifically we allow changing the representation of the data / format (e.g., removing some totally useless redundancy, compressing the data, transforming from strings to datetimes). We don't allow changing the semantics or filter columns. This is done dynamically in the client stage Load stage. The load stage simply saves the data into one of the supported backends. Typically, we prefer to save Historical data into Parquet format since it supports more naturally the access patterns needed for long simulations Real-time data into a database since this makes it easy to append and retrieve data in real-time. Often we want to also append real-time data to Parquet Client stage. The client stage allows downstream pipelines to access data from the backend storage. The access pattern is always for a model is always \"give me the columns XYZ for assets ABC in the period [..., ...]\". We prefer to perform some transformations that are lightweight (e.g., converting Unix epochs in timestamps) or still evolving (e.g., understanding the timing semantic of the data) are performed inside this stage, rather than in the transform stage. ETL primitives . We implement basic primitives that can be combined in different ways to create various ETL pipelines. Extract: Read data from an external source to memory (typically in the form of Pandas data structures) E.g., downloading data from a REST or Websocket interface Load: Load data stored in memory -> permanent storage (e.g., save as CSV or as Parquet) E.g., pd.to_parquet() DbSave Save to DB Create schema Client: From a permanent storage (e.g., disk) -> Memory E.g., pd.from_parquet() ClientFromDb DB -> Memory Creates the SQL query to read the data Validator https://github.com/cryptokaizen/cmamp/pull/3386/files Transform Just a class Example of ETL pipelines. Download historical data and save it as CSV or PQ Download order book data, compress it, and save it on S3 Insert 1 minute worth of data in the DB Write data into DB One could argue that operations on the DB might not look like extract but rather load We treat any backend (S3, local, DB) in the same way and the DB is just a backend [More detailed description]{.underline} Examples : Transformations CSV -> Parquet <-> DB Convert the CSV data into Parquet using certain indices Convert Parquet by-date into Parquet by-asset","title":"ETL"},{"location":"DataPull/#data-pipelines","text":"Download data by asset (as a time series) asset.csv.gz or Parquet Historical format Download data by time, e.g., 20211105/... csv.gz or Parquet This is typical of real-time flow","title":"Data pipelines"},{"location":"DataPull/#general-conventions","text":"Data invariants . We use the following invariants when storing data during data on-boarding and processing: - Data quantities are associated to intervals are [a, b) (e.g., the return over an interval) or to a single point in time (e.g., the close price at 9am UTC) - Every piece of data is labeled with the end of the sampling interval or with the point-in-time - E.g., for a quantity computed in an interval [06:40:00, 06:41:00) the timestamp is 06:41:00 - Timestamps are always time-zone aware and use UTC timezone - Every piece of data has a knowledge timestamp (aka \"as-of-date\") which represent when we were aware of the data according to our wall-clock: - Multiple timestamps associated with different events can be tracked, e.g., start_download_timestamp , end_download_timestamp - No system should depend on data available strictly before the knowledge timestamp - Data is versioned: every time we modify the schema or the semantics of the data, we bump up the version using semantic versioning and update the changelog of what each version contains An example of tabular data is below: {width=\"6.5in\" height=\"1.0138888888888888in\"} Data organization . We keep data together by execution run instead of by data element. E.g., assume we run a flow called XYZ_sanity_check every day and the flow generates three pieces of data, one file output.txt and two directories logs , temp_data . We want to organize the data in a directory structure like: Better - XYZ_sanity_check/ - run.{date}/ - output.txt - logs/ - temp_data/ - run.{date}.manual/ - output.txt - logs/ - temp_data/ Worse - XYZ_sanity_check/ - output.{date}/ - output.txt - logs.{date}/ - temp_data.{date}/ The reasons why the first data layout is superior are: It's easier to delete a single run by deleting a single dir instead of deleting multiple files It allows the format of the data to evolve over time without having to change the schema of the data retroactively It allows scripts post-processing the data to point to a directory with a specific run and work out of the box it's easier to move the data for a single run from one dir (e.g., locally) to another (e.g., a central location) in one command there is redundancy and visual noise, e.g., the same data is everywhere We can tag directory by a run mode (e.g., manual vs scheduled ) by adding the proper suffix to a date-dir. Directory with one file . Having a directory containing one single file often creates redundancy. We prefer not to use directories unless they contain more than one file. We can use directories if we believe that it's highly likely that more files will be needed, but as often happens YANGI (you are not going to need it) applies. Naming convention . We use . to separate conceptually different pieces of a file or a directory. We don't allow white spaces since they are not Linux friendly and need to be escaped. We replace white spaces with _ . We prefer not to use - whenever possible, since they create issues with Linux auto-completion and need to be escaped. E.g., bulk.airflow.csv instead of bulk_airflow.csv Data pipeline classification . A data pipeline can be any of the following: a downloader External DB (e.g., data provider) -> Internal DB: the data flows from an external API to an internal DB It downloads historical or real-time data and saves the dataset in a location The name of the script and the location of the data downloaded follow the naming scheme described below It is typically implemented as a Python script a QA flow for a single or multiple datasets Internal DB -> Process It computes some statistics from one or more datasets (primary or derived) and throws an exception if the data is malformed It aborts if the data has data not compliant to certain QA metrics It is typically implemented as a Python notebook backed by a Python library a derived dataset flow Internal DB -> Process -> Internal DB It computes some data derived from an existing data set E.g., resampling, computing features It is typically implemented as a Python script a model flow Internal DB -> Process -> Outside DB (e.g., exchange) E.g., it runs a computation from internal data and places some trades It is typically implemented as a Python script Data classification . Data can be from market sources or from non-market (aka alternative) sources. Each data source can come with metadata, e.g., List of assets in the universe over time Attributes of assets (e.g., industry and other classification) Asset universe . Often the data relates to a set of assets, e.g., currency pairs on different exchanges. The support of the data is referred to as the \"data universe\". This metadata is versioned as any other piece of data.","title":"General conventions"},{"location":"DataPull/#data-set-downloading-and-handling","text":"Data set naming scheme . Each data set is stored in a data lake with a path and name that describe its metadata according to the following signature: dataset_signature={download_mode}.{downloading_entity}.{action_tag}.{data_format}.{data_type}.{asset_type}.{universe}.{vendor}.{exchange_id}.{version[-snapshot]}.{extension} TODO(gp): @juraj add a {backend} = s3, postgres, mongo, local_file The signature schema might be dependent on the backend E.g., bulk/airflow/downloaded_1min/csv/ohlcv/futures/universe_v1_0/ccxt/binance/v1_0-20220210/BTC_USD.csv.gz We use - to separate pieces of the same attribute (e.g., version and snapshot) and _ as replacements of a space character. The organization of files in directories should reflect the naming scheme. We always use one directory per attribute for files (e.g., bulk.airflow.csv/... or bulk/airflow/csv/... ). When the metadata is used not to identify a file in the filesystem (e.g., for a script or as a tag) then we use . as separators between the attributes. Data set attributes . There are several \"attributes\" of a data set: download_mode : the type of downloading mode bulk Aka \"one-shot\", \"one-off\", and improperly \"historical\" Data downloaded in bulk mode, as one-off documented operations Sometimes it's referred to as \"historical\", since one downloads the historical data in bulk before the real-time flow is deployed periodic Aka \"scheduled\", \"streaming\", \"continuous\", and improperly \"real-time\" Data is captured regularly and continuously Sometimes it's referred as to \"real-time\" since one capture this data It can contain information about the frequency of downloading (e.g., periodic-5mins , periodic-EOD ) if it needs to be identified with respect to others unit_test Data used for unit test (independently if it was downloaded automatically or created manually) downloading_entity : different data depending on whom downloaded it, e.g., airflow : data was downloaded as part of the automatic flow manual : data download was triggered manually (e.g., running the download script) action_tag : information about the downloading, e.g., downloaded_1min or downloaded_EOD data_format : the format of the data, e.g., csv (always csv.gz, there is no reason for not compressing the data) parquet data_type : what type of data is stored, e.g., ohlcv , bid_ask , market_depth (aka order_book ), bid_ask_market_data (if it includes both), trades asset_type : what is the asset class E.g., futures, spot, options universe : the name of the universe containing the possible assets Typically the universe can have further characteristics and it can be also versioned E.g., universe_v1_7 vendor : the source that provided the data Aka \"provider\" E.g., ccxt , crypto_chassis , cryptodata_download , talos , kaiko , Data can also be downloaded directly from an exchange (e.g., coinbase , binance ) exchange_id : which exchange the data refers to E.g., binance version : any data set needs to have a version Version is represented as major, minor, patch according to semantic versioning in the format v{a}_{b}_{c} (e.g., v1_0_0) If the schema of the data is changed the major version is increased If a bug is fixed in the downloader that improves the semantic of the data but it's not a backward incompatible change, the minor version is increased The same version can also include an optional snapshot which refers to the date when the data was downloaded (e.g., a specific date 20220210 to represent when the day on which the historical data was downloaded, i.e., the data was the historical data as-of 2022-02-10) Note that snapshot and version have an overlapping but not identical meaning. snapshot represents when the data was downloaded, while version refers to the evolution of the semantic of the data and of the downloader. E.g., the same data source can be downloaded manually on different days with the same downloader (and thus with the same version). asset_type : which cryptocurrency the data refers to: Typically, there is one file per asset (e.g., BTC_USDT.csv.gz ) Certain data formats can organize the data in a more complex way E.g., Parquet files save the data in a directory structure {asset}/{year}/{month}/data.parquet It is possible that a single data set covers multiple values of a specific attribute E.g., a data set storing data for both futures and spot, can have asset_type=futures_spot Not all the cross-products are possible, e.g. there is no data set with download_mode=periodic scheduled by Airflow and downloading_entity=manual We organize the schema in terms of access pattern for the modeling and analysis stage E.g., snapshot comes before vendor since in different snapshots we can have different universes E.g., snapshot -> dataset -> vendor -> exchange -> coin A universe is just a mapping of a tag (e.g., v5) to a set of directories Each data set has multiple columns. References . The list of data sources on CK S3 bucket is [Bucket data organization]{.underline} Useful notebooks for processing data is [Master notebooks]{.underline} CK data specs: [Data pipelines - Specs]{.underline}","title":"Data set downloading and handling"},{"location":"DataPull/#data-on-boarding-flow","text":"Downloader types. For each data set, there are typically two types of downloaders: bulk and periodic. This step is often the equivalent of the Extract phase in an ETL / ELT / EtLT pipeline. E.g., an EtLT pipeline can consists of the following phases: E: extract 1 minute data from websocket, t: apply non-business logic related transformation from JSON to dataframe L: load into SQL T: transform the data resampling to 5 minute data Bulk downloaders . Download past data querying from an API. A characteristic of bulk downloaders is that the download is not scheduled to be repeated on a regular basis. It is mostly executed once (e.g., to get the historical data) or a few times (e.g., to catch up with an intermittent data feed). It is executed (or at least triggered) manually. The data is downloaded in bulk mode at $T_{dl,bulkhist}$ to catch up with the historical data up to the moment of the deployment of the periodic downloader scheduled every period $\\Delta t_{deploy,periodic}$. The bulk download flow is also needed any time we need to \"catch up\" with a missing periodic download, e.g., if the real-time capture system was down. Preferred bulk data format . Typically the data is saved in a format that allows data to be loaded depending on what's needed from downstream systems (e.g., Parquet using tiles on assets and period of times). Periodic downloaders . Download the data querying an API every period $\\Delta T_{dl,periodic}$, which depends on the application needs, e.g., every second, minute. Typically periodic downloaders are triggered automatically (e.g., by a workflow orchestrator like Apache Airflow). Another possible name is \"streaming\" data. Typical example is a websocket feed of continuous data. Preferred real-time data format . Typically we save data in a DB to be able to easily query the data from the model. Often we also save data to an historical-like format to have a backup copy of the data. Parquet format is not ideal since it's not easy to append data. TODO(gp): Add diagram Providers -> us It's the extract in ETL Downloader naming scheme. A downloader has a name that represents the characteristics of the data that is being downloaded in the format above. The downloaders usually don't encapsulate logic to download only a single dataset. This means that the naming conventions for downloaders are less strict than for the datasets themselves. More emphasis is put into providing a comprehensive docstring We can't use . in filenames as attribute separators because Python uses them to separate packages in import statements, so we replace them with _ in scripts The name should capture the most general use-case E.g. if a downloader can download both OHLCV and Bid/Ask data for given exchange in a given time interval and save to relational DB or S3 we can simply name it download_exchange_data.py TODO(Juraj): explain that Airflow DAG names follow similar naming conventions Notebooks and scripts follow the naming scheme using a description (e.g., resampler , notebook ) instead of downloader and a proper suffix (e.g., ipynb, py, sh) TODO(gp): The first cell of a notebook contains a description of the content, including which checks are performed Production notebooks decide what is an error, by asserting Idempotency and catch-up mode . TODO(gp): Add a paragraph on this. This is used for any data pipeline (both downloading and processing). Example . An example of system downloading price data has the following components +------+--------------+--------------+--------+------------+-------+ | ** | Dataset | _ | _ | Data | * | | Acti | signature | Frequency** | Dashb | location | *Acti | | on | | | oard | | ve?** | +======+==============+==============+========+============+=======+ | Hi | | - All of | ht | s3://... | Yes | | stor | | the past | tps:// | | | | ical | | day data | | | | | down | | | | | | | load | | - Once a | | | | | | | day at | | | | | | | 0:00:00 | | | | | | | UTC | | | | +------+--------------+--------------+--------+------------+-------+ | R | | - Last | ... | s3://... | Yes | | eal- | | minute | | | | | time | | data | | | | | down | | | | | | | load | | - Every | | | | | | | minute | | | | +------+--------------+--------------+--------+------------+-------+ [Airflow Active]{.underline} [Downloaders]{.underline} Describe ETL layer from [Design - Software components]{.underline} [OHLCV data pipeline]{.underline}","title":"Data on-boarding flow"},{"location":"DataPull/#data-qa-workflows","text":"Quality-assurance metrics . Each data set has QA metrics associated with it to make sure the data has the minimum expected data quality. E.g., for 1-minute OHLCV data, the possible QA metrics are: missing bars (timestamp) missing / nan OHLCV values within an individual bar points with volume = 0 data points where OHLCV data is not in the correct relationship (e.g., H and L are not higher or lower than O and C), data points where OHLCV data are outliers (e.g., they are more than N standard deviations from the running mean) Bulk data single-dataset QA metrics . It is possible to run the QA flow to compute the quality of the historical data. This is done typically as a one-off operation right after the historical data is downloaded in bulk. This touches only one dataset, namely the one that was just downloaded. Periodic QA metrics . Every N minutes of downloading real-time data, the QA flow is run to generate statistics about the quality of the data. In case of low data quality data the system sends a notification. Cross-datasets QA metrics . There are QA workflows that compare different data sets that are related to each other, e.g., consider the case of downloading the same data (e.g., 1-minute OHLCV for spot BTC_USDT from Binance exchange) from different providers (e.g., Binance directly and a third-party provider) and wanting to compare the data under the assumption to be the same - consider the case where there is a REST API that allows to get data for a period of data, and a WebSocket that streams the data consider the case where one gets an historical dump of the data from a third party provider vs the data from the exchange real-time stream consider the case of NASDAQ streaming data vs TAQ data disseminated once the market is close Historical / real-time QA flow. Every period $T_{dl,hist}$, a QA flow is run where the real-time data is compared to the historical data to ensure that the historical view of the data matches the real-time one. This is necessary but not sufficient to guarantee that the bulk historical data can be reliably used as a proxy for the real-time data as-of, in fact this is simply a self-consistency check. We do not have any guarantee that the data source collected correct historical data. Data QA workflow naming scheme. A QA workflow has a name that represents its characteristics in the format: {qa_type}.{dataset_signature} E.g., production_qa.{download_mode}.{downloading_entity}.{action_tag}.{data_format}.{data_type}.{asset_type}.{universe}.{vendor}.{exchange}.{version[-snapshot]}.{asset}.{extension} E.g., research_cross_comparison.periodic.airflow.downloaded_1sec_1min.all.bid_ask.futures.all.ccxt_cryptochassis.all.v1_0_0 where: qa_type : the type of the QA flow, e.g., production_qa : perform a QA flow on historical and real-time data. The interface should be an IM client and thus it should be possible to run QA on both historical and real-time data research_analysis : perform a free-form analysis of the data. This can then be the basis for a qa analysis compare_historical_real_time : compare historical and real-time data coming from the same source of data compare_historical_cross_comparison : compare historical data from two different data sources The same rules apply as in downloader and derived dataset for the naming scheme. Since cross-comparison involves two (or more dataset) we use a short notation merging the attributes that differ. E.g., a comparison between the datasets periodic.1minute.postgres.ohlcv.futures.1minute.ccxt.binance periodic.1day.postgres.ohlcv.futures.1minute.ccxt.binance is called: compare_qa.periodic.1minute-1day.postgres.ohlcv.futures.1minute.ccxt.binance since the only difference is in the frequency of the data sampling. It is possible to use a long format {dataset_signature1}-vs-{dataset_signature2} . Examples . +------+---------+----------------+------------+------+-------+-----+ | ** | ** | * | F | * | * | ** | | Symb | Dataset | *Description | requency | Das | Data | Act | | olic | sign | | | hboa | locat | ive | | na | ature | | | rd | ion | ? | | me | | | | | | | +======+=========+================+============+======+=======+=====+ | hist | His | - | - All of | | s3:/ | Yes | | dl1 | torical | | the | | /... | | | | d | | past | | | | | | ownload | | day | | | | | | | | data | | | | | | | | | | | | | | | | - Once a | | | | | | | | day at | | | | | | | | | | | | | | | | 0:00:00 | | | | | | | | UTC | | | | +------+---------+----------------+------------+------+-------+-----+ | rt | Re | - | Every | | s3:/ | Yes | | _dl1 | al-time | | minute | | /... | | | | d | | | | | | | | ownload | | | | | | +------+---------+----------------+------------+------+-------+-----+ | rt | Re | Check QA | Every 5 | | s3:/ | Yes | | _dl1 | al-time | metrics for | minutes | | /... | | | .qa1 | QA | dl1 | | | | | | | check | | | | | | +------+---------+----------------+------------+------+-------+-----+ | h | Check | Check | Once a day | | | | | ist | of | consistency | at 0:15:00 | | | | | dl1. | his | between | UTC | | | | | rt_d | torical | historical and | | | | | | l1.c | vs | real-time CCXT | | | | | | heck | re | binance data | | | | | | | al-time | | | | | | +------+---------+----------------+------------+------+-------+-----+ | rt | Re | - vendor | Every | | s3:/ | Yes | | _dl2 | al-time | =CryptoChassis | minute | | /... | | | | d | | | | | | | | ownload | - ex | | | | | | | | change=Binance | | | | | | | | | | | | | | | | - data | | | | | | | | | | | | | | | | type=bid/ask | | | | | +------+---------+----------------+------------+------+-------+-----+ | rt | Re | Check QA | Every 5 | | s3:/ | Yes | | _dl2 | al-time | metrics for | minutes | | /... | | | .qa2 | QA | dl3 | | | | | | | check | | | | | | +------+---------+----------------+------------+------+-------+-----+ | rt_d | Cro | Compare data | Every 5 | | | | | l1_d | ss-data | from rt_dl1 | minutes | | | | | l2.c | QA | and rt_dl2 | | | | | | heck | check | | | | | | +------+---------+----------------+------------+------+-------+-----+","title":"Data QA workflows"},{"location":"DataPull/#derived-data-workflows","text":"Derived data workflows . Data workflows can transform datasets into other datasets E.g., resample 1 second data into 1 minute data The data is then written back to the same data source as the originating data (e.g., DB for period / real-time data, Parquet / csv / S3 for historical data). TODO(gp): Add a plot (we are the source of the provider) Derived data naming scheme . We use the same naming scheme as in downloaded data set {dataset_signature} but we encode the information about the content of the newly generated data in the action_tag attribute of the data, e.g., resample_1min to distinguish it from downloaded_1sec . We use this approach so that the scheme of the derived data is the same as a downloaded data set. Derived data research flow. The goal is to decide how to transform the raw data into derived data and come up with QA metrics to assess the quality of the transformation It can be cross-vendor or not E.g., sample 1sec data to 1min and compare to a reference. The sampling is done on the fly since the researcher is trying to understand how to resample (e.g., removing outliers) to get a match Derived data production flow. The research flow is frozen and put in production E.g., run the resample script to sample and write back to DB and historical data. This flow can be run in historical mode (to populate the backend with the production data) and in real-time mode (to compute the streaming data) Derived data QA flow . the goal is to monitor that the production flow is still performing properly with respect to the QA metrics E.g., the 1-sec to 1-min resampling is not performed on-the-fly, but it uses the data already computed by the script in the production flow. This flow is mainly run in real-time, but we might want to look at QA performance also historically This same distinction can also be applied to feature computation and to the machine learning flow. Provider -> data -> Us -> derived flow -> Us -> features -> Us -> ML -> Exchange","title":"Derived data workflows"},{"location":"DataPull/#data-formats","text":"Storage-invariance of data. Data should be independent from its storage format, e.g., CSV, Parquet, Relational DB. In other words, converting data from one format to another should not yield losing any information. Data stored by-date. The by-date representation means that there is a file for the data for all the assets: 1546871400 ... Bitcoin 1546871400 ... Ethereum 1546871401 ... 1546871402 ... Thus the same timestamp is repeated for all the assets The original format of the data is by-date and for a single day is \"20190107.pq\" like: vendor_date start_time end_time ticker open close volume id 2019-01-07 1546871400 1546871460 A 65.64 65.75 19809 16572.0 2019-01-07 1546871400 1546871460 AA 28.53 28.52 31835 1218568.0 2019-01-07 1546871400 1546871460 AAAU 12.92 12.92 11509 1428781.0 2019-01-07 1546871400 1546871460 AABA 58.90 58.91 7124 10846.0 {width=\"6.5in\" height=\"1.0416666666666667in\"} There are 2 special columns in the by-date file: one that represents the timestamp (\"start_time\" in the example). This is unique and monotonic (start_time, ticker) are unique one that represents the asset (\"ticker\" in the example) Data stored by-asset. By-asset data means that there is a single file for each single asset with the data for all the timestamps (no timestamp is repeated in a single file) Bitcoin.pq 1546871400 ... 1546871401 1546871402 Eth.pq 1546871400 ... 1546871401 1546871402 By-asset Parquet data. Data pipelines can transform by-asset data into Parquet data, preserving all the columns. Successive stages of the pipeline perform other data transformations. By-asset means that the asset that is in the innermost directory dst_dir/ year=2021/ month=12/ day=11/ asset=BTC_USDT/ data.parquet asset=ETH_USDT/ data.parquet Typically, the by-date format is just a format that we receive data from, and we don't want to transform data to. The name of the asset can depend on the data and it can be asset , currency_pair , ticker . By default we use names of columns from the data and we reindex the partitioned dataframe on datetime, so saved parquet will all have the same datetime index. Partitioning on year/month/day is optional and should be provided as a switch in partitioning function. Data stored by-tile. Data is organized by tile when the Parquet files are partitioned by asset, by year, and by month so that it's possible to read only a chunk of that asset=BTC_USDT/ year=2021/ month=12/ data.parquet year=2021/ month=11/ data.parquet ... asset=ETH_USDT/ year=2021/ month=12/ data.parquet ...","title":"Data formats"},{"location":"DataPull/#sandbox","text":"This paragraph describes an example of infrastructure that implements the Sorrentum Data Protocol. The It is a Docker Container containing the following services: Airflow Jupyter notebook Postgres MongoDB Notes for CK devs (TO REMOVE) It is a separated code base from Sorrentum and it shares only a few base library (e.g., helpers ) It is a scaled down version of CK production infrastructure (e.g., managed Airflow is replaced by a local Airflow instance) The system follows closely the production system described in [Data pipelines - Specs]{.underline}","title":"Sandbox"},{"location":"DataPull/#data-client-stack","text":"Once the data is downloaded, it needs to be retrieved for processing in a common format (e.g., DataFlow format). We use a two layer approach to split the complexity / responsibilities of reading the data ImClient Data is downloaded and saved with minimal or no transformation Adapts from the vendor data to a standard internal \"MarketData\" format MarketData` implements \"behaviors\" that are orthogonal to vendors, e.g., RealTime or Historical Stitched (i.e.,\"overlap\" multiple data sources giving a single view of the data). E.g., the data from the last day comes from a real-time source while the data before that comes from an historical source Replayed = serialize the data to disk and read it back, implementing also knowledge time as-of-time semantic. This behavior is orthogonal to RealTime, Historical, Stitched, i.e., one can replay any MarketData , including an already replayed one Data format for ImClient / MarketData pipeline Both ImClient and MarketData have an output format that is enforced by the base and the derived classes together ImClient and MarketData have 3 interfaces each: an external \"input\" format for a class format of the data as input to a class derived from MarketData / ImClient an internal \"input\" format format that derived classes need to follow so that the corresponding base class can do its job, i.e., apply common transformations to all MarketData / ImClient classes an external \"output\" format format of the data outputted by any derived class from MarketData / ImClient - The chain of transformations is: - Class derived from ImClient - The transformations are vendor-specific ImClient The transformations are fixed Class derived from MarketData The transformations are specific to the MarketData concrete class :qa MarketData The transformations are fixed \\[Vendor\\] -> \\[DerivedImClient\\] \\[DerivedImClient\\] -> \\[AbstractImClient\\] \\[AbstractImClient\\] -> \\[DerivedMarketData\\] \\[DerivedMarketData\\] -> \\[AbstractMarketData\\] Transformations performed by classes derived from ImClient Whatever is needed to transform the vendor data into the internal format accepted by base ImClient Only derived classes ImClient knows what is exact semantic of the vendor-data Transformations performed by abstract class ImClient Implemented by ImClient._apply_im_normalization() Output format of ImClient TODO(*): Check the code in ImClient since that might be more up to date than this document and, if needed, update this doc The data in output of a class derived from ImClient is normalized so that: the index: represents the knowledge time is the end of the sampling interval is called timestamp is a tz-aware timestamp in UTC the data: is resampled on a 1 minute grid and filled with NaN values is sorted by index and full_symbol is guaranteed to have no duplicates belongs to intervals like [a, b] has a full_symbol column with a string representing the canonical name of the instrument TODO(gp): We are planning to use an ImClient data format closer to MarketData by using start_time , end_time , and knowledge_time since these can be inferred only from the vendor data semantic Transformations performed by classes derived from MarketData Classes derived from MarketData do whatever they need to do in _get_data() to get the data, but always pass back data that: is indexed with a progressive index has asset, start_time, end_time, knowledge_time start_time, end_time, knowledge_time are timezone aware E.g., asset_id start_time end_time close volume idx 0 17085 2021-07-26 13:41:00+00:00 2021-07-26 13:42:00+00:00 148.8600 400176 1 17085 2021-07-26 13:30:00+00:00 2021-07-26 13:31:00+00:00 148.5300 1407725 2 17085 2021-07-26 13:31:00+00:00 2021-07-26 13:32:00+00:00 148.0999 473869 Transformations performed by abstract class MarketData The transformations are done inside get_data_for_interval() , during normalization, and are: indexing by end_time converting end_time , start_time , knowledge_time to the desired timezone sorting by end_time and asset_id applying column remaps Output format of MarketData - The base MarketData normalizes the data by: - sorting by the columns that correspond to end_time and asset_id - indexing by the column that corresponds to end_time , so that it is suitable to DataFlow computation E.g., asset_id start_time close volume end_time 2021-07-20 09:31:00-04:00 17085 2021-07-20 09:30:00-04:00 143.990 1524506 2021-07-20 09:32:00-04:00 17085 2021-07-20 09:31:00-04:00 143.310 586654 2021-07-20 09:33:00-04:00 17085 2021-07-20 09:32:00-04:00 143.535 667639 Asset ids format ImClient uses assets encoded as full_symbols strings (e.g., binance::BTC_UTC ) There is a vendor-specific mapping: from full_symbols to corresponding data from asset_ids (ints) to full_symbols (strings) If the asset_ids -> full_symbols mapping is provided by the vendor, then we reuse it Otherwise, we build a mapping hashing full_symbols strings into numbers MarketData and everything downstream uses asset_ids that are encoded as ints This is because we want to use ints and not strings in dataframe Handling of asset_ids Different implementations of ImClient backing a MarketData are possible, e.g.: The caller needs to specify the requested asset_ids In this case the universe is provided by MarketData when calling the data access methods The reading backend is initialized with the desired universe of assets and then MarketData just uses or subsets that universe For these reasons, assets are selected at 3 different points: MarketData allows to specify or subset the assets through asset_ids through the constructor ImClient backends specify the assets returned E.g., a concrete implementation backed by a DB can stream the data for its entire available universe Certain class methods allow querying data for a specific asset or subset of assets For each stage, a value of None means no filtering Handling of filtering by time Clients of MarketData might want to query data by: using different interval types, namely `[a, b), [a, b], (a, b], (a, b)` filtering on either the start_ts or end_ts For this reason, this class supports all these different ways of providing data ImClient has a fixed semantic of the interval \\[a, b\\] MarketData adapts the fixed semantic to multiple ones Handling timezone ImClient always uses UTC as output MarketData adapts UTC to the desired timezone, as requested by the client","title":"Data client stack"},{"location":"DataPull/#checklist-for-releasing-a-new-data-set","text":"Decide what to do exactly (e.g., do we download only bulk data or also real-time?) Review what code we have and what can be generalized to accomplish the task at hand Decide what's the name of the data set according to our convention Create DAGs for Airflow Update the Raw Data Gallery im_v2/common/notebooks/Master_raw_data_gallery.ipynb Quick exploratory analysis to make sure the data is not malformed Update the table in Data pipelines - Specs Exploratory analysis for a thorough QA analysis Add QA system to Airflow prod","title":"Checklist for releasing a new data set"},{"location":"Design_Philosophy/","text":"Design Philosophy Design Philosophy Measure seven times, cut once (Russian proverb) Hacker laws Keep it simple Tips from a pro Designing software systems is tricky Get Advice Early! Interfaces Architecture Use design patterns Functions Avoid modifying the function input Prefer pure functions by default Measure seven times, cut once (Russian proverb) Before doing any work, sit down and plan Describe somewhere in writing your high-level plan. Put it in a Google doc to make it easier to collaborate and review. What should the code do? What are the functionalities you want to implement? What are the functionalities you don\u2019t want to implement? (what are you explicitly considering to be out-of-scope?) What is more important, what is less important? E.g., in terms of P0, P1, P2 What are the requirements/invariants? What are the semantics of the entities involved? What are the analyses, the comparisons, and the plots? What are the ideas (expressed without any code!)? ETA: Spend quality time thinking about it (e.g., 30 mins, 1 hr) Review the plan Look at the plan again with fresh eyes (e.g., go for a 5-min walk) Does the plan make sense? What can you remove? Can you make things simpler? What is not elegant? What entity is a special case of another? What is similar to what? ETA: Spend 30 mins thinking Ask for someone to review the plan Don\u2019t be ashamed of asking for advice Implement a design Transform the plan into high-level code, e.g., What are the objects / functions involved? What are the responsibilities of each class / function? What are the code invariants? What are the data structures? Write the interfaces Only the interfaces! Refrain from implementing the logic Comment the interfaces clearly Think of how the objects / functions interact (who does what, what is the data passed around) Sprinkle TODOs with ideas about potential problems, simpler approaches ETA: Spend 1/2 day, 1 day Do a PR of the design Once we converge on the design: Implement the functions Unit test PR Remember: We want to do quick interactions: every day there is communication, update and discussion Do not disappear for one week and come back with something that makes sense only to you, or that you didn\u2019t get buy-in from others on Hacker laws A list of interesting \"laws\" (some are more rule of thumbs / heuristics) related to computing: hacker-laws Keep it simple Follow the KISS principle . Pursue simple, elegant solutions. Some things are inherently complex, but even complex systems can (and should) be broken down into simple pieces. Designs that are simple are easier to Understand Modify Debug Tips from a pro Adapted from these slides from a Stanford talk given by Jeff Dean (the Chuck Norris of SWE) Designing software systems is tricky Need to balance: Simplicity [note that this comes first!] Scalability Performance Reliability Generality Features [note that this comes last!] Get Advice Early! Get advice Before you write any code Before you write any lengthy design documents [notice the implicit assumption that there is a design documented!] Before writing a doc or code Jot down some rough ideas (a few paragraphs) Chat about the design with colleagues Consider discussing multiple potential designs Interfaces Think carefully about interfaces in your system! Imagine other hypothetical clients trying to use your interface Document precisely, but avoid constraining the implementation Get feedback on your interfaces before implementing! The best way to learn is to look at well-designed interfaces Architecture Use design patterns Design patterns are idioms or recipes for solving problems that commonly appear in software engineering across projects and even languages. The classical introduction to design patterns is the so-called \"Gang of Four\" book . A free python-focused reference is available here . Expanding your knowledge of design patterns is a worthwhile investment, because design patterns Capture elegant solutions that have been developed by many experienced programmers over a long period of time Provide a framework and reference point for software architecture Are widely used and well-known and therefore quickly recognized by skilled programmers In other words, by using design patterns, you Don\u2019t have to re-invent the wheel Simplify the high-level picture of your code Make it easier for other people to understand your code Functions Avoid modifying the function input If, for example, a function f accepts a dataframe df as its (sole) argument, then, ideally, f(df) will not modify df . If modifications are desired, then instead one can do: def f(df): df = df.copy() ... return df in the function so that f(df) returns the desired new dataframe without modifying the dataframe that was passed in to the function. In some cases the memory costs associated with the copy are prohibitive, and so modifying in-place is appropriate. If such is the case, state it explicitly in the docstring. Functions that do not modify the input are especially convenient to have in notebook settings. In particular, using them makes it easy to write blocks of code in a notebook that will return the same results when re-executed out of order. Prefer pure functions by default Pure functions have two key properties: If the function arguments do not change, then the return value returned does not change (in contrast to, e.g., functions that rely upon global state) Function evaluation does not have side effects Some nice properties enjoyed by pure functions are: They are easy to understand and easy to test Using pure functions makes refactoring easier They allow chaining in an elegant way They are often a natural choice for data manipulation and analysis They are convenient in notebooks Though it is good to develop an appreciation for functional programming , and we like to adopt that style when appropriate, we recognize that it is not pragmatic to dogmatically insist upon a functional style (especially in our domain and when using Python).","title":"Design Philosophy"},{"location":"Design_Philosophy/#design-philosophy","text":"Design Philosophy Measure seven times, cut once (Russian proverb) Hacker laws Keep it simple Tips from a pro Designing software systems is tricky Get Advice Early! Interfaces Architecture Use design patterns Functions Avoid modifying the function input Prefer pure functions by default","title":"Design Philosophy"},{"location":"Design_Philosophy/#measure-seven-times-cut-once-russian-proverb","text":"Before doing any work, sit down and plan Describe somewhere in writing your high-level plan. Put it in a Google doc to make it easier to collaborate and review. What should the code do? What are the functionalities you want to implement? What are the functionalities you don\u2019t want to implement? (what are you explicitly considering to be out-of-scope?) What is more important, what is less important? E.g., in terms of P0, P1, P2 What are the requirements/invariants? What are the semantics of the entities involved? What are the analyses, the comparisons, and the plots? What are the ideas (expressed without any code!)? ETA: Spend quality time thinking about it (e.g., 30 mins, 1 hr) Review the plan Look at the plan again with fresh eyes (e.g., go for a 5-min walk) Does the plan make sense? What can you remove? Can you make things simpler? What is not elegant? What entity is a special case of another? What is similar to what? ETA: Spend 30 mins thinking Ask for someone to review the plan Don\u2019t be ashamed of asking for advice Implement a design Transform the plan into high-level code, e.g., What are the objects / functions involved? What are the responsibilities of each class / function? What are the code invariants? What are the data structures? Write the interfaces Only the interfaces! Refrain from implementing the logic Comment the interfaces clearly Think of how the objects / functions interact (who does what, what is the data passed around) Sprinkle TODOs with ideas about potential problems, simpler approaches ETA: Spend 1/2 day, 1 day Do a PR of the design Once we converge on the design: Implement the functions Unit test PR Remember: We want to do quick interactions: every day there is communication, update and discussion Do not disappear for one week and come back with something that makes sense only to you, or that you didn\u2019t get buy-in from others on","title":"Measure seven times, cut once (Russian proverb)"},{"location":"Design_Philosophy/#hacker-laws","text":"A list of interesting \"laws\" (some are more rule of thumbs / heuristics) related to computing: hacker-laws","title":"Hacker laws"},{"location":"Design_Philosophy/#keep-it-simple","text":"Follow the KISS principle . Pursue simple, elegant solutions. Some things are inherently complex, but even complex systems can (and should) be broken down into simple pieces. Designs that are simple are easier to Understand Modify Debug","title":"Keep it simple"},{"location":"Design_Philosophy/#tips-from-a-pro","text":"Adapted from these slides from a Stanford talk given by Jeff Dean (the Chuck Norris of SWE)","title":"Tips from a pro"},{"location":"Design_Philosophy/#designing-software-systems-is-tricky","text":"Need to balance: Simplicity [note that this comes first!] Scalability Performance Reliability Generality Features [note that this comes last!]","title":"Designing software systems is tricky"},{"location":"Design_Philosophy/#get-advice-early","text":"Get advice Before you write any code Before you write any lengthy design documents [notice the implicit assumption that there is a design documented!] Before writing a doc or code Jot down some rough ideas (a few paragraphs) Chat about the design with colleagues Consider discussing multiple potential designs","title":"Get Advice Early!"},{"location":"Design_Philosophy/#interfaces","text":"Think carefully about interfaces in your system! Imagine other hypothetical clients trying to use your interface Document precisely, but avoid constraining the implementation Get feedback on your interfaces before implementing! The best way to learn is to look at well-designed interfaces","title":"Interfaces"},{"location":"Design_Philosophy/#architecture","text":"","title":"Architecture"},{"location":"Design_Philosophy/#use-design-patterns","text":"Design patterns are idioms or recipes for solving problems that commonly appear in software engineering across projects and even languages. The classical introduction to design patterns is the so-called \"Gang of Four\" book . A free python-focused reference is available here . Expanding your knowledge of design patterns is a worthwhile investment, because design patterns Capture elegant solutions that have been developed by many experienced programmers over a long period of time Provide a framework and reference point for software architecture Are widely used and well-known and therefore quickly recognized by skilled programmers In other words, by using design patterns, you Don\u2019t have to re-invent the wheel Simplify the high-level picture of your code Make it easier for other people to understand your code","title":"Use design patterns"},{"location":"Design_Philosophy/#functions","text":"","title":"Functions"},{"location":"Design_Philosophy/#avoid-modifying-the-function-input","text":"If, for example, a function f accepts a dataframe df as its (sole) argument, then, ideally, f(df) will not modify df . If modifications are desired, then instead one can do: def f(df): df = df.copy() ... return df in the function so that f(df) returns the desired new dataframe without modifying the dataframe that was passed in to the function. In some cases the memory costs associated with the copy are prohibitive, and so modifying in-place is appropriate. If such is the case, state it explicitly in the docstring. Functions that do not modify the input are especially convenient to have in notebook settings. In particular, using them makes it easy to write blocks of code in a notebook that will return the same results when re-executed out of order.","title":"Avoid modifying the function input"},{"location":"Design_Philosophy/#prefer-pure-functions-by-default","text":"Pure functions have two key properties: If the function arguments do not change, then the return value returned does not change (in contrast to, e.g., functions that rely upon global state) Function evaluation does not have side effects Some nice properties enjoyed by pure functions are: They are easy to understand and easy to test Using pure functions makes refactoring easier They allow chaining in an elegant way They are often a natural choice for data manipulation and analysis They are convenient in notebooks Though it is good to develop an appreciation for functional programming , and we like to adopt that style when appropriate, we recognize that it is not pragmatic to dogmatically insist upon a functional style (especially in our domain and when using Python).","title":"Prefer pure functions by default"},{"location":"Development_workflow/","text":"Development Workflow Setting up Git credentials Preamble Check Git credentials Setting Git credentials Enforcing Git credentials Create the env Invoke Listing all the tasks Implementation details GitHub See all the workflows Getting help for a workflow Merge master in the current branch Create a PR Extract a PR from a larger one Using git Systematic code transformation Generate a local amp Docker image Update the dev amp Docker image Experiment in a local image GitHub Actions (CI) pytest Run with coverage Iterating on stacktrace of failing test Iterating on a failing regression test Detect mismatches with golden test outcomes Playback Publish a notebook Detailed instructions Publish notebooks Open a published notebook Start a server Using the dev box Using Windows browser How to create a private fork Integrate public to private: amp -> cmamp Set-up Ours vs theirs Sync the repos (after double integration) Updated sync Check that things are fine Integrate private to public: cmamp -> amp Squash commit of everything in the branch Double integration cmamp amp Script set-up Manual set-up branches High-level plan Sync im cmamp -> amp Sync everything Files that need to be different Lint everything Testing Setting up Git credentials Preamble Git allows setting credentials at different \"levels\": system (set for all the users in /etc/git ) global (set for a single user in $HOME/.gitconfig or $HOME/.config/git/config ) local (set on a per client basis in .git/config in the repo root) Git uses a hierarchical config approach in which settings of a broader scope are inherited if not overridden. Refs: How to customize Git: https://git-scm.com/book/en/v2/Customizing-Git-Git-Configuration Details on git config : https://git-scm.com/docs/git-config Check Git credentials You can check the Git credentials that will be used to commit in a client by running: ```bash git config -l | grep user user.name=saggese user.email=saggese@gmail.com github.user=gpsaggese ``` To know at which level each variable is defined, run ```bash git config --show-origin user.name file:/Users/saggese/.gitconfig saggese ``` You can see all the Authors in a Git repo history with: ```bash git log | grep -i Author | sort | uniq ... ``` Git doesn't do any validation of user.name and user.email but it just uses these values to compose a commit message like: ```bash git log -2 commit 31052d05c226b1c9834d954e0c3d5586ed35f41e (HEAD -> AmpTask1290_Avoid_committing_to_master_by_mistake) Author: saggese saggese@gmail.com Date: Mon Jun 21 16:22:25 2021 Update hooks ``` Setting Git credentials To keep things simple and avoid variability, our convention is to use: as user.name our Linux user name on the local computer we are using to commit which is returned by whoami (e.g., user.name=saggese ) as user.email the email that corresponds to that user (e.g,. user.email=saggese@gmail.com ) To accomplish the set-up above you can: use in /Users/saggese/.gitconfig the values for our open-source account, so that they are used by default ```bash git config --global user.name $(whoami) git config --global user.email YOUR_EMAIL ``` use the correct user / email in the repos that are not open-source ```bash cd $GIT_ROOT git config --local user.name $(whoami) git config --local user.email YOUR_EMAIL ``` Note that you need to set these local values on each Git client that you have cloned, since Git doesn't version control these values Enforcing Git credentials We use Git hooks to enforce that certain emails are used for certain repos (e.g., we should commit to our open-source repos only using our personal non-corporate email). You need to install the hooks in each Git client that you use. Conceptually this step is part of git clone : every time you clone a repo locally you need to set the hooks. TODO(gp): We could create a script to automate cloning a repo and setting it up. ```bash cd //amp ./dev_scripts/git/git_hooks/install_hooks.py --action install cd //lem ./amp/dev_scripts/git/git_hooks/install_hooks.py --action install ``` This procedure creates some links from .git/hook to the scripts in the repo. You can also use the action status to see the status and remove to the hooks. Create the env You can follow the ```bash # Build the client env dev_scripts/client_setup/build.sh source dev_scripts/setenv_amp.sh ``` Invoke We use invoke to implement workflows (aka \"tasks\") similar to Makefile targets, but using Python. The official documentation. We use invoke to automate tasks and package workflows for: docker: docker_* Git: git_* GitHub (relying on gh integration): gh_* running tests: run_* integrate: integrate_* releasing tools and Docker images: docker_* lint: lint_* pytest: Each set of commands starts with the topic, e.g., docker_* for all the docker elated tasks The best approach to getting familiar with the tasks is to browse the list and then check the output of the help ```bash invoke --help command i -h gh_issue_title Usage: inv[oke] [--core-opts] gh_issue_title [--options] [other tasks here ...] Docstring: Print the title that corresponds to the given issue and repo_short_name. E.g., AmpTask1251_Update_GH_actions_for_amp. :param pbcopy: save the result into the system clipboard (only on macOS) Options: -i STRING, --issue-id=STRING -p, --[no-]pbcopy -r STRING, --repo-short-name=STRING ``` I can guarantee you a 2x improvement in performance, if you master the workflows, but it takes some time and patience Listing all the tasks > invoke --list INFO: > cmd='/Users/saggese/src/venv/amp.client_venv/bin/invoke --list' Available tasks: check_python_files Compile and execute Python files checking for errors. docker_bash Start a bash shell inside the container corresponding to a stage. docker_build_local_image Build a local image (i.e., a release candidate \"dev\" image). docker_build_prod_image (ONLY CI/CD) Build a prod image. docker_cmd Execute the command `cmd` inside a container corresponding to a stage. docker_images_ls_repo List images in the logged in repo_short_name. docker_jupyter Run jupyter notebook server. docker_kill Kill the last Docker container started. docker_login Log in the AM Docker repo_short_name on AWS. docker_ps List all the running containers. docker_pull Pull latest dev image corresponding to the current repo from the registry. docker_pull_dev_tools Pull latest prod image of `dev_tools` from the registry. docker_push_dev_image (ONLY CI/CD) Push the \"dev\" image to ECR. docker_push_prod_image (ONLY CI/CD) Push the \"prod\" image to ECR. docker_release_all (ONLY CI/CD) Release both dev and prod image to ECR. docker_release_dev_image (ONLY CI/CD) Build, test, and release to ECR the latest \"dev\" image. docker_release_prod_image (ONLY CI/CD) Build, test, and release to ECR the prod image. docker_rollback_dev_image Rollback the version of the dev image. docker_rollback_prod_image Rollback the version of the prod image. docker_stats Report last started Docker container stats, e.g., CPU, RAM. docker_tag_local_image_as_dev (ONLY CI/CD) Mark the \"local\" image as \"dev\". find_check_string_output Find output of check_string() in the test running find_test_class Report test files containing `class_name` in a format compatible with find_test_decorator Report test files containing `class_name` in pytest format. fix_perms :param action: gh_create_pr Create a draft PR for the current branch in the corresponding gh_issue_title Print the title that corresponds to the given issue and repo_short_name. gh_workflow_list Report the status of the GH workflows. gh_workflow_run Run GH workflows in a branch. git_add_all_untracked Add all untracked files to Git. git_branch_copy Create a new branch with the same content of the current branch. git_branch_diff_with_base Diff files of the current branch with master at the branching point. git_branch_files Report which files were added, changed, and modified in the current branch git_branch_next_name Return a name derived from the branch so that the branch does not exist. git_clean Clean the repo_short_name and its submodules from artifacts. git_create_branch Create and push upstream branch `branch_name` or the one corresponding to git_create_patch Create a patch file for the entire repo_short_name client from the base git_delete_merged_branches Remove (both local and remote) branches that have been merged into master. git_files Report which files are changed in the current branch with respect to git_last_commit_files Print the status of the files in the previous commit. git_merge_master Merge `origin/master` into the current branch. git_pull Pull all the repos. git_fetch_master Pull master without changing branch. git_rename_branch Rename current branch both locally and remotely. integrate_compare_branch_with_base Compare the files modified in both the branches in src_dir and dst_dir to integrate_copy_dirs Copy dir `subdir` from dir `src_dir` to `dst_dir`. integrate_create_branch Create the branch for integration in the current dir. integrate_diff_dirs Integrate repos from dir `src_dir` to `dst_dir`. lint Lint files. lint_create_branch Create the branch for linting in the current dir. print_setup Print some configuration variables. print_tasks Print all the available tasks in `lib_tasks.py`. pytest_clean Clean pytest artifacts. pytest_compare Compare the output of two runs of `pytest -s --dbg` removing irrelevant pytest_failed Process the list of failed tests from a pytest run. pytest_failed_freeze_test_list Copy last list of failed tests to not overwrite with successive pytest run_blank_tests (ONLY CI/CD) Test that pytest in the container works. run_coverage_report run_fast_slow_tests Run fast and slow tests independently. run_fast_tests Run fast tests. run_qa_tests Run QA tests independently. run_slow_tests Run slow tests. run_superslow_tests Run superslow tests. traceback Parse the traceback from Pytest and navigate it with vim. Implementation details By convention all invoke targets are in *_lib_tasks.py , e.g., helpers/lib_tasks.py - tasks to be run in cmamp optimizer/opt_lib_tasks.py - tasks to be run in cmamp/optimizer All invoke tasks are functions with the @task decorator, e.g., ```python from invoke import task @task def invoke_task(...): ... ``` To run a task we use context.run(...) , see the official docs To be able to run a specified invoke task one should import it in tasks.py E.g., see cmamp/tasks.py A task can be run only in a dir where it is imported in a corresponding tasks.py , e.g., invoke_task1 is imported in cmamp/tasks.py so it can be run only from cmamp invoke_task2 is imported in cmamp/optimizer/tasks.py so it can be run only from cmamp/optimizer In other words one should do cd cmamp/optimizer before doing i invoke_task2 ... GitHub Get the official branch name corresponding to an Issue ```bash i gh_issue_title -i 256 ## gh_issue_title: issue_id='256', repo_short_name='current' # Copied to system clipboard: AmpTask256_Part_task2236_jenkins_cleanup_split_scripts: https://github.com/alphamatic/amp/pull/256 ``` See all the workflows Workflows are organized somehow around Linux command that they are related to, e.g., docker_* for all docker related workflows find_* for all workflows related to finding (e.g., unit tests) gh_* for GitHub related workflows git_* for Git related workflows etc. Once in a while it can be useful to list all the available workflows and see if something interesting was added. ```bash invoke --list Available tasks: docker_bash Start a bash shell inside the container corresponding to a stage. docker_build_local_image Build a local image (i.e., a release candidate \"dev\" image). ... ``` Getting help for a workflow You can get a more detailed help with ```bash invoke --help run_fast_tests Usage: inv[oke] [--core-opts] run_fast_tests [--options] [other tasks here ...] Docstring: Run fast tests. :param stage: select a specific stage for the Docker image :param pytest_opts: option for pytest :param pytest_mark: test list to select as @pytest.mark.XYZ :param dir_name: dir to start searching for tests :param skip_submodules: ignore all the dir inside a submodule :param coverage: enable coverage computation :param collect_only: do not run tests but show what will be executed Options: -c, --coverage -d STRING, --dir-name=STRING -k, --skip-submodules -o, --collect-only -p STRING, --pytest-opts=STRING -s STRING, --stage=STRING -y STRING, --pytest-mark=STRING ``` Merge master in the current branch > i git_merge_master Create a PR TODO(gp): Describe Extract a PR from a larger one My workflow is to have a feature branch (e.g., AmpTask1891_Sketch_out_the_design_of_RT_OMS ) that I develop in. When a piece of code can be merged: create a new branch (e.g., AmpTask1891_Sketch_out_the_design_of_RT_OMS_02 ) copy the current feature branch to the new branch remove the pieces that you don't want to merge run regressions PR merge into master merge master into the feature branch This workflow allows you to develop and regress / merge without too much hassle solving the problem of \"stacked PRs\". ```bash # Go to the client with the branch that you want to divvy up. git checkout ${feature_branch} # Make sure that the branch is up-to-date with master i git_merge_master # Lint. i lint -b # Create a patch from the branch (there are many options to tweak the workflow, check the help) i git_create_patch -b # To apply the patch and execute: git checkout 8f9cda97 git apply /Users/saggese/src/lemonade1/amp/patch.amp.8f9cda97.20210609_080439.patch ... ``` Go to a fresh Git client (I have 2-3 Git clients separated from the one in which I develop for this kind of operations) or go to master in the same Git client ```bash # Go to master git checkout master # Apply the patch from the run of git_create_patch git apply /Users/saggese/src/lemonade1/amp/patch.amp.8f9cda97.20210609_080439.patch # This patch should apply cleanly and with no errors from git, otherwise it means that your feature branch does not have the latest master # Remove what you don't want to commit. # Do not change anything or run the linter otherwise your feature branch will not merge easily. git diff git checkout master -- ... ... git commit; git push # Create a PR (non-draft so that GH can start running the tests) i gh_create_pr --no-draft # Regress the branch i run_fast_tests ... # Merge the PR into master # Go back to your feature branch and merge master gco ${feature_branch} git pull # Now one piece of your feature branch has been merged and you can repeat until all the code is merged. ``` Using git > git checkout `dst_branch` > git merge --squash --ff `src_branch` > git reset HEAD Systematic code transformation See the help of amp/dev_scripts/replace_text.py Generate a local amp Docker image This is a manual flow used to test and debug images before releasing them to the team. The flow is similar to the dev image, but by default tests are not run and the image is not released. ```bash # Build the local image (and update Poetry dependencies, if needed). i docker_build_local_image --update-poetry ... docker image ls 665840871993.dkr.ecr.us-east-1.amazonaws.com/amp:local REPOSITORY TAG IMAGE ID CREATED SIZE 665840871993.dkr.ecr.us-east-1.amazonaws.com/amp local 9b3f8f103a2c 1 second ago 1.72GB # Test the new \"local\" image i docker_bash --stage \"local\" python -c \"import async_solipsism\" python -c \"import async_solipsism; print(async_solipsism. version )\" # Run the tests with local image # Make sure the new image is used: e.g., add an import and trigger the tests. i run_fast_tests --stage \"local\" --pytest-opts core/dataflow/test/test_real_time.py i run_fast_slow_tests --stage \"local\" # Promote a local image to dev. i docker_tag_local_image_as_dev i docker_push_dev_image ``` ## Update the dev amp Docker image To implement the entire Docker QA process of a dev image ```bash # Clean all the Docker images locally, to make sure there is no hidden state. docker system prune --all # Update the needed packages. devops/docker_build/pyproject.toml # Visually inspect the updated packages. git diff devops/docker_build/poetry.lock # Run entire release process. i docker_release_dev_image ``` ## Experiment in a local image To install packages in an image, do i docker_bash ```bash # Switch to root and install package. sudo su - source /venv/bin/activate pip install # Switch back to user. exit ``` You should test that the package is installed for your user, e.g., ```bash source /venv/bin/activate python -c \"import foobar; print(foobar);print(foobar. version )\" ``` You can now use the package in this container. Note that if you exit the container, the modified image is lost, so you need to install it again. You can save the modified image, tagging the new image as local, while the container is still running. Copy your Container ID. You can find it in the docker bash session, e.g., if the command line in the container starts with user_1011@da8f3bb8f53b:/app$ , your Container ID is da8f3bb8f53b by listing running containers, e.g., run docker ps outside the container Commit image ```bash docker commit /cmamp:local-$USER ``` E.g. docker commit da8f3bb8f53b 665840871993.dkr.ecr.us-east-1.amazonaws.com/cmamp:local-julias If you are running inside a notebook using i docker_jupyter you can install packages using a one liner ! sudo su -; source ...; GitHub Actions (CI) ## Running a single test in GH Actions Create a branch Change .github/workflows/fast_tests.yml run: invoke run_fast_tests ## run: invoke run_fast_tests --pytest-opts=\"helpers/test/test_git.py::Test_git_modified_files1::test_get_modified_files_in_branch1 -s --dbg\" # In the current implementation (where we try to not run for branches) to run in a branch pytest From https://gist.github.com/kwmiebach/3fd49612ef7a52b5ce3a Run with coverage > i run_fast_tests --pytest-opts=\"core/test/test_finance.py\" --coverage Iterating on stacktrace of failing test Inside docker bash ```bash pytest ... ``` The test fails: switch to using pytest.sh to save the stacktrace to a file Then from outside Docker launch vim in quickfix mode ```bash invoke traceback ``` The short form is it Iterating on a failing regression test The workflow is: ```bash # Run a lot of tests, e.g., the entire regression suite. pytest ... # Some tests fail. # Run the pytest_repro to summarize test failures and to generate commands to reproduce them. invoke pytest_repro ``` Detect mismatches with golden test outcomes The command is ```bash i pytest_find_unused_goldens ``` The specific dir to check can be specified with the dir_name parameter. The invoke detects and logs mismatches between the tests and the golden outcome files. When goldens are required by the tests but the corresponding files do not exist This usually happens if the tests are skipped or commented out. Sometimes it's a FP hit (e.g. the method doesn't actually call check_string but instead has it in a string, or check_string is called on a missing file on purpose to verify that an exception is raised). When the existing golden files are not actually required by the corresponding tests. In most cases it means the files are outdated and can be deleted. Alternatively, it can be a FN hit: the test method A, which the golden outcome corresponds to, doesn't call check_string directly, but the test's class inherits from a different class, which in turn has a method B that calls check_string , and this method B is called in the test method A. For more details see CmTask528 . Playback Publish a notebook publish_notebook.py is a little tool that allows to: Opening a notebook in your browser (useful for read-only mode) E.g., without having to use Jupyter notebook (which modifies the file in your client) or github preview (which is slow or fails when the notebook is too large) Sharing a notebook with others in a simple way Pointing to detailed documentation in your analysis Google docs Reviewing someone's notebook Comparing multiple notebooks against each other in different browser windows Taking a snapshot / checkpoint of a notebook as a backup or before making changes This is a lightweight alternative to \"unit testing\" to capture the desired behavior of a notebook One can take a snapshot and visually compare multiple notebooks side-by-side for changes Detailed instructions You can get details by running: ```bash dev_scripts/notebooks/publish_notebook.py -h ``` Plug-in for Chrome my-s3-browser Publish notebooks Make sure that your environment is set up properly ```bash more ~/.aws/credentials [am] aws_access_key_id= aws_secret_access_key= aws_s3_bucket=alphamatic-data printenv | grep AM_ AM_AWS_PROFILE=am ``` If you don't have them, you need to re-run source dev_scripts/setenv.sh in all the shells. It might be easier to kill that tmux session and restart it ```bash tmux kill-session --t limeXYZ ~/go_lem.sh XYZ ``` Inside or outside a Docker bash run ```bash publish_notebook.py --file http://127.0.0.1:2908/notebooks/notebooks/Task40_Optimizer.ipynb --action publish_on_s3 ``` The file is copied to S3 bash Copying './Task40_Optimizer.20210717_010806.html' to 's3://alphamatic-data/notebooks/Task40_Optimizer.20210717_010806.html' You can also save the data locally: ```bash publish_notebook.py --file amp/oms/notebooks/Master_forecast_processor_reader.ipynb --action publish_on_s3 --aws_profile saml-spm-sasm ``` You can also use a different path or profile by specifying it directly ```bash publish_notebook.py \\ --file http://127.0.0.1:2908/notebooks/notebooks/Task40_Optimizer.ipynb \\ --action publish_on_s3 \\ --s3_path s3://alphamatic-data/notebooks \\ --aws_profile am ``` Open a published notebook Start a server (cd /local/home/share/html/published_notebooks; python3 -m http.server 8000) go to the page in the local browser Using the dev box To open a notebook saved on S3, *outside* a Docker container run: ```bash publish_notebook.py --action open --file s3://alphamatic-data/notebooks/Task40_Optimizer.20210717_010806.html ``` This opens a Chrome window through X-windows. To open files faster you can open a Chrome window in background with ```bash google-chrome ``` and then navigate to the path (e.g., /local/home/share/html/published_notebooks/Master_forecast_processor_reader.20220810-112328.html ) Using Windows browser Another approach is: ```bash aws s3 presign --expires-in 36000 s3://alphamatic-data/notebooks/Task40_Optimizer.20210716_194400.html | xclip ``` Open the link saved in the clipboard in the Windows browser For some reason, Chrome saves the link instead of opening, so you need to click on the saved link How to create a private fork https://stackoverflow.com/questions/10065526/github-how-to-make-a-fork-of-public-repository-private From https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/creating-a-repository-on-github/duplicating-a-repository ```bash git clone --bare git@github.com:alphamatic/amp.git amp_bare git push --mirror https://github.com/cryptomtc/cmamp.git ``` It worked only as cryptomtc, but not using my key Integrate public to private: amp -> cmamp Set-up > git remote add public git@github.com:alphamatic/amp # Go to cmamp > cd /data/saggese/src/cmamp1 > cd /Users/saggese/src/cmamp1 # Add the remote # git remote add public https://github.com/exampleuser/public-repo.git > git remote add public git@github.com:alphamatic/amp > git remote -v origin https://github.com/cryptomtc/cmamp.git (fetch) origin https://github.com/cryptomtc/cmamp.git (push) public git@github.com:alphamatic/amp (fetch) public git@github.com:alphamatic/amp(push) Ours vs theirs From https://stackoverflow.com/questions/25576415/what-is-the-precise-meaning-of-ours-and-theirs-in-git/25576672 When merging: ours = branch checked out (git checkout *ours*) theirs = branch being merged (git merge *theirs*) When rebasing the role is swapped ours = branch being rebased onto (e.g., master) theirs = branch being rebased (e.g., feature) Sync the repos (after double integration) > git fetch origin; git fetch public # Pull from both repos > git pull public master -X ours # You might want to use `git pull -X theirs` or `ours` > git pull -X theirs > git pull public master -s recursive -X ours # When there is a file added it is better to add > git diff --name-status --diff-filter=U | awk '{print $2}' im/ccxt/db/test/test_ccxt_db_utils.py # Merge branch > gs + git status On branch AmpTask1786_Integrate_20211128_02 Your branch and 'origin/AmpTask1786_Integrate_20211128_02' have diverged, and have 861 and 489 different commits each, respectively. (use \"git pull\" to merge the remote branch into yours) You are in a sparse checkout with 100% of tracked files present. nothing to commit, working tree clean > git pull -X ours ## Make sure it's synced at ToT > rsync --delete -r /Users/saggese/src/cmamp2/ /Users/saggese/src/cmamp1 --exclude='.git/' > diff -r --brief /Users/saggese/src/cmamp1 /Users/saggese/src/cmamp2 | grep -v \\.git Updated sync > git fetch origin; git fetch public Check that things are fine > git diff origin/master... >patch.txt > cd /Users/saggese/src/cmamp2 # Create a branch > git checkout -b Cmamp114_Integrate_amp_cmamp_20210928 > git apply patch.txt # Compare branch with references > dev_scripts/diff_to_vimdiff.py --dir1 /Users/saggese/src/cmamp1/im --dir2 /Users/saggese/src/cmamp2/im > diff -r --brief /Users/saggese/src/lemonade3/amp \\~/src/cmamp2 | grep -v \"/im\" # Creates a merge commit > git push origin master Integrate private to public: cmamp -> amp > cd /data/saggese/src/cmamp1 > tar cvzf patch.tgz $(git diff --name-onlyorigin/master public/master | grep -v repo_config.py) > cd /Users/saggese/src/amp1 git remote add cmamp > git@github.com:cryptomtc/cmamp.git > GIT_SSH_COMMAND=\"ssh -i \\~/.ssh/cryptomatic/id_rsa.cryptomtc.github\" git fetch > git@github.com:cryptomtc/cmamp.git > git checkout -b Integrate_20210928 > GIT_SSH_COMMAND=\"ssh -i \\~/.ssh/cryptomatic/id_rsa.cryptomtc.github\" git pull > cmamp master -X ours Squash commit of everything in the branch From https://stackoverflow.com/questions/25356810/git-how-to-squash-all-commits-on-branch ```bash git checkout yourBranch git reset $(git merge-base master $(git branch --show-current)) git add -A git commit -m \"Squash\" git push --force ``` Double integration cmamp < -- > amp The bug is https://github.com/alphamatic/amp/issues/1786 Script set-up > vi /Users/saggese/src/amp1/dev_scripts/integrate_repos/setup.sh Update the date > vi /Users/saggese/src/amp1/dev_scripts/integrate_repos/* > cd \\~/src/amp1 > source /Users/saggese/src/amp1/dev_scripts/integrate_repos/setup.sh > cd \\~/src/cmamp1 > source /Users/saggese/src/amp1/dev_scripts/integrate_repos/setup.sh Manual set-up branches # Go to cmamp1 > go_amp.sh cmamp 1 # Set up the env vars in both clients > export AMP_DIR=/Users/saggese/src/amp1; export CMAMP_DIR=/Users/saggese/src/cmamp1; echo \"$AMP_DIR\"; ls $AMP_DIR; echo \"$CMAMP_DIR\"; ls $CMAMP_DIR # Create two branches > export BRANCH_NAME=AmpTask1786_Integrate_20211010 export BRANCH_NAME=AmpTask1786_Integrate_2021117 ... > cd $AMP_DIR # Create automatically > i git_create_branch -b $BRANCH_NAME # Create manually > git checkout -b $BRANCH_NAME > git push --set-upstream origin $BRANCH_NAME > cd $CMAMP_DIR > i git_create_branch -b $BRANCH_NAME High-level plan SUBDIR=im Typically cmamp is copied on top of amp SUBDIR=devops cmamp and amp need to be different (until we unify the Docker flow) Everything else Typically amp -> cmamp Sync im cmamp -> amp SUBDIR=im # Check different files > diff -r --brief $AMP_DIR/$SUBDIR $CMAMP_DIR/$SUBDIR | grep -v .git # Diff the entire dirs with vimdiff > dev_scripts/diff_to_vimdiff.py --dir1 $AMP_DIR/$SUBDIR --dir2 $CMAMP_DIR/$SUBDIR # Find different files > find $AMP_DIR/$SUBDIR -name \"*\"; find $CMAMP_DIR/$SUBDIR -name \"*\" sdiff /tmp/dir1 /tmp/dir2 # Copy cmamp -> amp > rsync --delete -au $CMAMP_DIR/$SUBDIR/ $AMP_DIR/$SUBDIR -a = archive -u = ignore newer # Add all the untracked files > cd $AMP_DIR/$SUBDIR && git add $(git ls-files -o --exclude-standard) # Check that there are no differences after copying > dev_scripts/diff_to_vimdiff.py --dir1 $AMP_DIR/$SUBDIR --dir2 $CMAMP_DIR/$SUBDIR ========== > rsync --delete -rtu $AMP_DIR/$SUBDIR/ $CMAMP_DIR/$SUBDIR > rsync --dry-run -rtui --delete $AMP_DIR/$SUBDIR/ $CMAMP_DIR/$SUBDIR/ .d..t.... ./ > f..t.... __init__.py cd+++++++ features/ > f+++++++ features/__init__.py > f+++++++ features/pipeline.py cd+++++++ features/test/ > f+++++++ features/test/test_feature_pipeline.py cd+++++++ features/test/TestFeaturePipeline.test1/ cd+++++++ features/test/TestFeaturePipeline.test1/output/ > f+++++++ features/test/TestFeaturePipeline.test1/output/test.txt .d..t.... price/ .d..t.... real_time/ > f..t.... real_time/__init__.py .d..t.... real_time/notebooks/ > f..t.... real_time/notebooks/Implement_RT_interface.ipynb > f..t.... real_time/notebooks/Implement_RT_interface.py .d..t.... real_time/test/ cd+++++++ real_time/test/TestRealTimeReturnPipeline1.test1/ cd+++++++ real_time/test/TestRealTimeReturnPipeline1.test1/output/ > f+++++++ real_time/test/TestRealTimeReturnPipeline1.test1/output/test.txt .d..t.... returns/ > f..t.... returns/__init__.py > f..t.... returns/pipeline.py .d..t.... returns/test/ > f..t.... returns/test/test_returns_pipeline.py .d..t.... returns/test/TestReturnsBuilder.test_equities1/ .d..t.... returns/test/TestReturnsBuilder.test_equities1/output/ .d..t.... returns/test/TestReturnsBuilder.test_futures1/ .d..t.... returns/test/TestReturnsBuilder.test_futures1/output/ > rsync --dry-run -rtui --delete $CMAMP_DIR/$SUBDIR/ $AMP_DIR/$SUBDIR/ > f..t.... price/__init__.py > f..t.... price/pipeline.py > f..t.... real_time/pipeline.py > f..t.... real_time/test/test_dataflow_amp_real_time_pipeline.py > f..t.... returns/test/TestReturnsBuilder.test_equities1/output/test.txt > f..t.... returns/test/TestReturnsBuilder.test_futures1/output/test.txt Sync everything # Check if there is anything in cmamp more recent than amp > rsync -au --exclude='.git' --exclude='devops' $CMAMP_DIR/ $AMP_DIR # vimdiff > dev_scripts/diff_to_vimdiff.py --dir1 $AMP_DIR --dir2 $CMAMP_DIR F1: skip F9: choose left (i.e., amp) F10: choose right (i.e,. cmamp) # Copy > rsync -au --delete --exclude='.git' --exclude='devops' --exclude='im' $AMP_DIR/ $CMAMP_DIR # Add all the untracked files > (cd $CMAMP_DIR/$SUBDIR && git add $(git ls-files -o --exclude-standard)) > diff -r --brief $AMP_DIR $CMAMP_DIR | grep -v .git | grep Only Files that need to be different amp needs an if False helpers/lib_tasks.py amp needs two tests disabled im/ccxt/data/load/test/test_loader.py im/ccxt/data/load/test/test_loader.py TODO(gp): How to copy files in vimdiff including last line? Have a script to remove all the last lines Some files end with an 0x0a tr -d '\\r' ```bash find . -name \"*.txt\" | xargs perl -pi -e 's/\\r\\n/\\n/g' # Remove No newline at end of file find . -name \"*.txt\" | xargs perl -pi -e 'chomp if eof' ``` Lint everything > autoflake amp_check_filename amp_isort amp_flake8 amp_class_method_order amp_normalize_import amp_format_separating_line amp_black > i lint --phases=\"amp_isort amp_class_method_order amp_normalize_import amp_format_separating_line amp_black\" --files='$(find . -name \"\\*.py\")' Testing Run amp on my laptop (or on the server) IN PROGRESS: Get amp PR to pass on GH IN PROGRESS: Run lemonade on my laptop Run cmamp on the dev server Get cmamp PR to pass on GH Run dev_tools on the dev server","title":"Development Workflow"},{"location":"Development_workflow/#development-workflow","text":"Setting up Git credentials Preamble Check Git credentials Setting Git credentials Enforcing Git credentials Create the env Invoke Listing all the tasks Implementation details GitHub See all the workflows Getting help for a workflow Merge master in the current branch Create a PR Extract a PR from a larger one Using git Systematic code transformation Generate a local amp Docker image Update the dev amp Docker image Experiment in a local image GitHub Actions (CI) pytest Run with coverage Iterating on stacktrace of failing test Iterating on a failing regression test Detect mismatches with golden test outcomes Playback Publish a notebook Detailed instructions Publish notebooks Open a published notebook Start a server Using the dev box Using Windows browser How to create a private fork Integrate public to private: amp -> cmamp Set-up Ours vs theirs Sync the repos (after double integration) Updated sync Check that things are fine Integrate private to public: cmamp -> amp Squash commit of everything in the branch Double integration cmamp amp Script set-up Manual set-up branches High-level plan Sync im cmamp -> amp Sync everything Files that need to be different Lint everything Testing","title":"Development Workflow"},{"location":"Development_workflow/#setting-up-git-credentials","text":"","title":"Setting up Git credentials"},{"location":"Development_workflow/#preamble","text":"Git allows setting credentials at different \"levels\": system (set for all the users in /etc/git ) global (set for a single user in $HOME/.gitconfig or $HOME/.config/git/config ) local (set on a per client basis in .git/config in the repo root) Git uses a hierarchical config approach in which settings of a broader scope are inherited if not overridden. Refs: How to customize Git: https://git-scm.com/book/en/v2/Customizing-Git-Git-Configuration Details on git config : https://git-scm.com/docs/git-config","title":"Preamble"},{"location":"Development_workflow/#check-git-credentials","text":"You can check the Git credentials that will be used to commit in a client by running: ```bash git config -l | grep user user.name=saggese user.email=saggese@gmail.com github.user=gpsaggese ``` To know at which level each variable is defined, run ```bash git config --show-origin user.name file:/Users/saggese/.gitconfig saggese ``` You can see all the Authors in a Git repo history with: ```bash git log | grep -i Author | sort | uniq ... ``` Git doesn't do any validation of user.name and user.email but it just uses these values to compose a commit message like: ```bash git log -2 commit 31052d05c226b1c9834d954e0c3d5586ed35f41e (HEAD -> AmpTask1290_Avoid_committing_to_master_by_mistake) Author: saggese saggese@gmail.com Date: Mon Jun 21 16:22:25 2021 Update hooks ```","title":"Check Git credentials"},{"location":"Development_workflow/#setting-git-credentials","text":"To keep things simple and avoid variability, our convention is to use: as user.name our Linux user name on the local computer we are using to commit which is returned by whoami (e.g., user.name=saggese ) as user.email the email that corresponds to that user (e.g,. user.email=saggese@gmail.com ) To accomplish the set-up above you can: use in /Users/saggese/.gitconfig the values for our open-source account, so that they are used by default ```bash git config --global user.name $(whoami) git config --global user.email YOUR_EMAIL ``` use the correct user / email in the repos that are not open-source ```bash cd $GIT_ROOT git config --local user.name $(whoami) git config --local user.email YOUR_EMAIL ``` Note that you need to set these local values on each Git client that you have cloned, since Git doesn't version control these values","title":"Setting Git credentials"},{"location":"Development_workflow/#enforcing-git-credentials","text":"We use Git hooks to enforce that certain emails are used for certain repos (e.g., we should commit to our open-source repos only using our personal non-corporate email). You need to install the hooks in each Git client that you use. Conceptually this step is part of git clone : every time you clone a repo locally you need to set the hooks. TODO(gp): We could create a script to automate cloning a repo and setting it up. ```bash cd //amp ./dev_scripts/git/git_hooks/install_hooks.py --action install cd //lem ./amp/dev_scripts/git/git_hooks/install_hooks.py --action install ``` This procedure creates some links from .git/hook to the scripts in the repo. You can also use the action status to see the status and remove to the hooks.","title":"Enforcing Git credentials"},{"location":"Development_workflow/#create-the-env","text":"You can follow the ```bash # Build the client env dev_scripts/client_setup/build.sh source dev_scripts/setenv_amp.sh ```","title":"Create the env"},{"location":"Development_workflow/#invoke","text":"We use invoke to implement workflows (aka \"tasks\") similar to Makefile targets, but using Python. The official documentation. We use invoke to automate tasks and package workflows for: docker: docker_* Git: git_* GitHub (relying on gh integration): gh_* running tests: run_* integrate: integrate_* releasing tools and Docker images: docker_* lint: lint_* pytest: Each set of commands starts with the topic, e.g., docker_* for all the docker elated tasks The best approach to getting familiar with the tasks is to browse the list and then check the output of the help ```bash invoke --help command i -h gh_issue_title Usage: inv[oke] [--core-opts] gh_issue_title [--options] [other tasks here ...] Docstring: Print the title that corresponds to the given issue and repo_short_name. E.g., AmpTask1251_Update_GH_actions_for_amp. :param pbcopy: save the result into the system clipboard (only on macOS) Options: -i STRING, --issue-id=STRING -p, --[no-]pbcopy -r STRING, --repo-short-name=STRING ``` I can guarantee you a 2x improvement in performance, if you master the workflows, but it takes some time and patience","title":"Invoke"},{"location":"Development_workflow/#listing-all-the-tasks","text":"> invoke --list INFO: > cmd='/Users/saggese/src/venv/amp.client_venv/bin/invoke --list' Available tasks: check_python_files Compile and execute Python files checking for errors. docker_bash Start a bash shell inside the container corresponding to a stage. docker_build_local_image Build a local image (i.e., a release candidate \"dev\" image). docker_build_prod_image (ONLY CI/CD) Build a prod image. docker_cmd Execute the command `cmd` inside a container corresponding to a stage. docker_images_ls_repo List images in the logged in repo_short_name. docker_jupyter Run jupyter notebook server. docker_kill Kill the last Docker container started. docker_login Log in the AM Docker repo_short_name on AWS. docker_ps List all the running containers. docker_pull Pull latest dev image corresponding to the current repo from the registry. docker_pull_dev_tools Pull latest prod image of `dev_tools` from the registry. docker_push_dev_image (ONLY CI/CD) Push the \"dev\" image to ECR. docker_push_prod_image (ONLY CI/CD) Push the \"prod\" image to ECR. docker_release_all (ONLY CI/CD) Release both dev and prod image to ECR. docker_release_dev_image (ONLY CI/CD) Build, test, and release to ECR the latest \"dev\" image. docker_release_prod_image (ONLY CI/CD) Build, test, and release to ECR the prod image. docker_rollback_dev_image Rollback the version of the dev image. docker_rollback_prod_image Rollback the version of the prod image. docker_stats Report last started Docker container stats, e.g., CPU, RAM. docker_tag_local_image_as_dev (ONLY CI/CD) Mark the \"local\" image as \"dev\". find_check_string_output Find output of check_string() in the test running find_test_class Report test files containing `class_name` in a format compatible with find_test_decorator Report test files containing `class_name` in pytest format. fix_perms :param action: gh_create_pr Create a draft PR for the current branch in the corresponding gh_issue_title Print the title that corresponds to the given issue and repo_short_name. gh_workflow_list Report the status of the GH workflows. gh_workflow_run Run GH workflows in a branch. git_add_all_untracked Add all untracked files to Git. git_branch_copy Create a new branch with the same content of the current branch. git_branch_diff_with_base Diff files of the current branch with master at the branching point. git_branch_files Report which files were added, changed, and modified in the current branch git_branch_next_name Return a name derived from the branch so that the branch does not exist. git_clean Clean the repo_short_name and its submodules from artifacts. git_create_branch Create and push upstream branch `branch_name` or the one corresponding to git_create_patch Create a patch file for the entire repo_short_name client from the base git_delete_merged_branches Remove (both local and remote) branches that have been merged into master. git_files Report which files are changed in the current branch with respect to git_last_commit_files Print the status of the files in the previous commit. git_merge_master Merge `origin/master` into the current branch. git_pull Pull all the repos. git_fetch_master Pull master without changing branch. git_rename_branch Rename current branch both locally and remotely. integrate_compare_branch_with_base Compare the files modified in both the branches in src_dir and dst_dir to integrate_copy_dirs Copy dir `subdir` from dir `src_dir` to `dst_dir`. integrate_create_branch Create the branch for integration in the current dir. integrate_diff_dirs Integrate repos from dir `src_dir` to `dst_dir`. lint Lint files. lint_create_branch Create the branch for linting in the current dir. print_setup Print some configuration variables. print_tasks Print all the available tasks in `lib_tasks.py`. pytest_clean Clean pytest artifacts. pytest_compare Compare the output of two runs of `pytest -s --dbg` removing irrelevant pytest_failed Process the list of failed tests from a pytest run. pytest_failed_freeze_test_list Copy last list of failed tests to not overwrite with successive pytest run_blank_tests (ONLY CI/CD) Test that pytest in the container works. run_coverage_report run_fast_slow_tests Run fast and slow tests independently. run_fast_tests Run fast tests. run_qa_tests Run QA tests independently. run_slow_tests Run slow tests. run_superslow_tests Run superslow tests. traceback Parse the traceback from Pytest and navigate it with vim.","title":"Listing all the tasks"},{"location":"Development_workflow/#implementation-details","text":"By convention all invoke targets are in *_lib_tasks.py , e.g., helpers/lib_tasks.py - tasks to be run in cmamp optimizer/opt_lib_tasks.py - tasks to be run in cmamp/optimizer All invoke tasks are functions with the @task decorator, e.g., ```python from invoke import task @task def invoke_task(...): ... ``` To run a task we use context.run(...) , see the official docs To be able to run a specified invoke task one should import it in tasks.py E.g., see cmamp/tasks.py A task can be run only in a dir where it is imported in a corresponding tasks.py , e.g., invoke_task1 is imported in cmamp/tasks.py so it can be run only from cmamp invoke_task2 is imported in cmamp/optimizer/tasks.py so it can be run only from cmamp/optimizer In other words one should do cd cmamp/optimizer before doing i invoke_task2 ...","title":"Implementation details"},{"location":"Development_workflow/#github","text":"Get the official branch name corresponding to an Issue ```bash i gh_issue_title -i 256 ## gh_issue_title: issue_id='256', repo_short_name='current' # Copied to system clipboard: AmpTask256_Part_task2236_jenkins_cleanup_split_scripts: https://github.com/alphamatic/amp/pull/256 ```","title":"GitHub"},{"location":"Development_workflow/#see-all-the-workflows","text":"Workflows are organized somehow around Linux command that they are related to, e.g., docker_* for all docker related workflows find_* for all workflows related to finding (e.g., unit tests) gh_* for GitHub related workflows git_* for Git related workflows etc. Once in a while it can be useful to list all the available workflows and see if something interesting was added. ```bash invoke --list Available tasks: docker_bash Start a bash shell inside the container corresponding to a stage. docker_build_local_image Build a local image (i.e., a release candidate \"dev\" image). ... ```","title":"See all the workflows"},{"location":"Development_workflow/#getting-help-for-a-workflow","text":"You can get a more detailed help with ```bash invoke --help run_fast_tests Usage: inv[oke] [--core-opts] run_fast_tests [--options] [other tasks here ...] Docstring: Run fast tests. :param stage: select a specific stage for the Docker image :param pytest_opts: option for pytest :param pytest_mark: test list to select as @pytest.mark.XYZ :param dir_name: dir to start searching for tests :param skip_submodules: ignore all the dir inside a submodule :param coverage: enable coverage computation :param collect_only: do not run tests but show what will be executed Options: -c, --coverage -d STRING, --dir-name=STRING -k, --skip-submodules -o, --collect-only -p STRING, --pytest-opts=STRING -s STRING, --stage=STRING -y STRING, --pytest-mark=STRING ```","title":"Getting help for a workflow"},{"location":"Development_workflow/#merge-master-in-the-current-branch","text":"> i git_merge_master","title":"Merge master in the current branch"},{"location":"Development_workflow/#create-a-pr","text":"TODO(gp): Describe","title":"Create a PR"},{"location":"Development_workflow/#extract-a-pr-from-a-larger-one","text":"My workflow is to have a feature branch (e.g., AmpTask1891_Sketch_out_the_design_of_RT_OMS ) that I develop in. When a piece of code can be merged: create a new branch (e.g., AmpTask1891_Sketch_out_the_design_of_RT_OMS_02 ) copy the current feature branch to the new branch remove the pieces that you don't want to merge run regressions PR merge into master merge master into the feature branch This workflow allows you to develop and regress / merge without too much hassle solving the problem of \"stacked PRs\". ```bash # Go to the client with the branch that you want to divvy up. git checkout ${feature_branch} # Make sure that the branch is up-to-date with master i git_merge_master # Lint. i lint -b # Create a patch from the branch (there are many options to tweak the workflow, check the help) i git_create_patch -b # To apply the patch and execute: git checkout 8f9cda97 git apply /Users/saggese/src/lemonade1/amp/patch.amp.8f9cda97.20210609_080439.patch ... ``` Go to a fresh Git client (I have 2-3 Git clients separated from the one in which I develop for this kind of operations) or go to master in the same Git client ```bash # Go to master git checkout master # Apply the patch from the run of git_create_patch git apply /Users/saggese/src/lemonade1/amp/patch.amp.8f9cda97.20210609_080439.patch # This patch should apply cleanly and with no errors from git, otherwise it means that your feature branch does not have the latest master # Remove what you don't want to commit. # Do not change anything or run the linter otherwise your feature branch will not merge easily. git diff git checkout master -- ... ... git commit; git push # Create a PR (non-draft so that GH can start running the tests) i gh_create_pr --no-draft # Regress the branch i run_fast_tests ... # Merge the PR into master # Go back to your feature branch and merge master gco ${feature_branch} git pull # Now one piece of your feature branch has been merged and you can repeat until all the code is merged. ```","title":"Extract a PR from a larger one"},{"location":"Development_workflow/#using-git","text":"> git checkout `dst_branch` > git merge --squash --ff `src_branch` > git reset HEAD","title":"Using git"},{"location":"Development_workflow/#systematic-code-transformation","text":"See the help of amp/dev_scripts/replace_text.py","title":"Systematic code transformation"},{"location":"Development_workflow/#generate-a-local-amp-docker-image","text":"This is a manual flow used to test and debug images before releasing them to the team. The flow is similar to the dev image, but by default tests are not run and the image is not released. ```bash # Build the local image (and update Poetry dependencies, if needed). i docker_build_local_image --update-poetry ... docker image ls 665840871993.dkr.ecr.us-east-1.amazonaws.com/amp:local REPOSITORY TAG IMAGE ID CREATED SIZE 665840871993.dkr.ecr.us-east-1.amazonaws.com/amp local 9b3f8f103a2c 1 second ago 1.72GB # Test the new \"local\" image i docker_bash --stage \"local\" python -c \"import async_solipsism\" python -c \"import async_solipsism; print(async_solipsism. version )\" # Run the tests with local image # Make sure the new image is used: e.g., add an import and trigger the tests. i run_fast_tests --stage \"local\" --pytest-opts core/dataflow/test/test_real_time.py i run_fast_slow_tests --stage \"local\" # Promote a local image to dev. i docker_tag_local_image_as_dev i docker_push_dev_image ``` ## Update the dev amp Docker image To implement the entire Docker QA process of a dev image ```bash # Clean all the Docker images locally, to make sure there is no hidden state. docker system prune --all # Update the needed packages. devops/docker_build/pyproject.toml # Visually inspect the updated packages. git diff devops/docker_build/poetry.lock # Run entire release process. i docker_release_dev_image ``` ## Experiment in a local image To install packages in an image, do i docker_bash ```bash # Switch to root and install package. sudo su - source /venv/bin/activate pip install # Switch back to user. exit ``` You should test that the package is installed for your user, e.g., ```bash source /venv/bin/activate python -c \"import foobar; print(foobar);print(foobar. version )\" ``` You can now use the package in this container. Note that if you exit the container, the modified image is lost, so you need to install it again. You can save the modified image, tagging the new image as local, while the container is still running. Copy your Container ID. You can find it in the docker bash session, e.g., if the command line in the container starts with user_1011@da8f3bb8f53b:/app$ , your Container ID is da8f3bb8f53b by listing running containers, e.g., run docker ps outside the container Commit image ```bash docker commit /cmamp:local-$USER ``` E.g. docker commit da8f3bb8f53b 665840871993.dkr.ecr.us-east-1.amazonaws.com/cmamp:local-julias If you are running inside a notebook using i docker_jupyter you can install packages using a one liner ! sudo su -; source ...;","title":"Generate a local amp Docker image"},{"location":"Development_workflow/#github-actions-ci","text":"## Running a single test in GH Actions Create a branch Change .github/workflows/fast_tests.yml run: invoke run_fast_tests ## run: invoke run_fast_tests --pytest-opts=\"helpers/test/test_git.py::Test_git_modified_files1::test_get_modified_files_in_branch1 -s --dbg\" # In the current implementation (where we try to not run for branches) to run in a branch","title":"GitHub Actions (CI)"},{"location":"Development_workflow/#pytest","text":"From https://gist.github.com/kwmiebach/3fd49612ef7a52b5ce3a","title":"pytest"},{"location":"Development_workflow/#run-with-coverage","text":"> i run_fast_tests --pytest-opts=\"core/test/test_finance.py\" --coverage","title":"Run with coverage"},{"location":"Development_workflow/#iterating-on-stacktrace-of-failing-test","text":"Inside docker bash ```bash pytest ... ``` The test fails: switch to using pytest.sh to save the stacktrace to a file Then from outside Docker launch vim in quickfix mode ```bash invoke traceback ``` The short form is it","title":"Iterating on stacktrace of failing test"},{"location":"Development_workflow/#iterating-on-a-failing-regression-test","text":"The workflow is: ```bash # Run a lot of tests, e.g., the entire regression suite. pytest ... # Some tests fail. # Run the pytest_repro to summarize test failures and to generate commands to reproduce them. invoke pytest_repro ```","title":"Iterating on a failing regression test"},{"location":"Development_workflow/#detect-mismatches-with-golden-test-outcomes","text":"The command is ```bash i pytest_find_unused_goldens ``` The specific dir to check can be specified with the dir_name parameter. The invoke detects and logs mismatches between the tests and the golden outcome files. When goldens are required by the tests but the corresponding files do not exist This usually happens if the tests are skipped or commented out. Sometimes it's a FP hit (e.g. the method doesn't actually call check_string but instead has it in a string, or check_string is called on a missing file on purpose to verify that an exception is raised). When the existing golden files are not actually required by the corresponding tests. In most cases it means the files are outdated and can be deleted. Alternatively, it can be a FN hit: the test method A, which the golden outcome corresponds to, doesn't call check_string directly, but the test's class inherits from a different class, which in turn has a method B that calls check_string , and this method B is called in the test method A. For more details see CmTask528 .","title":"Detect mismatches with golden test outcomes"},{"location":"Development_workflow/#playback","text":"","title":"Playback"},{"location":"Development_workflow/#publish-a-notebook","text":"publish_notebook.py is a little tool that allows to: Opening a notebook in your browser (useful for read-only mode) E.g., without having to use Jupyter notebook (which modifies the file in your client) or github preview (which is slow or fails when the notebook is too large) Sharing a notebook with others in a simple way Pointing to detailed documentation in your analysis Google docs Reviewing someone's notebook Comparing multiple notebooks against each other in different browser windows Taking a snapshot / checkpoint of a notebook as a backup or before making changes This is a lightweight alternative to \"unit testing\" to capture the desired behavior of a notebook One can take a snapshot and visually compare multiple notebooks side-by-side for changes","title":"Publish a notebook"},{"location":"Development_workflow/#detailed-instructions","text":"You can get details by running: ```bash dev_scripts/notebooks/publish_notebook.py -h ``` Plug-in for Chrome my-s3-browser","title":"Detailed instructions"},{"location":"Development_workflow/#publish-notebooks","text":"Make sure that your environment is set up properly ```bash more ~/.aws/credentials [am] aws_access_key_id= aws_secret_access_key= aws_s3_bucket=alphamatic-data printenv | grep AM_ AM_AWS_PROFILE=am ``` If you don't have them, you need to re-run source dev_scripts/setenv.sh in all the shells. It might be easier to kill that tmux session and restart it ```bash tmux kill-session --t limeXYZ ~/go_lem.sh XYZ ``` Inside or outside a Docker bash run ```bash publish_notebook.py --file http://127.0.0.1:2908/notebooks/notebooks/Task40_Optimizer.ipynb --action publish_on_s3 ``` The file is copied to S3 bash Copying './Task40_Optimizer.20210717_010806.html' to 's3://alphamatic-data/notebooks/Task40_Optimizer.20210717_010806.html' You can also save the data locally: ```bash publish_notebook.py --file amp/oms/notebooks/Master_forecast_processor_reader.ipynb --action publish_on_s3 --aws_profile saml-spm-sasm ``` You can also use a different path or profile by specifying it directly ```bash publish_notebook.py \\ --file http://127.0.0.1:2908/notebooks/notebooks/Task40_Optimizer.ipynb \\ --action publish_on_s3 \\ --s3_path s3://alphamatic-data/notebooks \\ --aws_profile am ```","title":"Publish notebooks"},{"location":"Development_workflow/#open-a-published-notebook","text":"","title":"Open a published notebook"},{"location":"Development_workflow/#start-a-server","text":"(cd /local/home/share/html/published_notebooks; python3 -m http.server 8000) go to the page in the local browser","title":"Start a server"},{"location":"Development_workflow/#using-the-dev-box","text":"To open a notebook saved on S3, *outside* a Docker container run: ```bash publish_notebook.py --action open --file s3://alphamatic-data/notebooks/Task40_Optimizer.20210717_010806.html ``` This opens a Chrome window through X-windows. To open files faster you can open a Chrome window in background with ```bash google-chrome ``` and then navigate to the path (e.g., /local/home/share/html/published_notebooks/Master_forecast_processor_reader.20220810-112328.html )","title":"Using the dev box"},{"location":"Development_workflow/#using-windows-browser","text":"Another approach is: ```bash aws s3 presign --expires-in 36000 s3://alphamatic-data/notebooks/Task40_Optimizer.20210716_194400.html | xclip ``` Open the link saved in the clipboard in the Windows browser For some reason, Chrome saves the link instead of opening, so you need to click on the saved link","title":"Using Windows browser"},{"location":"Development_workflow/#how-to-create-a-private-fork","text":"https://stackoverflow.com/questions/10065526/github-how-to-make-a-fork-of-public-repository-private From https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/creating-a-repository-on-github/duplicating-a-repository ```bash git clone --bare git@github.com:alphamatic/amp.git amp_bare git push --mirror https://github.com/cryptomtc/cmamp.git ``` It worked only as cryptomtc, but not using my key","title":"How to create a private fork"},{"location":"Development_workflow/#integrate-public-to-private-amp-cmamp","text":"","title":"Integrate public to private: amp -&gt; cmamp"},{"location":"Development_workflow/#set-up","text":"> git remote add public git@github.com:alphamatic/amp # Go to cmamp > cd /data/saggese/src/cmamp1 > cd /Users/saggese/src/cmamp1 # Add the remote # git remote add public https://github.com/exampleuser/public-repo.git > git remote add public git@github.com:alphamatic/amp > git remote -v origin https://github.com/cryptomtc/cmamp.git (fetch) origin https://github.com/cryptomtc/cmamp.git (push) public git@github.com:alphamatic/amp (fetch) public git@github.com:alphamatic/amp(push)","title":"Set-up"},{"location":"Development_workflow/#ours-vs-theirs","text":"From https://stackoverflow.com/questions/25576415/what-is-the-precise-meaning-of-ours-and-theirs-in-git/25576672 When merging: ours = branch checked out (git checkout *ours*) theirs = branch being merged (git merge *theirs*) When rebasing the role is swapped ours = branch being rebased onto (e.g., master) theirs = branch being rebased (e.g., feature)","title":"Ours vs theirs"},{"location":"Development_workflow/#sync-the-repos-after-double-integration","text":"> git fetch origin; git fetch public # Pull from both repos > git pull public master -X ours # You might want to use `git pull -X theirs` or `ours` > git pull -X theirs > git pull public master -s recursive -X ours # When there is a file added it is better to add > git diff --name-status --diff-filter=U | awk '{print $2}' im/ccxt/db/test/test_ccxt_db_utils.py # Merge branch > gs + git status On branch AmpTask1786_Integrate_20211128_02 Your branch and 'origin/AmpTask1786_Integrate_20211128_02' have diverged, and have 861 and 489 different commits each, respectively. (use \"git pull\" to merge the remote branch into yours) You are in a sparse checkout with 100% of tracked files present. nothing to commit, working tree clean > git pull -X ours ## Make sure it's synced at ToT > rsync --delete -r /Users/saggese/src/cmamp2/ /Users/saggese/src/cmamp1 --exclude='.git/' > diff -r --brief /Users/saggese/src/cmamp1 /Users/saggese/src/cmamp2 | grep -v \\.git","title":"Sync the repos (after double integration)"},{"location":"Development_workflow/#updated-sync","text":"> git fetch origin; git fetch public","title":"Updated sync"},{"location":"Development_workflow/#check-that-things-are-fine","text":"> git diff origin/master... >patch.txt > cd /Users/saggese/src/cmamp2 # Create a branch > git checkout -b Cmamp114_Integrate_amp_cmamp_20210928 > git apply patch.txt # Compare branch with references > dev_scripts/diff_to_vimdiff.py --dir1 /Users/saggese/src/cmamp1/im --dir2 /Users/saggese/src/cmamp2/im > diff -r --brief /Users/saggese/src/lemonade3/amp \\~/src/cmamp2 | grep -v \"/im\" # Creates a merge commit > git push origin master","title":"Check that things are fine"},{"location":"Development_workflow/#integrate-private-to-public-cmamp-amp","text":"> cd /data/saggese/src/cmamp1 > tar cvzf patch.tgz $(git diff --name-onlyorigin/master public/master | grep -v repo_config.py) > cd /Users/saggese/src/amp1 git remote add cmamp > git@github.com:cryptomtc/cmamp.git > GIT_SSH_COMMAND=\"ssh -i \\~/.ssh/cryptomatic/id_rsa.cryptomtc.github\" git fetch > git@github.com:cryptomtc/cmamp.git > git checkout -b Integrate_20210928 > GIT_SSH_COMMAND=\"ssh -i \\~/.ssh/cryptomatic/id_rsa.cryptomtc.github\" git pull > cmamp master -X ours","title":"Integrate private to public: cmamp -&gt; amp"},{"location":"Development_workflow/#squash-commit-of-everything-in-the-branch","text":"From https://stackoverflow.com/questions/25356810/git-how-to-squash-all-commits-on-branch ```bash git checkout yourBranch git reset $(git merge-base master $(git branch --show-current)) git add -A git commit -m \"Squash\" git push --force ```","title":"Squash commit of everything in the branch"},{"location":"Development_workflow/#double-integration-cmamp-amp","text":"The bug is https://github.com/alphamatic/amp/issues/1786","title":"Double integration cmamp &lt; -- &gt; amp"},{"location":"Development_workflow/#script-set-up","text":"> vi /Users/saggese/src/amp1/dev_scripts/integrate_repos/setup.sh Update the date > vi /Users/saggese/src/amp1/dev_scripts/integrate_repos/* > cd \\~/src/amp1 > source /Users/saggese/src/amp1/dev_scripts/integrate_repos/setup.sh > cd \\~/src/cmamp1 > source /Users/saggese/src/amp1/dev_scripts/integrate_repos/setup.sh","title":"Script set-up"},{"location":"Development_workflow/#manual-set-up-branches","text":"# Go to cmamp1 > go_amp.sh cmamp 1 # Set up the env vars in both clients > export AMP_DIR=/Users/saggese/src/amp1; export CMAMP_DIR=/Users/saggese/src/cmamp1; echo \"$AMP_DIR\"; ls $AMP_DIR; echo \"$CMAMP_DIR\"; ls $CMAMP_DIR # Create two branches > export BRANCH_NAME=AmpTask1786_Integrate_20211010 export BRANCH_NAME=AmpTask1786_Integrate_2021117 ... > cd $AMP_DIR # Create automatically > i git_create_branch -b $BRANCH_NAME # Create manually > git checkout -b $BRANCH_NAME > git push --set-upstream origin $BRANCH_NAME > cd $CMAMP_DIR > i git_create_branch -b $BRANCH_NAME","title":"Manual set-up branches"},{"location":"Development_workflow/#high-level-plan","text":"SUBDIR=im Typically cmamp is copied on top of amp SUBDIR=devops cmamp and amp need to be different (until we unify the Docker flow) Everything else Typically amp -> cmamp","title":"High-level plan"},{"location":"Development_workflow/#sync-im-cmamp-amp","text":"SUBDIR=im # Check different files > diff -r --brief $AMP_DIR/$SUBDIR $CMAMP_DIR/$SUBDIR | grep -v .git # Diff the entire dirs with vimdiff > dev_scripts/diff_to_vimdiff.py --dir1 $AMP_DIR/$SUBDIR --dir2 $CMAMP_DIR/$SUBDIR # Find different files > find $AMP_DIR/$SUBDIR -name \"*\"; find $CMAMP_DIR/$SUBDIR -name \"*\" sdiff /tmp/dir1 /tmp/dir2 # Copy cmamp -> amp > rsync --delete -au $CMAMP_DIR/$SUBDIR/ $AMP_DIR/$SUBDIR -a = archive -u = ignore newer # Add all the untracked files > cd $AMP_DIR/$SUBDIR && git add $(git ls-files -o --exclude-standard) # Check that there are no differences after copying > dev_scripts/diff_to_vimdiff.py --dir1 $AMP_DIR/$SUBDIR --dir2 $CMAMP_DIR/$SUBDIR ========== > rsync --delete -rtu $AMP_DIR/$SUBDIR/ $CMAMP_DIR/$SUBDIR > rsync --dry-run -rtui --delete $AMP_DIR/$SUBDIR/ $CMAMP_DIR/$SUBDIR/ .d..t.... ./ > f..t.... __init__.py cd+++++++ features/ > f+++++++ features/__init__.py > f+++++++ features/pipeline.py cd+++++++ features/test/ > f+++++++ features/test/test_feature_pipeline.py cd+++++++ features/test/TestFeaturePipeline.test1/ cd+++++++ features/test/TestFeaturePipeline.test1/output/ > f+++++++ features/test/TestFeaturePipeline.test1/output/test.txt .d..t.... price/ .d..t.... real_time/ > f..t.... real_time/__init__.py .d..t.... real_time/notebooks/ > f..t.... real_time/notebooks/Implement_RT_interface.ipynb > f..t.... real_time/notebooks/Implement_RT_interface.py .d..t.... real_time/test/ cd+++++++ real_time/test/TestRealTimeReturnPipeline1.test1/ cd+++++++ real_time/test/TestRealTimeReturnPipeline1.test1/output/ > f+++++++ real_time/test/TestRealTimeReturnPipeline1.test1/output/test.txt .d..t.... returns/ > f..t.... returns/__init__.py > f..t.... returns/pipeline.py .d..t.... returns/test/ > f..t.... returns/test/test_returns_pipeline.py .d..t.... returns/test/TestReturnsBuilder.test_equities1/ .d..t.... returns/test/TestReturnsBuilder.test_equities1/output/ .d..t.... returns/test/TestReturnsBuilder.test_futures1/ .d..t.... returns/test/TestReturnsBuilder.test_futures1/output/ > rsync --dry-run -rtui --delete $CMAMP_DIR/$SUBDIR/ $AMP_DIR/$SUBDIR/ > f..t.... price/__init__.py > f..t.... price/pipeline.py > f..t.... real_time/pipeline.py > f..t.... real_time/test/test_dataflow_amp_real_time_pipeline.py > f..t.... returns/test/TestReturnsBuilder.test_equities1/output/test.txt > f..t.... returns/test/TestReturnsBuilder.test_futures1/output/test.txt","title":"Sync im cmamp -&gt; amp"},{"location":"Development_workflow/#sync-everything","text":"# Check if there is anything in cmamp more recent than amp > rsync -au --exclude='.git' --exclude='devops' $CMAMP_DIR/ $AMP_DIR # vimdiff > dev_scripts/diff_to_vimdiff.py --dir1 $AMP_DIR --dir2 $CMAMP_DIR F1: skip F9: choose left (i.e., amp) F10: choose right (i.e,. cmamp) # Copy > rsync -au --delete --exclude='.git' --exclude='devops' --exclude='im' $AMP_DIR/ $CMAMP_DIR # Add all the untracked files > (cd $CMAMP_DIR/$SUBDIR && git add $(git ls-files -o --exclude-standard)) > diff -r --brief $AMP_DIR $CMAMP_DIR | grep -v .git | grep Only","title":"Sync everything"},{"location":"Development_workflow/#files-that-need-to-be-different","text":"amp needs an if False helpers/lib_tasks.py amp needs two tests disabled im/ccxt/data/load/test/test_loader.py im/ccxt/data/load/test/test_loader.py TODO(gp): How to copy files in vimdiff including last line? Have a script to remove all the last lines Some files end with an 0x0a tr -d '\\r' ```bash find . -name \"*.txt\" | xargs perl -pi -e 's/\\r\\n/\\n/g' # Remove No newline at end of file find . -name \"*.txt\" | xargs perl -pi -e 'chomp if eof' ```","title":"Files that need to be different"},{"location":"Development_workflow/#lint-everything","text":"> autoflake amp_check_filename amp_isort amp_flake8 amp_class_method_order amp_normalize_import amp_format_separating_line amp_black > i lint --phases=\"amp_isort amp_class_method_order amp_normalize_import amp_format_separating_line amp_black\" --files='$(find . -name \"\\*.py\")'","title":"Lint everything"},{"location":"Development_workflow/#testing","text":"Run amp on my laptop (or on the server) IN PROGRESS: Get amp PR to pass on GH IN PROGRESS: Run lemonade on my laptop Run cmamp on the dev server Get cmamp PR to pass on GH Run dev_tools on the dev server","title":"Testing"},{"location":"Docker/","text":"Docker Introduction Concepts Docker image Docker container Docker registry High level philosophy Thin client amp / cmamp container Prod container Infra container Relevant bugs Poetry Build a Docker image General Dockerfile Base image Copy files Install OS packages Install Python packages Build an image from a Dockerfile Run multi-container Docker application Version Images Bind mount Environment variables Basic commands How to test a package in a Docker container Hacky approach to patch up a container How to release a Docker image Multi-architecture build Stages Local Dev Prod Overview of how to release an image How to add a Python package to Docker image How to find unused packages Import-based approach using pipreqs How it works Limitations Usage How to build a local image Testing the local image Tag local image as dev Push image End-to-end flow for dev image Build prod image QA for prod image End-to-end flow for prod image Flow for both dev and prod images Docker-in-docker (dind) Sibling container approach Connecting to Postgres instance using sibling containers Release flow cmamp dev_tools Design release flow - discussion QA flow Dev_tools container Optimizer container Rationale Build and run a local version of opt Internals One container per Git repo Multiple containers per Git repo Mounting only optimizer dir inside Docker Mounting the supermodule (e.g., lime, lemonade, amp) inside Docker Invariants Release and ECR flow Unit testing code inside opt container Avoid compiling code depending from cvxopt when running amp Run optimizer tests in a stand-alone opt container Run optimizer tests as part of running unit tests for cmamp Call a Dockerized executable from a container Introduction Docker is an open-source tool designed to make our life typically easier (although sometimes it makes it harder) when creating, building, deploying, and running software applications. Docker can package an application and its dependencies in a virtual container that can run on any Linux, Windows, or macOS computer. Our Docker containers have everything required (e.g. OS packages, Python packages) inside to run certain applications/code. Concepts Docker image A Docker image is a read-only template with instructions for creating a Docker container Typically instructions include information about which packages and their versions to install, e.g. list of python packages and their corresponding versions All steps needed to create the image and run it are defined in a Dockerfile, e.g. dev_tools/devops/docker_build/dev.Dockerfile Docker container A Docker container is a runnable instance of an image. One can run code inside a docker container having all requirements installed. Docker registry A Docker registry stores docker images. In other words, Docker registry for docker images is like GitHub for code. High level philosophy We always want to separate things that don't need to run together in different containers (e.g., dev / prod cmamp , optimizer , im , oms , dev_tools ), along a logic of \"independently runnable / deployable directories\". The problem is that when we put too many dependencies in a single container, trying to simplify the release approach we start having huge containers that are difficult to deploy and are unstable in terms of building even using poetry . Each dir that can be \"deployed\" and run should have a devops dir to build / qa / release containers with all the needed dependencies Certain containers that need to be widely available to the team and deployed go through the release process and ECR Other containers that are lightweight and used only by one person (e.g., the infra container) can be built on the fly using docker compose / docker build . Thin client To bootstrap the system we use a \"thin client\" which installs in a virtual env the minimum set of packages to run (e.g., installs invoke , docker , etc). TODO(gp): Audit / make sure we can simplify the thin env amp / cmamp container The dev version is used to develop The prod version can be used for deployment as shortcut to creating a smaller container with only the strictly needed dependencies Prod container In order to avoid shipping the monster cmamp dev / prod container, we want to start building smaller containers with only the dependencies that specific prod scripts need Infra container To run infra script, if we only need boto3 and moto , we can create a Python library create a script interface create an invoke task that calls i docker_cmd --cmd ... reusing the cmamp container, (since that container already has boto3 and moto that are dependencies we can't remove) This approach is similar to calling the linter If we think we need to add new packages only for running infra scripts then we will create a new infra container. We can build on the fly and not release through ECR We can start with approach 1, which will also allow us to transition to 2 transparently, if needed Relevant bugs https://github.com/cryptokaizen/cmamp/issues/1060 Tool to extract the dependency from a project #1038 Create tool for poetry debugging #1026 Fix tests that fail due to pandas update and release cmamp image #1002 Poetry Poetry is a tool for managing Python packages and dependencies: List packages you want to install with some constraints, e.g., pandas must be above 1.0 in devops/docker_build/pyproject.toml Given a list of packages you need to install to get the desired environment, you want poetry to \"optimize\" the packages and generate devops/docker_build/poetry.lock , which contains the list of versions of the packages to install If there is a new version of a package re-running poetry might give you an updated list of packages to install Build a Docker image General A docker image is built from a Dockerfile . The image is then used to run a Docker container. There is /devops dir under a project's dir that contains Docker-related files, e.g. cmamp/devops . Dockerfile A Dockerfile is a text document that contains all the commands to call on the command line to assemble an image. E.g. cmamp/devops/docker_build/dev.Dockerfile . Base image A Dockerfile should start with specifying a base image. Base image is an image that a new image is built from. A new Docker image will have all the packages/dependencies that are installed in the base image. Use FROM statement to specify a base image, e.g. FROM ubuntu:20.4 Copy files Copy files that are required to build a Docker image to the Docker filesystem. To copy a file from /source_dir (your filesystem) to /dst_dir (Docker filesystem) do: COPY source_dir/file dst_dir E.g., the command below will copy install_packages.sh from devops/docker_build to the Docker's root directory so that install_packages.sh can be accessed by Docker. COPY devops/docker_build/install_packages.sh . Install OS packages Install OS packages that are needed for a Docker app, but that are not installed for a base image. Use RUN instruction to install a package, e.g. RUN apt-get install postgresql-client Alternatively you can package all installation instructions in a .sh file and run it. Do not forget to copy a .sh file to the Docker filesystem so that Docker can see it. E.g., COPY devops/docker_build/install_packages.sh . RUN /bin/sh -c \"./install_packages.sh\" Install Python packages We prefer to install Python packages with poetry . Make sure that there is instruction to install pip3 and poetry . You can either put it in a Dockerfile or in a separate file like install_packages.sh . RUN apt-get install python3-pip RUN pip3 install poetry Copy poetry-related files to the Docker filesystem so that files can be accessed by Docker COPY devops/docker_build/poetry.toml COPY devops/docker_build/poetry.lock Update Python packages RUN poetry install Build an image from a Dockerfile To build an image from a Dockerfile run: ``` docker build . ``` The Dockerfile must be called Dockerfile and located in the root of the context. You can point to any Dockerfile by using -f : ``` docker build -f /path_to_dockerfile/dockerfile_name ``` Run multi-container Docker application Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application's services. Version At the beginning of a docker-compose.yaml file specify the docker-compose version. We use the version 3.0 , for more information see the official documents . version: \"3.0\" Images You can either re-use a public image or build a new one from a Dockerfile . The app service below uses the image that is built from the dev.Dockerfile . app: build: context: . dockerfile: dev.Dockerfile The im_postgres_local service below uses the public postgres image pulled from the Docker hub registry . im_postgres_local: image: postgres: 13 Bind mount If you want to be able to access code inside a Docker container, you should bind-mount a directory with the code on the host. Mount a directory on the host inside a Docker container, e.g. mount current directory to /app dir inside a Docker container: ``` app: volumes: .:/app ``` Environment variables You can either use variables directly from the environment or pass them in a docker-compose.yaml file. It is supposed that POSTGRES_VERSION is already defined in the shell. db: image: \"postgres:${POSTGRES_VERSION}\" Set environment variable in a service's container ``` db: environment: POSTGRES_VERSION=13 image: \"postgres:${POSTGRES_VERSION}\" ``` Set environment variable with .env file ``` db: env_file: ./postgres_env.env image: \"postgres:${POSTGRES_VERSION}\" ``` File postgres_env.env ``` cat ./postgres_env.env POSTGRES_VERSION=13 ``` Basic commands To check more advanced usage, please see the official documentation. Build, (re)create, start, and attach to containers for a service. It is assumed that a docker-compose file has the name docker-compose.yaml and is located in the current dir. ``` docker-compose up ``` List containers ``` docker-compose ps ``` Stop containers created by down . ``` docker-compose down ``` How to test a package in a Docker container > sudo /bin/bash -c \"(source /venv/bin/activate; pip install yfinance)\" > python -c \"import finance\" Hacky approach to patch up a container # After install create a new version of the container > docker commit d2916dd5f122 > 623860924167.dkr.ecr.eu-north-1.amazonaws.com/cmamp:dev_ccxtpr # Push to the repo > docker push 623860924167.dkr.ecr.eu-north-1.amazonaws.com/cmamp:dev_ccxtpro # Then you can push and pull on different machines > docker pull 623860924167.dkr.ecr.eu-north-1.amazonaws.com/cmamp:dev_ccxtpro # To use `docker_bash` you might need to retag it to match what the system expects > docker tag 623860924167.dkr.ecr.eu-north-1.amazonaws.com/cmamp:dev_ccxtpro How to release a Docker image All the invoke tasks to run the release flow are in //amp/helpers/lib_tasks.py . Depending on the type of changes sometimes one needs to rebuild only the prod image, other times one needs to rebuild also the dev image. E.g., If you change Docker build-related things (e.g., add a Python package), you need to rebuild the dev image and then the prod image from the dev image If you change the code for a production system, you need to create a new prod image We try to use the same flow, conventions, and code for all the containers (e.g., amp, cmamp, dev_tools, opt). Multi-architecture build To build multi-arch (e.g., x86 , arm ) docker image using docker_build_local_image we should use --multi-build flag To build for specific platforms specify the platform name: For x86 - linux/amd64 For arm - linux/arm64 ``` i docker_build_local_image --version --multi-build --platform ``` To build for both arm and x86 architectures: ``` i docker_build_local_image --version --multi-build --platform linux/amd64,linux/arm64 ``` Multi-arch images are built using docker buildx which do not generate any local image by default Images are pushed to the remote registry and pulled for testing and usage To tag the local image as dev and push it to the registry, use ``` i docker_tag_push_multi_build_local_image_as_dev --version ``` Stages A \"stage\" is a step (e.g., local, dev, prod) in our release workflow of Docker images, code, or infrastructure. To run a Docker container in a certain stage use the stage parameter E.g. i docker_bash --stage=\"local\" creates a bash session inside the local docker amp container Local A local image is used to develop and test an update to the Docker container, e.g. after updating a package, installing a new package, etc. Local images can only be accessed locally by a developer, i.e. the team members can not / should not use local images. In practice local images are like dev images but private to users and servers. Dev A dev image is used by our team to develop our systems (e.g., to add new functionalities to the dev_tools code). Typically the source code is mounted through a bind mount in Docker so that one can change the code and execute it in Docker. The image is tested, blessed, and released so that users and CI can use it without worries. Once a dev image is pushed to the docker registry it can be pulled and used by the team members. Prod A prod image is used to run a system by final users. E.g., the linter inside dev_tools , some prod system inside Airflow. It is self-contained (it should have no dependencies) since it has everything required to run a system installed inside it, e.g., code (e.g., the linter), Python packages. It is typically created from the dev image by copying the released code inside the prod image. Overview of how to release an image The release flow consists of the following phases Make changes to the image E.g., add Python package Update the changelog Build a local image Run specific tests (e.g., make sure that the new packages are installed) Run unit tests Run QA tests Tag local image as dev image Push dev image to ECR If there is also an associated prod image Build prod image from dev image Run unit / QA tests Push prod image to ECR How to add a Python package to Docker image To add a new Python package to a Docker image you need to update poetry files and release a new image: Add a new package to amp/devops/docker_build/pyproject.toml file to the [tool.poetry.dependencies] section E.g., to add pytest-timeout do: [tool.poetry.dependencies] ... pytest-timeout = \"*\" ... In general we use the latest version of a package ( * ) until the tests fail or the system stops working If the system fails, we freeze the version of the problematic packages to a known-good version to get the tests back to green until the problem is solved. We switch back to the latest version once the problem is fixed If you need to put a constraint on the package version, follow the official docs , and explain in a comment why this is needed making reference to GitHub issues To verify that package is installed correctly one can build a local image and update poetry ``` i docker_build_local_image --version {new version} --update-poetry ``` run a docker container based on the local image ``` i docker_bash --stage local --version {new version} ``` verify what package was installed with pip show {package name} , e.g., ``` pip show pytest-rerunfailures Name: pytest-rerunfailures Version: 10.2 Summary: pytest plugin to re-run tests to eliminate flaky failures ... Location: /venv/lib/python3.8/site-packages Requires: pytest, setuptools Required-by: ``` run regressions for the local image, i.e. ``` i run_fast_tests --stage local --version {new version} i run_slow_tests --stage local --version {new version} ``` Update the changelog describing the new version Send a PR with the updated poetry files and any other change needed to make the tests pass Release the new image. To do so follow the # Release a Docker image section, use --update-poetry flag to resolve the dependencies How to find unused packages While installing Python packages we need to make sure that we do not install packages that we do not use Import-based approach using pipreqs How it works To do so we use an import-based approach provided by pipreqs . Under the hood it uses the regex below and os.walk for selected dir: REGEXP = [ re.compile(r'^import (.+)$'), re.compile(r'^from ((?!\\.+).*?) import (?:.*)$') ] Limitations Not all packages that we use are necessarily imported, e.g. awscli , jupyter , pytest-cov , etc. -> pipreqs won't find these packages The import name is not always equal to the package actual name, see the mapping here Usage See the official docs for the advanced usage. Run a bash session inside a Docker container Install pipreqs with sudo pip install pipreqs We install it temporary within a Docker bash session in order to introduce another dependency You need to re-install pipreqs everytime you create a new Docker bash session To run for a root dir do: pipreqs . --savepath ./tmp.requirements.txt The command above will generate ./tmp.requirements.txt with the list of the imported packages, e.g., amp==1.1.4 async_solipsism==0.3 beautifulsoup4==4.11.1 botocore==1.24.37 cvxopt==1.3.0 cvxpy==1.2.0 dill==0.3.4 environs==9.5.0 ... You can grep for a package name to see where it is used, e.g., ``` jackpy \"dill\" helpers/hpickle.py:108: import dill ... ``` How to build a local image The recipe to build a local image is in devops/docker_build/dev.Dockerfile . This launches various scripts to install: OS Python venv + Python packages jupyter extensions application-specific packages (e.g., for the linter) To build a local image run: ``` i docker_build_local_image --version 1.0.0 # Build from scratch and not incrementally. i docker_build_local_image --version 1.0.0 --no-cache # Update poetry package list. i docker_build_local_image --version 1.0.0 --update-poetry # Update poetry package list and build from scratch. i docker_build_local_image --version 1.0.0 --update-poetry --no-cache # See more options: i docker_build_local_image -h ``` Once an image is built, it is tagged as local-${user}-${version} , e.g., local-saggese-1.0.0 ``` Successfully tagged 665840871993.dkr.ecr.us-east-1.amazonaws.com/amp:local-gsaggese-1.0.9 docker image ls 665840871993.dkr.ecr.us-east-1.amazonaws.com/amp:local-gsaggese-1.0.9 REPOSITORY TAG IMAGE ID CREATED SIZE 665840871993.dkr.ecr.us-east-1.amazonaws.com/amp local-gsaggese-1.0.9 cf16e3e3d1c7 Less than a second ago 2.75GB ``` A local image is a candidate for becoming a dev image. ``` i run_fast_tests --stage local --version 1.0.0 ``` Testing the local image Testing the local image ``` i docker_bash pip list | tee pip_packages.dev.txt i docker_cmd --cmd \"pip list | tee pip_packages.dev.txt\" i docker_bash --stage local --version 1.0.9 pip list | tee pip_packages.local.txt ``` or in one command: ``` i docker_cmd --cmd \"pip list | tee pip_packages.dev.txt\"; i docker_cmd --stage=local --version=1.0.9 --cmd \"pip list | tee pip_packages.local.txt\" vimdiff pip_packages.dev.txt pip_packages.local.txt ``` You can move the local image on different servers for testing by pushing it on ECR: ``` i docker_login i docker push 665840871993.dkr.ecr.us-east-1.amazonaws.com/amp:local-gsaggese-1.1.0 ``` Tag local image as dev Docker tag is just a way of referring to an image. A good analogy is how Git tags refer to a particular commit in your history. Basically, tagging is creating a reference from one image ( local-saggese-1.0.0 ) to another ( dev ) Once the local image is tagged as dev , your dev image becomes equal to local-saggese-1.0.0 dev image is also tagged with dev-${version} , e.g., dev-1.0.0 to preserve history and allow for quick rollback. Locally in git repository a git tag ${repo_name}-${version} , e.g. cmamp-1.0.0 is created in order to properly control sync between code and container. Push image To push dev or prod image means to send it to the docker registry. It is more like pushing a commit to the GitHub Once an image is pushed, it can be used by the team members by running i docker_pull Local git tag ${repo_name}-${version} , e.g. cmamp-1.0.0 , is pushed at this stage to the remote repository to allow others to properly control sync between code and container. To be able to push an image to the ECR one should have permissions to do so End-to-end flow for dev image Conceptually the flow consists of the following phases: Build a local image of docker i docker_build_local_image --version 1.0.0 Run fast tests to verify that nothing is broken i run_fast_tests --stage local --version 1.0.0 Run end-to-end tests by, e.g., running linter on some file i lint --files helpers/tasks.py --stage local --version 1.0.0 Tag local image as dev i docker_tag_local_image_as_dev --version 1.0.0 Push dev image to the docker registry i docker_push_dev_image --version 1.0.0 The mentioned flow is executed by Build dev image GH action and that is a preferred way to do an image release. For specific cases that can not be done via GH action see commands below: ``` # To run the official flow end-to-end: i docker_release_dev_image --version 1.0.0 # To see the options: i docker_release_dev_image -h # Run from scratch and not incrementally: i docker_release_dev_image --version 1.0.0 --no-cache # Force an update to poetry to pick up new packages i docker_release_dev_image --version 1.0.0 --update-poetry # Skip running the QA tests i docker_release_dev_image --version 1.0.0 --no-qa-tests # Skip running the tests i docker_release_dev_image --version 1.0.0 --skip-tests # Skip end-to-end tests i docker_release_dev_image --version 1.0.0 --no-run-end-to-end-tests ``` Build prod image The recipe to build a prod image is in dev_tools/devops/docker_build/prod.Dockerfile . The main difference between dev image and prod image is that source code is accessed through a bind mount for dev image (so that it can be easily modified) and copied inside the image for a prod image (since we want to package the code) requirements to be installed are different: dev image requires packages to develop and run the code prod image requires packages only to run the code To build the prod image run: ``` i docker_build_prod_image --version 1.0.0 # Check the options: i docker_build_prod_image -h # To build from scratch and not incrementally: i docker_build_prod_image --version 1.0.0 --no-cache ``` To run a command inside the prod image ``` docker run --rm -t --user $(id -u):$(id -g) --workdir=/app 665840871993.dkr.ecr.us-east-1.amazonaws.com/cmamp:prod-1.0.3 \"ls -l /app\" ``` Example of a complex command: ``` docker run --rm -t --workdir=/app 665840871993.dkr.ecr.us-east-1.amazonaws.com/cmamp:prod-1.0.3 \"python /app/im_v2/ccxt/data/extract/download_realtime.py --to_datetime '20211204-194432' --from_datetime '20211204-193932' --dst_dir 'test/ccxt_test' --data_type 'ohlcv' --api_keys 'API_keys.json' --universe 'v03'\" ``` QA for prod image In dev_scripts repo test: ``` i lint --files \"linters/amp_black.py\" ``` In amp repo make sure: ``` i lint -f \"helpers/dbg.py\" ``` End-to-end flow for prod image Build docker prod image i docker_build_prod_image --version 1.0.0 Run fast tests to verify that nothing is broken i run_fast_tests Push prod image to the docker registry i docker_push_prod_image --version 1.0.0 To run the flow end-to-end do: ``` i docker_release_prod_image --version 1.0.0 ``` same options are available as for i docker_release_dev_image check options i docker_release_prod_image -h Flow for both dev and prod images To run both flows end-to-end do: i docker_release_all Alternatively, one can run the release stages step-by-step. Docker-in-docker (dind) It is possible to install a Docker engine inside a Docker container so that one can run Docker container (e.g., OMS or IM) inside an isolated amp container. The problems with this approach are: dind requires to run the external container in privileged mode, which might not be possible due to security concerns the Docker / build cache is not shared across parent and children containers, so one needs to pull / build an image every time the outermost container is restarted An alternative approach is the \"sibling container\" approach Sibling container approach Refs: Can I run Docker-in-Docker without using the --privileged flag - Stack Overflow https://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/ Often what's really needed is the ability to build / run a container from another container (e.g., CI or unit test). This can be achieved by mounting the Docker socket /var/run/docker.sock to the container, so that a container can talk to Docker Engine. This approach allows reuse of the build cache across the sibling containers. The downside is less isolation from the external container, e.g., spawned containers can be left hanging or can collide. E.g., `` # Run docker ps` in a container, showing the containers running in the main container docker run -ti --rm \\ -v /var/run/docker.sock:/var/run/docker.sock \\ dindtest \\ docker ps # Start a sibling hello world container: docker run -it --rm \\ -v /var/run/docker.sock:/var/run/docker.sock \\ dindtest \\ docker run -ti --rm hello-world ``` Connecting to Postgres instance using sibling containers We can start the Docker container with Postgres as a service from outside the container. ``` (cd oms; i oms_docker_up -s local) INFO: > cmd='/local/home/gsaggese/src/venv/amp.client_venv/bin/invoke oms_docker_up -s local' report_memory_usage=False report_cpu_usage=False docker-compose \\ --file /local/home/gsaggese/src/sasm-lime4/amp/oms/devops/compose/docker-compose.yml \\ --env-file /local/home/gsaggese/src/sasm-lime4/amp/oms/devops/env/local.oms_db_config.env \\ up \\ oms_postgres Creating compose_oms_postgres_1 ... done Attaching to compose_oms_postgres_1 oms_postgres_1 | oms_postgres_1 | PostgreSQL Database directory appears to contain a database; Skipping initialization oms_postgres_1 | oms_postgres_1 | 2022-05-19 22:57:15.659 UTC [1] LOG: starting PostgreSQL 13.5 (Debian 13.5-1.pgdg110+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit oms_postgres_1 | 2022-05-19 22:57:15.659 UTC [1] LOG: listening on IPv4 address \"0.0.0.0\", port 5432 oms_postgres_1 | 2022-05-19 22:57:15.659 UTC [1] LOG: listening on IPv6 address \"::\", port 5432 oms_postgres_1 | 2022-05-19 22:57:15.663 UTC [1] LOG: listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\" oms_postgres_1 | 2022-05-19 22:57:15.670 UTC [25] LOG: database system was shut down at 2022-05-19 22:56:50 UTC oms_postgres_1 | 2022-05-19 22:57:15.674 UTC [1] LOG: database system is ready to accept connections ``` Note that Postgres needs to be Start a container able to From inside a container I launch postgres through the /var/... ``` docker ps | grep postgres CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 83bba0818c74 postgres:13 \"docker-entrypoint.s...\" 6 minutes ago Up 6 minutes 0.0.0.0:5432->5432/tcp compose-oms_postgres-1 ``` Test connection to the DB from outside the container ``` psql --host=cf-spm-dev4 --port=5432 --user aljsdalsd -d oms_postgres_db_local Password for user aljsdalsd: psql (9.5.25, server 13.5 (Debian 13.5-1.pgdg110+1)) WARNING: psql major version 9.5, server major version 13. Some psql features might not work. Type \"help\" for help. oms_postgres_db_local=# ``` Test connection to the DB from inside the container ``` psql --host=cf-spm-dev4 --port=5432 --user aljsdalsd -d oms_postgres_db_local ... ``` Release flow cmamp File an Issue for the release (e.g., \"Add package foobar to cmamp image\") Create the corresponding branch with i git_create_branch -i ${issue_number} Change the code Update the changelog, i.e. //cmamp/changelog.txt Specify what was changed Pick the release version accordingly We use semantic versioning convention For example, adding a package to the image would mean bumping up version 1.0.0 to 1.0.1 Test the change using the local release flow i docker_build_local_image -v ${version} If a new package is added run docker_build_local_image with --update-poetry option and check in a poetry.lock file Make sure that the tests pass i run_fast_slow_tests -s local -v ${version} , and that the goal of the Issue is achieved (e.g., a new package is visible, the package version has been updated) Do a PR with the change including the updated changelog.txt , the poetry files (both the specs devops/docker_build/poetry.toml and the package version devops/docker_build/poetry.lock ) Run the release flow manually (or rely on GH Action build workflow to create the new image) ``` # Release dev image i docker_release_dev_image --version $version # Pick up the new image from ECR i docker_pull ``` Send a message on the all@ chat telling people that a new version of the XYZ container has been released Users need to do a i docker_pull to get the new container Users that don't update should see a message telling them that the code and container are not in sync any more, e.g.,: ``` This code is not in sync with the container: code_version='1.0.3' != container_version='amp-1.0.3' You need to: - merge origin/master into your branch with invoke git_merge_master - pull the latest container with invoke docker_pull ``` dev_tools File an Issue for the release Create the corresponding branch in dev_tools Change the code Run the release flow end-to-end ``` i docker_release_dev_image --version 1.1.0 i docker_release_prod_image --version 1.1.0 ``` Update the changelog, i.e. //dev_tools/changelog.txt The changelog should be updated only after the image is released; otherwise the sanity checks will assert that the release's version is not higher than the latest version recorded in the changelog. Specify what has changed Pick the release version accordingly NB! The release version should consist of 3 digits, e.g. \"1.1.0\" instead of \"1.1\" We use semantic versioning convention For example, adding a package to the image would mean bumping up version 1.0.0 to 1.0.1 Do a PR with the change including the updated changelog.txt Send a message on the all@ chat telling people that a new version of the container has been released Users need to do i docker_pull from dev_tools , i docker_pull_dev_tools from cmamp Users need to make sure to pull docker after the master is up-to-date (including amp submodules) Design release flow - discussion TODO(gp, Vitalii): Turn this into a description of the release flow Let's assume that we want to release dev image with version 1.2.3: > i docker_build_local_image --tag-name 1.2.3 Initially we thought about using Git tags to mark releases points in the source repo for dev and prod releases (but not local since local is reserved to private use by a user). This approach is elegant, but it has some corner cases when used with containers for multiple repos that contain Git submodules. We decided to use an approach where a changelog.txt file contains the latest code version all test tasks now also use hversion.get_code_version() that calls hgit.git_describe() to get latest tag in the repo (1.0.0 in this case) Agree. git_describe will need to accept a dir to find the tag of the releasable dir when we are satisfied with local image, we run i docker_tag_local_image_as_dev We will still need to pass --version 1.0.0 invoke internally tags local-1.0.0 as dev-1.0.0 , in addition to dev Both for Git tags and docker tags then we run i docker_push_dev_image We will still need to pass --version 1.0.0 invoke internally pushes both dev-1.0.0 and dev images to ECR **AND** pushes local 1.0.0 git tag to remote git repo (github) docker_release_dev_image will do basically the same (will require tag_name). Of course docker_release... is just a convenience wrapper running all the stages Now let's assume we want to promote dev image to prod: then we run i docker_build_prod_image invoke internally checks with hversion.get_code_version() and builds prod-1.0.0 based on dev-1.0.0 , also tagging prod-1.0.0 as prod then we run i docker_push_prod_image invoke pushes prod-1.0.0 and prod tags to ECR docker_release_prod_image will do basically the same (will require tag_name). Q0: Is the flow ok? Yes Q1: The flow is the same for dev_tools and cmamp , but to update the version of image on which dev_tools is based -- we'll need to modify Dockerfile now. Is that ok? Maybe we should just create the dev_tools from scratch using the full-blown flow instead of build on top of it The idea of building on top of it, was just a shortcut but it is creating more problems that what it's worth it Then everything looks and behaves the same TODO(vitalii): File a bug, if we don't have it yet Q2: If the flow is run in the submodule, e.g. in amp dir, currently the behaviour is not well defined. Commands will try to build cmamp image in this case, but code version will be from dev_tools -- should we fix this? We are going towards the concept of \"releasable dirs\" (see im, optimizer). If there is a dir with devops, then that dir runs inside a container The \"Git version\" should be associated to the dir we are releasing (e.g., cmamp, im, optimizer, dev_tools) Vitalii: If we will have monorepo with releasable dirs, then indeed git tags are not that comfortable to use, however I could argue that when one releases im image with version 1.0.0, he gets docker image im:dev-1.0.0 , im:prod-1.0.0 , im:dev and im:prod -- but how then one is able to find corresponding code that was used in that image? Perhaps instead, we could share namespace of git tags between all tags. E.g. in git repo (github) we will have: im-dev-1.0.0 cmamp-dev-1.0.0 im-prod-1.0.0 GP: Point taken. In fact the code in a releasable dir still needs code from other submodules (e.g., helpers). One approach is to put the Git hash in version.txt. The one you suggest (of tagging the entire repo) with also info on the dir makes sense. I think the Git tags are designed to do what we want, so let's use them. Q3: We don't need version.txt file in this flow. I will remove it, ok? Yes, we can remove version.txt and use a README or changelog in the releasable dir The flow is similar to what I thought. Some observations / questions: INV: version becomes mandatory in the release flow This requires a lot of cosmetic changes to the code since now it's optional, but it's worth make the changes We need to ensure that version can only be created going fwd. We can do a comparison of the current version with the new version as tuples (we could use semver but it feels not needed) The workflows are: Build a local image Release a dev image Release a prod image Rollback an image We rarely move the dev / prod tag back, but rather users needs to docker pull an older image and pass --base name --stage and --version to docker {bash, cmd, jupyter} Then the image is fixed going forward A releasable dir has a repo_config Maybe we should call it component_config since now also dirs can be released README.md or changelog.md devops tasks.py (with the exposed Invoke tasks) lib_tasks.py (with the custom invoke tasks) We want to try to move to helpers/lib_tasks all the \"common\" code without dependencies from the specific sw components. We pass function pointers for callbacks. What to do with: CONTAINER_VERSION='amp-1.1.1' BUILD_TAG='amp-1.1.1-20211114_093142-AmpTask1845_Get_docker_in_docker_to_work-47fb46513f084b8f3c9008a2e623ec05040a10e9' QA flow The goal is to test that the container as a whole works We want to run the container as a user would do Usually we run tests inside a container to verify that the code is correct To test the container itself right now we test outside (in the thin client) > pytest -m qa test --image_stage dev The problem is that now the thin client needs to have a bunch of deps (including pytest, pandas and so on) which defeats the purpose of the thin env dev_scripts_devto/client_setup/ E.g., //amp/dev_scripts/client_setup/requirements.txt A hack is to vimdiff /Users/saggese/src/lemonade2/amp/dev_scripts/client_setup/requirements.txt dev_scripts_devto/client_setup/requirements.txt > source dev_scripts_devto/client_setup/build.sh A possible solution is to use Docker-in-Docker in this way we don't have to pollute the thin env with a bunch of stuff Talk to Grisha and Vitalii This works in dev_tools because the code for the import detector is there and we are using a dev container which binds the src dir to the container > i lint_detect_cycles --dir-name import_check/test/Test_detect_import_cycles.test1/input/ --stage dev In all the other repos, one needs to use the prod of dev_tools container (that's what the user would do) Next steps: TODO(Sonya + Grisha): release the prod dev_toools container as it is TODO(Sonya + Grisha): document dev_tools, release procedure TODO(Sonya): pull prod dev_tools (i docker_pull_dev_tools) and test that now in cmamp the tool works TODO(gp): figure out the QA workflow (and improve the thin client with dind) To break the circular dep we release a prod-candidate Dev_tools container For specific dev_tools workflows see Optimizer container Rationale The high-level goal is to move towards containerized Python scripts running in smaller containers instead of keep adding packages to amp / cmamp , which makes the amp / cmamp container bloated and risky to build Along this design philosophy similar to microservices, we want to have a Docker container, called opt with a Python script that uses some packages that are not compatible with amp (specifically cvxopt, cvxpy) This is similar to what we do for the dev_tools , which is like a containerized Python script for the linter Build and run a local version of opt You can build the container locally with: ``` cd optimizer i opt_docker_build_local_image --version 0.1.0 ``` This process takes around 5 mins and then you should have the container docker image ls 665840871993.dkr.ecr.us-east-1.amazonaws.com/opt:local-saggese-0.1.0 REPOSITORY TAG IMAGE ID CREATED SIZE 665840871993.dkr.ecr.us-east-1.amazonaws.com/opt local-saggese-0.1.0 bb7d60d6a7d0 7 seconds ago 1.23GB Run the container as: ``` i opt_docker_bash --stage local --version 0.1.0 ``` To run a Jupyter notebook in the opt container: Internals One container per Git repo A simple approach is to have each deployable unit (i.e., container) corresponding to a Git repo The consequence would be: a multiplication of repos no implicit sharing of code across different containers some mechanism to share code (e.g., helpers ) across repos (e.g., using bind mount) not playing nice with Git subrepo mechanism since Docker needs to see the entire repo So the code would be organized in 4 repos: - lemonade / lime - helpers - optimizer - oms - models in amp where the dependency between containers are lemonade -> amp amp -> optimizer, helpers optimizer -> helpers, core Multiple containers per Git repo Another approach is to have optimizer as a directory inside amp This keeps amp and optimizer in a single repo To build / run optimizer code in its container one needs to cd in the dir The problem then becomes how to share helpers Mounting only optimizer dir inside Docker From devops/compose/docker-compose.yml 42 volumes: 43 # Move one dir up to include the entire git repo (see AmpTask1017). 44 - ../../:/app 45 # Move one dir down to include the entire git repo (see AmpTask1017). 46 working_dir: /app From devops/docker_build/dev.Dockerfile ENTRYPOINT [\"devops/docker_run/entrypoint.sh\"] The problem is that Git repo doesn't work anymore git --version: git version 2.30.2 fatal: not a git repository (or any parent up to mount point /) Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set). A work around is to inject .git in /git of the container and then point git to that ``` environment: ... - GIT_DIR=/git volumes: # Move one dir up to include the entire git repo (see AmpTask1017). - ../../:/app - ../../../../.git:/git - ../../../../amp/helpers:/app/helpers ``` Git works but it gets confused with the paths modified: .dockerignore deleted: .github/gh_requirements.txt deleted: .github/workflows/build_image.yml.DISABLED deleted: .github/workflows/fast_tests.yml deleted: .github/workflows/linter.yml.DISABLED deleted: .github/workflows/slow_tests.yml deleted: .github/workflows/superslow_tests.yml.DISABLED deleted: .gitignore Mounting the supermodule (e.g., lime, lemonade, amp) inside Docker From devops/compose/docker-compose.yml 42 volumes: 43 # Move one dir up to include the entire git repo (see AmpTask1017). 44 - ../../../:/app 45 # Move one dir down to include the entire git repo (see AmpTask1017). 46 working_dir: /app/amp From devops/docker_build/dev.Dockerfile ENTRYPOINT [\"optimizer/devops/docker_run/entrypoint.sh\"] This approach mounts 4 dirs up from devops/compose/docker-compose.yml, i.e., //lime The problem with this approach is that now repo_config.py is incorrect i opt_docker_build_local_image --version 0.4.0 32 - ../../../helpers:/app/amp/optimizer/helpers 33 34 # Shared cache. This is specific of lime. 35 - /local/home/share/cache:/cache 36 37 # Mount `amp` when it is used as submodule. In this case we need to 38 # mount the super project in the container (to make git work with the 39 # supermodule) and then change dir to `amp`. 40 app: 41 extends: 42 base_app 43 volumes: 44 # Move one dir up to include the entire git repo (see AmpTask1017). 45 - ../../../../:/app 46 # Move one dir down to include the entire git repo (see AmpTask1017). 47 working_dir: /app/amp/optimizer 48 #entrypoint: /bin/bash -c \"ls helpers\" Invariants A deployable dir is a dir under a Git repo It corresponds to a software component (code + library = Docker container) Anything that has a devops dir is \"deployable\" Each Docker container is run from its corresponding dir, e.g., amp container from the amp dir amp container from the lemonade dir (this is just a shortcut since lemonade has the same deps right now as amp) Always mount the outermost Git repo under /app Set the Docker working dir as the current dir Each deployable dir specifies all the needed information in repo_config.py (which is the one in the current dir) What container to run What functionality is supported on different servers (e.g., privileged way) The changelog.txt file is in the deployable dir (e.g., optimizer/changelog.txt) Each One run the invoke commands from optimizer dir When the Docker container starts the current dir is optimizer helpers, core is mounted in the same dir You can't see code outside optimizer TODO(gp): running in amp under lemonade should use the local repo_config Release and ECR flow TODO(gp): Implement this Unit testing code inside opt container Since we want to segregate the package dependencies in different containers, tests that have a dependency from cvxopt /cvxpy can't be run inside the amp container but need to be run inside opt . We want to: (as always) write and run unit tests for the optimizer code in isolation, i.e., test the code in the directory optimizer by itself run all the tests for the entire repo (relying on both containers amp and optimizer with a single command invocation) be able to run tests belonging to only one of the containers to shorten the debugging cycle To achieve this we need to solve the 3 problems below. Avoid compiling code depending from cvxopt when running amp We can't parse code (e.g., in pytest ) that includes packages that are not present in a container E.g., pytest running in amp should not parse code in //amp/optimizer since it contains imports that will fail Solution 1 We use the pytest mechanism cvx = pytest.importorskip(\"cvxpy\") which is conceptually equivalent to: ``` try: import cvxopt has_cvxopt = True except ImportError: has_cvxopt = False if has_cvxopt: def utils1(): cvxopt\u2026 ``` Solution 2 Test in eachfile for the existence of the needed packages and enclose the code in an if _has_package Pros: We can skip code based dynamically on a try ... except ImportModule to check what packages are present Cons: Repeat the same piece of try ... except in many places Solution: we can factor it out in a function We need to enclose the code in a if ... that screws up the indentation and makes the code weird Solution 3 Exclude certain directories (e.g., //amp/optimizer ) from pytest Pros: We don't have to spread the try ... except and if \\_has_package in the code Cons: The directory is relative to the top directory Solution: we can use a regex to specify the dir without the full path Which directories are included and excluded depends on where pytest is run E.g., running pytest in an amp container we need to skip the optimizer dir, while pytest in an optimizer container should skip everything but the optimizer dir Solution 4 Exclude certain directories or files based on which container we are running in Cons: We need to have a way to determine in which container we are running Solution: we can use the env vars we use for versioning ``` echo $AM_CONTAINER_VERSION amp-1.0.3- ``` Given the pros and cons, we decided to follow Solution 1 and Solution 3 Run optimizer tests in a stand-alone opt container To run the optimizer tests, you can create an opt container and then run pytest ``` cd optimizer i opt_docker_bash docker> pytest . ``` We wrap this in an invoke target like i opt_run_fast_tests Alternative solution We can use dind to run the opt container inside a cmamp one Cons: dind complicates the system dind is not supported everywhere (one needs privileged containers) dind is slower since there are 2 levels of (relatively fast) virtualization Run optimizer tests as part of running unit tests for cmamp We use the same mechanism as run_fast_slow_superslow_tests to pull together different test lists Call a Dockerized executable from a container From https://github.com/cryptokaizen/cmamp/issues/1357 We need to call something from amp to opt Docker Solution 1 Inside the code we build the command line cmd = 'docker run -it ... '; system(cmd) Cons: there is code replicated between here and the invoke task (e.g., the info about the container, ...) Solution 2 Call the Dockerized executable using the docker_cmd invoke target cmd = \"invoke opt_docker_cmd -cmd '...'\" system(cmd) Pros: All the Docker commands go through the same interface inside invoke Cons Bash interpolation in the command Another level of indirection: do a system call to call invoke , invoke calls docker, docker does the work invoke needs to be installed inside the calling container Solution 3 Call opt_lib_tasks.py opt_docker_cmd(cmd, ...) Pros avoid doing a call to invoke can deal with bash interpolation in Python We should always use Solution 3, although in the code sometimes we use Solution 1 and 2 (but we should replace in favor of Solution 3). The interface to the Dockerized optimizer is in run_optimizer in //amp/oms/call_optimizer.py To run the examples ``` cd //lime i docker_bash pytest ./amp/oms/test/test_call_optimizer.py::Test_run_dockerized_optimizer1 ```","title":"Docker"},{"location":"Docker/#docker","text":"Introduction Concepts Docker image Docker container Docker registry High level philosophy Thin client amp / cmamp container Prod container Infra container Relevant bugs Poetry Build a Docker image General Dockerfile Base image Copy files Install OS packages Install Python packages Build an image from a Dockerfile Run multi-container Docker application Version Images Bind mount Environment variables Basic commands How to test a package in a Docker container Hacky approach to patch up a container How to release a Docker image Multi-architecture build Stages Local Dev Prod Overview of how to release an image How to add a Python package to Docker image How to find unused packages Import-based approach using pipreqs How it works Limitations Usage How to build a local image Testing the local image Tag local image as dev Push image End-to-end flow for dev image Build prod image QA for prod image End-to-end flow for prod image Flow for both dev and prod images Docker-in-docker (dind) Sibling container approach Connecting to Postgres instance using sibling containers Release flow cmamp dev_tools Design release flow - discussion QA flow Dev_tools container Optimizer container Rationale Build and run a local version of opt Internals One container per Git repo Multiple containers per Git repo Mounting only optimizer dir inside Docker Mounting the supermodule (e.g., lime, lemonade, amp) inside Docker Invariants Release and ECR flow Unit testing code inside opt container Avoid compiling code depending from cvxopt when running amp Run optimizer tests in a stand-alone opt container Run optimizer tests as part of running unit tests for cmamp Call a Dockerized executable from a container","title":"Docker"},{"location":"Docker/#introduction","text":"Docker is an open-source tool designed to make our life typically easier (although sometimes it makes it harder) when creating, building, deploying, and running software applications. Docker can package an application and its dependencies in a virtual container that can run on any Linux, Windows, or macOS computer. Our Docker containers have everything required (e.g. OS packages, Python packages) inside to run certain applications/code.","title":"Introduction"},{"location":"Docker/#concepts","text":"","title":"Concepts"},{"location":"Docker/#docker-image","text":"A Docker image is a read-only template with instructions for creating a Docker container Typically instructions include information about which packages and their versions to install, e.g. list of python packages and their corresponding versions All steps needed to create the image and run it are defined in a Dockerfile, e.g. dev_tools/devops/docker_build/dev.Dockerfile","title":"Docker image"},{"location":"Docker/#docker-container","text":"A Docker container is a runnable instance of an image. One can run code inside a docker container having all requirements installed.","title":"Docker container"},{"location":"Docker/#docker-registry","text":"A Docker registry stores docker images. In other words, Docker registry for docker images is like GitHub for code.","title":"Docker registry"},{"location":"Docker/#high-level-philosophy","text":"We always want to separate things that don't need to run together in different containers (e.g., dev / prod cmamp , optimizer , im , oms , dev_tools ), along a logic of \"independently runnable / deployable directories\". The problem is that when we put too many dependencies in a single container, trying to simplify the release approach we start having huge containers that are difficult to deploy and are unstable in terms of building even using poetry . Each dir that can be \"deployed\" and run should have a devops dir to build / qa / release containers with all the needed dependencies Certain containers that need to be widely available to the team and deployed go through the release process and ECR Other containers that are lightweight and used only by one person (e.g., the infra container) can be built on the fly using docker compose / docker build .","title":"High level philosophy"},{"location":"Docker/#thin-client","text":"To bootstrap the system we use a \"thin client\" which installs in a virtual env the minimum set of packages to run (e.g., installs invoke , docker , etc). TODO(gp): Audit / make sure we can simplify the thin env","title":"Thin client"},{"location":"Docker/#amp-cmamp-container","text":"The dev version is used to develop The prod version can be used for deployment as shortcut to creating a smaller container with only the strictly needed dependencies","title":"amp / cmamp container"},{"location":"Docker/#prod-container","text":"In order to avoid shipping the monster cmamp dev / prod container, we want to start building smaller containers with only the dependencies that specific prod scripts need","title":"Prod container"},{"location":"Docker/#infra-container","text":"To run infra script, if we only need boto3 and moto , we can create a Python library create a script interface create an invoke task that calls i docker_cmd --cmd ... reusing the cmamp container, (since that container already has boto3 and moto that are dependencies we can't remove) This approach is similar to calling the linter If we think we need to add new packages only for running infra scripts then we will create a new infra container. We can build on the fly and not release through ECR We can start with approach 1, which will also allow us to transition to 2 transparently, if needed","title":"Infra container"},{"location":"Docker/#relevant-bugs","text":"https://github.com/cryptokaizen/cmamp/issues/1060 Tool to extract the dependency from a project #1038 Create tool for poetry debugging #1026 Fix tests that fail due to pandas update and release cmamp image #1002","title":"Relevant bugs"},{"location":"Docker/#poetry","text":"Poetry is a tool for managing Python packages and dependencies: List packages you want to install with some constraints, e.g., pandas must be above 1.0 in devops/docker_build/pyproject.toml Given a list of packages you need to install to get the desired environment, you want poetry to \"optimize\" the packages and generate devops/docker_build/poetry.lock , which contains the list of versions of the packages to install If there is a new version of a package re-running poetry might give you an updated list of packages to install","title":"Poetry"},{"location":"Docker/#build-a-docker-image","text":"","title":"Build a Docker image"},{"location":"Docker/#general","text":"A docker image is built from a Dockerfile . The image is then used to run a Docker container. There is /devops dir under a project's dir that contains Docker-related files, e.g. cmamp/devops .","title":"General"},{"location":"Docker/#dockerfile","text":"A Dockerfile is a text document that contains all the commands to call on the command line to assemble an image. E.g. cmamp/devops/docker_build/dev.Dockerfile .","title":"Dockerfile"},{"location":"Docker/#base-image","text":"A Dockerfile should start with specifying a base image. Base image is an image that a new image is built from. A new Docker image will have all the packages/dependencies that are installed in the base image. Use FROM statement to specify a base image, e.g. FROM ubuntu:20.4","title":"Base image"},{"location":"Docker/#copy-files","text":"Copy files that are required to build a Docker image to the Docker filesystem. To copy a file from /source_dir (your filesystem) to /dst_dir (Docker filesystem) do: COPY source_dir/file dst_dir E.g., the command below will copy install_packages.sh from devops/docker_build to the Docker's root directory so that install_packages.sh can be accessed by Docker. COPY devops/docker_build/install_packages.sh .","title":"Copy files"},{"location":"Docker/#install-os-packages","text":"Install OS packages that are needed for a Docker app, but that are not installed for a base image. Use RUN instruction to install a package, e.g. RUN apt-get install postgresql-client Alternatively you can package all installation instructions in a .sh file and run it. Do not forget to copy a .sh file to the Docker filesystem so that Docker can see it. E.g., COPY devops/docker_build/install_packages.sh . RUN /bin/sh -c \"./install_packages.sh\"","title":"Install OS packages"},{"location":"Docker/#install-python-packages","text":"We prefer to install Python packages with poetry . Make sure that there is instruction to install pip3 and poetry . You can either put it in a Dockerfile or in a separate file like install_packages.sh . RUN apt-get install python3-pip RUN pip3 install poetry Copy poetry-related files to the Docker filesystem so that files can be accessed by Docker COPY devops/docker_build/poetry.toml COPY devops/docker_build/poetry.lock Update Python packages RUN poetry install","title":"Install Python packages"},{"location":"Docker/#build-an-image-from-a-dockerfile","text":"To build an image from a Dockerfile run: ``` docker build . ``` The Dockerfile must be called Dockerfile and located in the root of the context. You can point to any Dockerfile by using -f : ``` docker build -f /path_to_dockerfile/dockerfile_name ```","title":"Build an image from a Dockerfile"},{"location":"Docker/#run-multi-container-docker-application","text":"Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application's services.","title":"Run multi-container Docker application"},{"location":"Docker/#version","text":"At the beginning of a docker-compose.yaml file specify the docker-compose version. We use the version 3.0 , for more information see the official documents . version: \"3.0\"","title":"Version"},{"location":"Docker/#images","text":"You can either re-use a public image or build a new one from a Dockerfile . The app service below uses the image that is built from the dev.Dockerfile . app: build: context: . dockerfile: dev.Dockerfile The im_postgres_local service below uses the public postgres image pulled from the Docker hub registry . im_postgres_local: image: postgres: 13","title":"Images"},{"location":"Docker/#bind-mount","text":"If you want to be able to access code inside a Docker container, you should bind-mount a directory with the code on the host. Mount a directory on the host inside a Docker container, e.g. mount current directory to /app dir inside a Docker container: ``` app: volumes: .:/app ```","title":"Bind mount"},{"location":"Docker/#environment-variables","text":"You can either use variables directly from the environment or pass them in a docker-compose.yaml file. It is supposed that POSTGRES_VERSION is already defined in the shell. db: image: \"postgres:${POSTGRES_VERSION}\" Set environment variable in a service's container ``` db: environment: POSTGRES_VERSION=13 image: \"postgres:${POSTGRES_VERSION}\" ``` Set environment variable with .env file ``` db: env_file: ./postgres_env.env image: \"postgres:${POSTGRES_VERSION}\" ``` File postgres_env.env ``` cat ./postgres_env.env POSTGRES_VERSION=13 ```","title":"Environment variables"},{"location":"Docker/#basic-commands","text":"To check more advanced usage, please see the official documentation. Build, (re)create, start, and attach to containers for a service. It is assumed that a docker-compose file has the name docker-compose.yaml and is located in the current dir. ``` docker-compose up ``` List containers ``` docker-compose ps ``` Stop containers created by down . ``` docker-compose down ```","title":"Basic commands"},{"location":"Docker/#how-to-test-a-package-in-a-docker-container","text":"> sudo /bin/bash -c \"(source /venv/bin/activate; pip install yfinance)\" > python -c \"import finance\"","title":"How to test a package in a Docker container"},{"location":"Docker/#hacky-approach-to-patch-up-a-container","text":"# After install create a new version of the container > docker commit d2916dd5f122 > 623860924167.dkr.ecr.eu-north-1.amazonaws.com/cmamp:dev_ccxtpr # Push to the repo > docker push 623860924167.dkr.ecr.eu-north-1.amazonaws.com/cmamp:dev_ccxtpro # Then you can push and pull on different machines > docker pull 623860924167.dkr.ecr.eu-north-1.amazonaws.com/cmamp:dev_ccxtpro # To use `docker_bash` you might need to retag it to match what the system expects > docker tag 623860924167.dkr.ecr.eu-north-1.amazonaws.com/cmamp:dev_ccxtpro","title":"Hacky approach to patch up a container"},{"location":"Docker/#how-to-release-a-docker-image","text":"All the invoke tasks to run the release flow are in //amp/helpers/lib_tasks.py . Depending on the type of changes sometimes one needs to rebuild only the prod image, other times one needs to rebuild also the dev image. E.g., If you change Docker build-related things (e.g., add a Python package), you need to rebuild the dev image and then the prod image from the dev image If you change the code for a production system, you need to create a new prod image We try to use the same flow, conventions, and code for all the containers (e.g., amp, cmamp, dev_tools, opt).","title":"How to release a Docker image"},{"location":"Docker/#multi-architecture-build","text":"To build multi-arch (e.g., x86 , arm ) docker image using docker_build_local_image we should use --multi-build flag To build for specific platforms specify the platform name: For x86 - linux/amd64 For arm - linux/arm64 ``` i docker_build_local_image --version --multi-build --platform ``` To build for both arm and x86 architectures: ``` i docker_build_local_image --version --multi-build --platform linux/amd64,linux/arm64 ``` Multi-arch images are built using docker buildx which do not generate any local image by default Images are pushed to the remote registry and pulled for testing and usage To tag the local image as dev and push it to the registry, use ``` i docker_tag_push_multi_build_local_image_as_dev --version ```","title":"Multi-architecture build"},{"location":"Docker/#stages","text":"A \"stage\" is a step (e.g., local, dev, prod) in our release workflow of Docker images, code, or infrastructure. To run a Docker container in a certain stage use the stage parameter E.g. i docker_bash --stage=\"local\" creates a bash session inside the local docker amp container","title":"Stages"},{"location":"Docker/#local","text":"A local image is used to develop and test an update to the Docker container, e.g. after updating a package, installing a new package, etc. Local images can only be accessed locally by a developer, i.e. the team members can not / should not use local images. In practice local images are like dev images but private to users and servers.","title":"Local"},{"location":"Docker/#dev","text":"A dev image is used by our team to develop our systems (e.g., to add new functionalities to the dev_tools code). Typically the source code is mounted through a bind mount in Docker so that one can change the code and execute it in Docker. The image is tested, blessed, and released so that users and CI can use it without worries. Once a dev image is pushed to the docker registry it can be pulled and used by the team members.","title":"Dev"},{"location":"Docker/#prod","text":"A prod image is used to run a system by final users. E.g., the linter inside dev_tools , some prod system inside Airflow. It is self-contained (it should have no dependencies) since it has everything required to run a system installed inside it, e.g., code (e.g., the linter), Python packages. It is typically created from the dev image by copying the released code inside the prod image.","title":"Prod"},{"location":"Docker/#overview-of-how-to-release-an-image","text":"The release flow consists of the following phases Make changes to the image E.g., add Python package Update the changelog Build a local image Run specific tests (e.g., make sure that the new packages are installed) Run unit tests Run QA tests Tag local image as dev image Push dev image to ECR If there is also an associated prod image Build prod image from dev image Run unit / QA tests Push prod image to ECR","title":"Overview of how to release an image"},{"location":"Docker/#how-to-add-a-python-package-to-docker-image","text":"To add a new Python package to a Docker image you need to update poetry files and release a new image: Add a new package to amp/devops/docker_build/pyproject.toml file to the [tool.poetry.dependencies] section E.g., to add pytest-timeout do: [tool.poetry.dependencies] ... pytest-timeout = \"*\" ... In general we use the latest version of a package ( * ) until the tests fail or the system stops working If the system fails, we freeze the version of the problematic packages to a known-good version to get the tests back to green until the problem is solved. We switch back to the latest version once the problem is fixed If you need to put a constraint on the package version, follow the official docs , and explain in a comment why this is needed making reference to GitHub issues To verify that package is installed correctly one can build a local image and update poetry ``` i docker_build_local_image --version {new version} --update-poetry ``` run a docker container based on the local image ``` i docker_bash --stage local --version {new version} ``` verify what package was installed with pip show {package name} , e.g., ``` pip show pytest-rerunfailures Name: pytest-rerunfailures Version: 10.2 Summary: pytest plugin to re-run tests to eliminate flaky failures ... Location: /venv/lib/python3.8/site-packages Requires: pytest, setuptools Required-by: ``` run regressions for the local image, i.e. ``` i run_fast_tests --stage local --version {new version} i run_slow_tests --stage local --version {new version} ``` Update the changelog describing the new version Send a PR with the updated poetry files and any other change needed to make the tests pass Release the new image. To do so follow the # Release a Docker image section, use --update-poetry flag to resolve the dependencies","title":"How to add a Python package to Docker image"},{"location":"Docker/#how-to-find-unused-packages","text":"While installing Python packages we need to make sure that we do not install packages that we do not use","title":"How to find unused packages"},{"location":"Docker/#import-based-approach-using-pipreqs","text":"","title":"Import-based approach using pipreqs"},{"location":"Docker/#how-it-works","text":"To do so we use an import-based approach provided by pipreqs . Under the hood it uses the regex below and os.walk for selected dir: REGEXP = [ re.compile(r'^import (.+)$'), re.compile(r'^from ((?!\\.+).*?) import (?:.*)$') ]","title":"How it works"},{"location":"Docker/#limitations","text":"Not all packages that we use are necessarily imported, e.g. awscli , jupyter , pytest-cov , etc. -> pipreqs won't find these packages The import name is not always equal to the package actual name, see the mapping here","title":"Limitations"},{"location":"Docker/#usage","text":"See the official docs for the advanced usage. Run a bash session inside a Docker container Install pipreqs with sudo pip install pipreqs We install it temporary within a Docker bash session in order to introduce another dependency You need to re-install pipreqs everytime you create a new Docker bash session To run for a root dir do: pipreqs . --savepath ./tmp.requirements.txt The command above will generate ./tmp.requirements.txt with the list of the imported packages, e.g., amp==1.1.4 async_solipsism==0.3 beautifulsoup4==4.11.1 botocore==1.24.37 cvxopt==1.3.0 cvxpy==1.2.0 dill==0.3.4 environs==9.5.0 ... You can grep for a package name to see where it is used, e.g., ``` jackpy \"dill\" helpers/hpickle.py:108: import dill ... ```","title":"Usage"},{"location":"Docker/#how-to-build-a-local-image","text":"The recipe to build a local image is in devops/docker_build/dev.Dockerfile . This launches various scripts to install: OS Python venv + Python packages jupyter extensions application-specific packages (e.g., for the linter) To build a local image run: ``` i docker_build_local_image --version 1.0.0 # Build from scratch and not incrementally. i docker_build_local_image --version 1.0.0 --no-cache # Update poetry package list. i docker_build_local_image --version 1.0.0 --update-poetry # Update poetry package list and build from scratch. i docker_build_local_image --version 1.0.0 --update-poetry --no-cache # See more options: i docker_build_local_image -h ``` Once an image is built, it is tagged as local-${user}-${version} , e.g., local-saggese-1.0.0 ``` Successfully tagged 665840871993.dkr.ecr.us-east-1.amazonaws.com/amp:local-gsaggese-1.0.9 docker image ls 665840871993.dkr.ecr.us-east-1.amazonaws.com/amp:local-gsaggese-1.0.9 REPOSITORY TAG IMAGE ID CREATED SIZE 665840871993.dkr.ecr.us-east-1.amazonaws.com/amp local-gsaggese-1.0.9 cf16e3e3d1c7 Less than a second ago 2.75GB ``` A local image is a candidate for becoming a dev image. ``` i run_fast_tests --stage local --version 1.0.0 ```","title":"How to build a local image"},{"location":"Docker/#testing-the-local-image","text":"Testing the local image ``` i docker_bash pip list | tee pip_packages.dev.txt i docker_cmd --cmd \"pip list | tee pip_packages.dev.txt\" i docker_bash --stage local --version 1.0.9 pip list | tee pip_packages.local.txt ``` or in one command: ``` i docker_cmd --cmd \"pip list | tee pip_packages.dev.txt\"; i docker_cmd --stage=local --version=1.0.9 --cmd \"pip list | tee pip_packages.local.txt\" vimdiff pip_packages.dev.txt pip_packages.local.txt ``` You can move the local image on different servers for testing by pushing it on ECR: ``` i docker_login i docker push 665840871993.dkr.ecr.us-east-1.amazonaws.com/amp:local-gsaggese-1.1.0 ```","title":"Testing the local image"},{"location":"Docker/#tag-local-image-as-dev","text":"Docker tag is just a way of referring to an image. A good analogy is how Git tags refer to a particular commit in your history. Basically, tagging is creating a reference from one image ( local-saggese-1.0.0 ) to another ( dev ) Once the local image is tagged as dev , your dev image becomes equal to local-saggese-1.0.0 dev image is also tagged with dev-${version} , e.g., dev-1.0.0 to preserve history and allow for quick rollback. Locally in git repository a git tag ${repo_name}-${version} , e.g. cmamp-1.0.0 is created in order to properly control sync between code and container.","title":"Tag local image as dev"},{"location":"Docker/#push-image","text":"To push dev or prod image means to send it to the docker registry. It is more like pushing a commit to the GitHub Once an image is pushed, it can be used by the team members by running i docker_pull Local git tag ${repo_name}-${version} , e.g. cmamp-1.0.0 , is pushed at this stage to the remote repository to allow others to properly control sync between code and container. To be able to push an image to the ECR one should have permissions to do so","title":"Push image"},{"location":"Docker/#end-to-end-flow-for-dev-image","text":"Conceptually the flow consists of the following phases: Build a local image of docker i docker_build_local_image --version 1.0.0 Run fast tests to verify that nothing is broken i run_fast_tests --stage local --version 1.0.0 Run end-to-end tests by, e.g., running linter on some file i lint --files helpers/tasks.py --stage local --version 1.0.0 Tag local image as dev i docker_tag_local_image_as_dev --version 1.0.0 Push dev image to the docker registry i docker_push_dev_image --version 1.0.0 The mentioned flow is executed by Build dev image GH action and that is a preferred way to do an image release. For specific cases that can not be done via GH action see commands below: ``` # To run the official flow end-to-end: i docker_release_dev_image --version 1.0.0 # To see the options: i docker_release_dev_image -h # Run from scratch and not incrementally: i docker_release_dev_image --version 1.0.0 --no-cache # Force an update to poetry to pick up new packages i docker_release_dev_image --version 1.0.0 --update-poetry # Skip running the QA tests i docker_release_dev_image --version 1.0.0 --no-qa-tests # Skip running the tests i docker_release_dev_image --version 1.0.0 --skip-tests # Skip end-to-end tests i docker_release_dev_image --version 1.0.0 --no-run-end-to-end-tests ```","title":"End-to-end flow for dev image"},{"location":"Docker/#build-prod-image","text":"The recipe to build a prod image is in dev_tools/devops/docker_build/prod.Dockerfile . The main difference between dev image and prod image is that source code is accessed through a bind mount for dev image (so that it can be easily modified) and copied inside the image for a prod image (since we want to package the code) requirements to be installed are different: dev image requires packages to develop and run the code prod image requires packages only to run the code To build the prod image run: ``` i docker_build_prod_image --version 1.0.0 # Check the options: i docker_build_prod_image -h # To build from scratch and not incrementally: i docker_build_prod_image --version 1.0.0 --no-cache ``` To run a command inside the prod image ``` docker run --rm -t --user $(id -u):$(id -g) --workdir=/app 665840871993.dkr.ecr.us-east-1.amazonaws.com/cmamp:prod-1.0.3 \"ls -l /app\" ``` Example of a complex command: ``` docker run --rm -t --workdir=/app 665840871993.dkr.ecr.us-east-1.amazonaws.com/cmamp:prod-1.0.3 \"python /app/im_v2/ccxt/data/extract/download_realtime.py --to_datetime '20211204-194432' --from_datetime '20211204-193932' --dst_dir 'test/ccxt_test' --data_type 'ohlcv' --api_keys 'API_keys.json' --universe 'v03'\" ```","title":"Build prod image"},{"location":"Docker/#qa-for-prod-image","text":"In dev_scripts repo test: ``` i lint --files \"linters/amp_black.py\" ``` In amp repo make sure: ``` i lint -f \"helpers/dbg.py\" ```","title":"QA for prod image"},{"location":"Docker/#end-to-end-flow-for-prod-image","text":"Build docker prod image i docker_build_prod_image --version 1.0.0 Run fast tests to verify that nothing is broken i run_fast_tests Push prod image to the docker registry i docker_push_prod_image --version 1.0.0 To run the flow end-to-end do: ``` i docker_release_prod_image --version 1.0.0 ``` same options are available as for i docker_release_dev_image check options i docker_release_prod_image -h","title":"End-to-end flow for prod image"},{"location":"Docker/#flow-for-both-dev-and-prod-images","text":"To run both flows end-to-end do: i docker_release_all Alternatively, one can run the release stages step-by-step.","title":"Flow for both dev and prod images"},{"location":"Docker/#docker-in-docker-dind","text":"It is possible to install a Docker engine inside a Docker container so that one can run Docker container (e.g., OMS or IM) inside an isolated amp container. The problems with this approach are: dind requires to run the external container in privileged mode, which might not be possible due to security concerns the Docker / build cache is not shared across parent and children containers, so one needs to pull / build an image every time the outermost container is restarted An alternative approach is the \"sibling container\" approach","title":"Docker-in-docker (dind)"},{"location":"Docker/#sibling-container-approach","text":"Refs: Can I run Docker-in-Docker without using the --privileged flag - Stack Overflow https://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/ Often what's really needed is the ability to build / run a container from another container (e.g., CI or unit test). This can be achieved by mounting the Docker socket /var/run/docker.sock to the container, so that a container can talk to Docker Engine. This approach allows reuse of the build cache across the sibling containers. The downside is less isolation from the external container, e.g., spawned containers can be left hanging or can collide. E.g., `` # Run docker ps` in a container, showing the containers running in the main container docker run -ti --rm \\ -v /var/run/docker.sock:/var/run/docker.sock \\ dindtest \\ docker ps # Start a sibling hello world container: docker run -it --rm \\ -v /var/run/docker.sock:/var/run/docker.sock \\ dindtest \\ docker run -ti --rm hello-world ```","title":"Sibling container approach"},{"location":"Docker/#connecting-to-postgres-instance-using-sibling-containers","text":"We can start the Docker container with Postgres as a service from outside the container. ``` (cd oms; i oms_docker_up -s local) INFO: > cmd='/local/home/gsaggese/src/venv/amp.client_venv/bin/invoke oms_docker_up -s local' report_memory_usage=False report_cpu_usage=False docker-compose \\ --file /local/home/gsaggese/src/sasm-lime4/amp/oms/devops/compose/docker-compose.yml \\ --env-file /local/home/gsaggese/src/sasm-lime4/amp/oms/devops/env/local.oms_db_config.env \\ up \\ oms_postgres Creating compose_oms_postgres_1 ... done Attaching to compose_oms_postgres_1 oms_postgres_1 | oms_postgres_1 | PostgreSQL Database directory appears to contain a database; Skipping initialization oms_postgres_1 | oms_postgres_1 | 2022-05-19 22:57:15.659 UTC [1] LOG: starting PostgreSQL 13.5 (Debian 13.5-1.pgdg110+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit oms_postgres_1 | 2022-05-19 22:57:15.659 UTC [1] LOG: listening on IPv4 address \"0.0.0.0\", port 5432 oms_postgres_1 | 2022-05-19 22:57:15.659 UTC [1] LOG: listening on IPv6 address \"::\", port 5432 oms_postgres_1 | 2022-05-19 22:57:15.663 UTC [1] LOG: listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\" oms_postgres_1 | 2022-05-19 22:57:15.670 UTC [25] LOG: database system was shut down at 2022-05-19 22:56:50 UTC oms_postgres_1 | 2022-05-19 22:57:15.674 UTC [1] LOG: database system is ready to accept connections ``` Note that Postgres needs to be Start a container able to From inside a container I launch postgres through the /var/... ``` docker ps | grep postgres CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 83bba0818c74 postgres:13 \"docker-entrypoint.s...\" 6 minutes ago Up 6 minutes 0.0.0.0:5432->5432/tcp compose-oms_postgres-1 ``` Test connection to the DB from outside the container ``` psql --host=cf-spm-dev4 --port=5432 --user aljsdalsd -d oms_postgres_db_local Password for user aljsdalsd: psql (9.5.25, server 13.5 (Debian 13.5-1.pgdg110+1)) WARNING: psql major version 9.5, server major version 13. Some psql features might not work. Type \"help\" for help. oms_postgres_db_local=# ``` Test connection to the DB from inside the container ``` psql --host=cf-spm-dev4 --port=5432 --user aljsdalsd -d oms_postgres_db_local ... ```","title":"Connecting to Postgres instance using sibling containers"},{"location":"Docker/#release-flow","text":"","title":"Release flow"},{"location":"Docker/#cmamp","text":"File an Issue for the release (e.g., \"Add package foobar to cmamp image\") Create the corresponding branch with i git_create_branch -i ${issue_number} Change the code Update the changelog, i.e. //cmamp/changelog.txt Specify what was changed Pick the release version accordingly We use semantic versioning convention For example, adding a package to the image would mean bumping up version 1.0.0 to 1.0.1 Test the change using the local release flow i docker_build_local_image -v ${version} If a new package is added run docker_build_local_image with --update-poetry option and check in a poetry.lock file Make sure that the tests pass i run_fast_slow_tests -s local -v ${version} , and that the goal of the Issue is achieved (e.g., a new package is visible, the package version has been updated) Do a PR with the change including the updated changelog.txt , the poetry files (both the specs devops/docker_build/poetry.toml and the package version devops/docker_build/poetry.lock ) Run the release flow manually (or rely on GH Action build workflow to create the new image) ``` # Release dev image i docker_release_dev_image --version $version # Pick up the new image from ECR i docker_pull ``` Send a message on the all@ chat telling people that a new version of the XYZ container has been released Users need to do a i docker_pull to get the new container Users that don't update should see a message telling them that the code and container are not in sync any more, e.g.,: ``` This code is not in sync with the container: code_version='1.0.3' != container_version='amp-1.0.3' You need to: - merge origin/master into your branch with invoke git_merge_master - pull the latest container with invoke docker_pull ```","title":"cmamp"},{"location":"Docker/#dev_tools","text":"File an Issue for the release Create the corresponding branch in dev_tools Change the code Run the release flow end-to-end ``` i docker_release_dev_image --version 1.1.0 i docker_release_prod_image --version 1.1.0 ``` Update the changelog, i.e. //dev_tools/changelog.txt The changelog should be updated only after the image is released; otherwise the sanity checks will assert that the release's version is not higher than the latest version recorded in the changelog. Specify what has changed Pick the release version accordingly NB! The release version should consist of 3 digits, e.g. \"1.1.0\" instead of \"1.1\" We use semantic versioning convention For example, adding a package to the image would mean bumping up version 1.0.0 to 1.0.1 Do a PR with the change including the updated changelog.txt Send a message on the all@ chat telling people that a new version of the container has been released Users need to do i docker_pull from dev_tools , i docker_pull_dev_tools from cmamp Users need to make sure to pull docker after the master is up-to-date (including amp submodules)","title":"dev_tools"},{"location":"Docker/#design-release-flow-discussion","text":"TODO(gp, Vitalii): Turn this into a description of the release flow Let's assume that we want to release dev image with version 1.2.3: > i docker_build_local_image --tag-name 1.2.3 Initially we thought about using Git tags to mark releases points in the source repo for dev and prod releases (but not local since local is reserved to private use by a user). This approach is elegant, but it has some corner cases when used with containers for multiple repos that contain Git submodules. We decided to use an approach where a changelog.txt file contains the latest code version all test tasks now also use hversion.get_code_version() that calls hgit.git_describe() to get latest tag in the repo (1.0.0 in this case) Agree. git_describe will need to accept a dir to find the tag of the releasable dir when we are satisfied with local image, we run i docker_tag_local_image_as_dev We will still need to pass --version 1.0.0 invoke internally tags local-1.0.0 as dev-1.0.0 , in addition to dev Both for Git tags and docker tags then we run i docker_push_dev_image We will still need to pass --version 1.0.0 invoke internally pushes both dev-1.0.0 and dev images to ECR **AND** pushes local 1.0.0 git tag to remote git repo (github) docker_release_dev_image will do basically the same (will require tag_name). Of course docker_release... is just a convenience wrapper running all the stages Now let's assume we want to promote dev image to prod: then we run i docker_build_prod_image invoke internally checks with hversion.get_code_version() and builds prod-1.0.0 based on dev-1.0.0 , also tagging prod-1.0.0 as prod then we run i docker_push_prod_image invoke pushes prod-1.0.0 and prod tags to ECR docker_release_prod_image will do basically the same (will require tag_name). Q0: Is the flow ok? Yes Q1: The flow is the same for dev_tools and cmamp , but to update the version of image on which dev_tools is based -- we'll need to modify Dockerfile now. Is that ok? Maybe we should just create the dev_tools from scratch using the full-blown flow instead of build on top of it The idea of building on top of it, was just a shortcut but it is creating more problems that what it's worth it Then everything looks and behaves the same TODO(vitalii): File a bug, if we don't have it yet Q2: If the flow is run in the submodule, e.g. in amp dir, currently the behaviour is not well defined. Commands will try to build cmamp image in this case, but code version will be from dev_tools -- should we fix this? We are going towards the concept of \"releasable dirs\" (see im, optimizer). If there is a dir with devops, then that dir runs inside a container The \"Git version\" should be associated to the dir we are releasing (e.g., cmamp, im, optimizer, dev_tools) Vitalii: If we will have monorepo with releasable dirs, then indeed git tags are not that comfortable to use, however I could argue that when one releases im image with version 1.0.0, he gets docker image im:dev-1.0.0 , im:prod-1.0.0 , im:dev and im:prod -- but how then one is able to find corresponding code that was used in that image? Perhaps instead, we could share namespace of git tags between all tags. E.g. in git repo (github) we will have: im-dev-1.0.0 cmamp-dev-1.0.0 im-prod-1.0.0 GP: Point taken. In fact the code in a releasable dir still needs code from other submodules (e.g., helpers). One approach is to put the Git hash in version.txt. The one you suggest (of tagging the entire repo) with also info on the dir makes sense. I think the Git tags are designed to do what we want, so let's use them. Q3: We don't need version.txt file in this flow. I will remove it, ok? Yes, we can remove version.txt and use a README or changelog in the releasable dir The flow is similar to what I thought. Some observations / questions: INV: version becomes mandatory in the release flow This requires a lot of cosmetic changes to the code since now it's optional, but it's worth make the changes We need to ensure that version can only be created going fwd. We can do a comparison of the current version with the new version as tuples (we could use semver but it feels not needed) The workflows are: Build a local image Release a dev image Release a prod image Rollback an image We rarely move the dev / prod tag back, but rather users needs to docker pull an older image and pass --base name --stage and --version to docker {bash, cmd, jupyter} Then the image is fixed going forward A releasable dir has a repo_config Maybe we should call it component_config since now also dirs can be released README.md or changelog.md devops tasks.py (with the exposed Invoke tasks) lib_tasks.py (with the custom invoke tasks) We want to try to move to helpers/lib_tasks all the \"common\" code without dependencies from the specific sw components. We pass function pointers for callbacks. What to do with: CONTAINER_VERSION='amp-1.1.1' BUILD_TAG='amp-1.1.1-20211114_093142-AmpTask1845_Get_docker_in_docker_to_work-47fb46513f084b8f3c9008a2e623ec05040a10e9'","title":"Design release flow - discussion"},{"location":"Docker/#qa-flow","text":"The goal is to test that the container as a whole works We want to run the container as a user would do Usually we run tests inside a container to verify that the code is correct To test the container itself right now we test outside (in the thin client) > pytest -m qa test --image_stage dev The problem is that now the thin client needs to have a bunch of deps (including pytest, pandas and so on) which defeats the purpose of the thin env dev_scripts_devto/client_setup/ E.g., //amp/dev_scripts/client_setup/requirements.txt A hack is to vimdiff /Users/saggese/src/lemonade2/amp/dev_scripts/client_setup/requirements.txt dev_scripts_devto/client_setup/requirements.txt > source dev_scripts_devto/client_setup/build.sh A possible solution is to use Docker-in-Docker in this way we don't have to pollute the thin env with a bunch of stuff Talk to Grisha and Vitalii This works in dev_tools because the code for the import detector is there and we are using a dev container which binds the src dir to the container > i lint_detect_cycles --dir-name import_check/test/Test_detect_import_cycles.test1/input/ --stage dev In all the other repos, one needs to use the prod of dev_tools container (that's what the user would do) Next steps: TODO(Sonya + Grisha): release the prod dev_toools container as it is TODO(Sonya + Grisha): document dev_tools, release procedure TODO(Sonya): pull prod dev_tools (i docker_pull_dev_tools) and test that now in cmamp the tool works TODO(gp): figure out the QA workflow (and improve the thin client with dind) To break the circular dep we release a prod-candidate","title":"QA flow"},{"location":"Docker/#dev_tools-container","text":"For specific dev_tools workflows see","title":"Dev_tools container"},{"location":"Docker/#optimizer-container","text":"","title":"Optimizer container"},{"location":"Docker/#rationale","text":"The high-level goal is to move towards containerized Python scripts running in smaller containers instead of keep adding packages to amp / cmamp , which makes the amp / cmamp container bloated and risky to build Along this design philosophy similar to microservices, we want to have a Docker container, called opt with a Python script that uses some packages that are not compatible with amp (specifically cvxopt, cvxpy) This is similar to what we do for the dev_tools , which is like a containerized Python script for the linter","title":"Rationale"},{"location":"Docker/#build-and-run-a-local-version-of-opt","text":"You can build the container locally with: ``` cd optimizer i opt_docker_build_local_image --version 0.1.0 ``` This process takes around 5 mins and then you should have the container docker image ls 665840871993.dkr.ecr.us-east-1.amazonaws.com/opt:local-saggese-0.1.0 REPOSITORY TAG IMAGE ID CREATED SIZE 665840871993.dkr.ecr.us-east-1.amazonaws.com/opt local-saggese-0.1.0 bb7d60d6a7d0 7 seconds ago 1.23GB Run the container as: ``` i opt_docker_bash --stage local --version 0.1.0 ``` To run a Jupyter notebook in the opt container:","title":"Build and run a local version of opt"},{"location":"Docker/#internals","text":"","title":"Internals"},{"location":"Docker/#one-container-per-git-repo","text":"A simple approach is to have each deployable unit (i.e., container) corresponding to a Git repo The consequence would be: a multiplication of repos no implicit sharing of code across different containers some mechanism to share code (e.g., helpers ) across repos (e.g., using bind mount) not playing nice with Git subrepo mechanism since Docker needs to see the entire repo So the code would be organized in 4 repos: - lemonade / lime - helpers - optimizer - oms - models in amp where the dependency between containers are lemonade -> amp amp -> optimizer, helpers optimizer -> helpers, core","title":"One container per Git repo"},{"location":"Docker/#multiple-containers-per-git-repo","text":"Another approach is to have optimizer as a directory inside amp This keeps amp and optimizer in a single repo To build / run optimizer code in its container one needs to cd in the dir The problem then becomes how to share helpers","title":"Multiple containers per Git repo"},{"location":"Docker/#mounting-only-optimizer-dir-inside-docker","text":"From devops/compose/docker-compose.yml 42 volumes: 43 # Move one dir up to include the entire git repo (see AmpTask1017). 44 - ../../:/app 45 # Move one dir down to include the entire git repo (see AmpTask1017). 46 working_dir: /app From devops/docker_build/dev.Dockerfile ENTRYPOINT [\"devops/docker_run/entrypoint.sh\"] The problem is that Git repo doesn't work anymore git --version: git version 2.30.2 fatal: not a git repository (or any parent up to mount point /) Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set). A work around is to inject .git in /git of the container and then point git to that ``` environment: ... - GIT_DIR=/git volumes: # Move one dir up to include the entire git repo (see AmpTask1017). - ../../:/app - ../../../../.git:/git - ../../../../amp/helpers:/app/helpers ``` Git works but it gets confused with the paths modified: .dockerignore deleted: .github/gh_requirements.txt deleted: .github/workflows/build_image.yml.DISABLED deleted: .github/workflows/fast_tests.yml deleted: .github/workflows/linter.yml.DISABLED deleted: .github/workflows/slow_tests.yml deleted: .github/workflows/superslow_tests.yml.DISABLED deleted: .gitignore","title":"Mounting only optimizer dir inside Docker"},{"location":"Docker/#mounting-the-supermodule-eg-lime-lemonade-amp-inside-docker","text":"From devops/compose/docker-compose.yml 42 volumes: 43 # Move one dir up to include the entire git repo (see AmpTask1017). 44 - ../../../:/app 45 # Move one dir down to include the entire git repo (see AmpTask1017). 46 working_dir: /app/amp From devops/docker_build/dev.Dockerfile ENTRYPOINT [\"optimizer/devops/docker_run/entrypoint.sh\"] This approach mounts 4 dirs up from devops/compose/docker-compose.yml, i.e., //lime The problem with this approach is that now repo_config.py is incorrect i opt_docker_build_local_image --version 0.4.0 32 - ../../../helpers:/app/amp/optimizer/helpers 33 34 # Shared cache. This is specific of lime. 35 - /local/home/share/cache:/cache 36 37 # Mount `amp` when it is used as submodule. In this case we need to 38 # mount the super project in the container (to make git work with the 39 # supermodule) and then change dir to `amp`. 40 app: 41 extends: 42 base_app 43 volumes: 44 # Move one dir up to include the entire git repo (see AmpTask1017). 45 - ../../../../:/app 46 # Move one dir down to include the entire git repo (see AmpTask1017). 47 working_dir: /app/amp/optimizer 48 #entrypoint: /bin/bash -c \"ls helpers\"","title":"Mounting the supermodule (e.g., lime, lemonade, amp) inside Docker"},{"location":"Docker/#invariants","text":"A deployable dir is a dir under a Git repo It corresponds to a software component (code + library = Docker container) Anything that has a devops dir is \"deployable\" Each Docker container is run from its corresponding dir, e.g., amp container from the amp dir amp container from the lemonade dir (this is just a shortcut since lemonade has the same deps right now as amp) Always mount the outermost Git repo under /app Set the Docker working dir as the current dir Each deployable dir specifies all the needed information in repo_config.py (which is the one in the current dir) What container to run What functionality is supported on different servers (e.g., privileged way) The changelog.txt file is in the deployable dir (e.g., optimizer/changelog.txt) Each One run the invoke commands from optimizer dir When the Docker container starts the current dir is optimizer helpers, core is mounted in the same dir You can't see code outside optimizer TODO(gp): running in amp under lemonade should use the local repo_config","title":"Invariants"},{"location":"Docker/#release-and-ecr-flow","text":"TODO(gp): Implement this","title":"Release and ECR flow"},{"location":"Docker/#unit-testing-code-inside-opt-container","text":"Since we want to segregate the package dependencies in different containers, tests that have a dependency from cvxopt /cvxpy can't be run inside the amp container but need to be run inside opt . We want to: (as always) write and run unit tests for the optimizer code in isolation, i.e., test the code in the directory optimizer by itself run all the tests for the entire repo (relying on both containers amp and optimizer with a single command invocation) be able to run tests belonging to only one of the containers to shorten the debugging cycle To achieve this we need to solve the 3 problems below.","title":"Unit testing code inside opt container"},{"location":"Docker/#avoid-compiling-code-depending-from-cvxopt-when-running-amp","text":"We can't parse code (e.g., in pytest ) that includes packages that are not present in a container E.g., pytest running in amp should not parse code in //amp/optimizer since it contains imports that will fail Solution 1 We use the pytest mechanism cvx = pytest.importorskip(\"cvxpy\") which is conceptually equivalent to: ``` try: import cvxopt has_cvxopt = True except ImportError: has_cvxopt = False if has_cvxopt: def utils1(): cvxopt\u2026 ``` Solution 2 Test in eachfile for the existence of the needed packages and enclose the code in an if _has_package Pros: We can skip code based dynamically on a try ... except ImportModule to check what packages are present Cons: Repeat the same piece of try ... except in many places Solution: we can factor it out in a function We need to enclose the code in a if ... that screws up the indentation and makes the code weird Solution 3 Exclude certain directories (e.g., //amp/optimizer ) from pytest Pros: We don't have to spread the try ... except and if \\_has_package in the code Cons: The directory is relative to the top directory Solution: we can use a regex to specify the dir without the full path Which directories are included and excluded depends on where pytest is run E.g., running pytest in an amp container we need to skip the optimizer dir, while pytest in an optimizer container should skip everything but the optimizer dir Solution 4 Exclude certain directories or files based on which container we are running in Cons: We need to have a way to determine in which container we are running Solution: we can use the env vars we use for versioning ``` echo $AM_CONTAINER_VERSION amp-1.0.3- ``` Given the pros and cons, we decided to follow Solution 1 and Solution 3","title":"Avoid compiling code depending from cvxopt when running amp"},{"location":"Docker/#run-optimizer-tests-in-a-stand-alone-opt-container","text":"To run the optimizer tests, you can create an opt container and then run pytest ``` cd optimizer i opt_docker_bash docker> pytest . ``` We wrap this in an invoke target like i opt_run_fast_tests Alternative solution We can use dind to run the opt container inside a cmamp one Cons: dind complicates the system dind is not supported everywhere (one needs privileged containers) dind is slower since there are 2 levels of (relatively fast) virtualization","title":"Run optimizer tests in a stand-alone opt container"},{"location":"Docker/#run-optimizer-tests-as-part-of-running-unit-tests-for-cmamp","text":"We use the same mechanism as run_fast_slow_superslow_tests to pull together different test lists","title":"Run optimizer tests as part of running unit tests for cmamp"},{"location":"Docker/#call-a-dockerized-executable-from-a-container","text":"From https://github.com/cryptokaizen/cmamp/issues/1357 We need to call something from amp to opt Docker Solution 1 Inside the code we build the command line cmd = 'docker run -it ... '; system(cmd) Cons: there is code replicated between here and the invoke task (e.g., the info about the container, ...) Solution 2 Call the Dockerized executable using the docker_cmd invoke target cmd = \"invoke opt_docker_cmd -cmd '...'\" system(cmd) Pros: All the Docker commands go through the same interface inside invoke Cons Bash interpolation in the command Another level of indirection: do a system call to call invoke , invoke calls docker, docker does the work invoke needs to be installed inside the calling container Solution 3 Call opt_lib_tasks.py opt_docker_cmd(cmd, ...) Pros avoid doing a call to invoke can deal with bash interpolation in Python We should always use Solution 3, although in the code sometimes we use Solution 1 and 2 (but we should replace in favor of Solution 3).","title":"Call a Dockerized executable from a container"},{"location":"Docker/#_1","text":"The interface to the Dockerized optimizer is in run_optimizer in //amp/oms/call_optimizer.py To run the examples ``` cd //lime i docker_bash pytest ./amp/oms/test/test_call_optimizer.py::Test_run_dockerized_optimizer1 ```","title":""},{"location":"Documentation_about_guidelines/","text":"Documentation about guidelines Guidelines for describing workflows Markdown vs Google Docs In general Markdown pros Google Docs pros Rules of thumb Useful references Style and cosmetic lints Always use markdown linter Table of content (TOC) Use nice 80 columns formatting for txt files Empty line after heading Bullet lists Using code style Indenting code style Embedding screenshots Improve your written English Make sure your markdown looks good Google docs style conventions Headings Font Convert between Gdocs and Markdown Gdocs -> Markdown Using convert_docx_to_markdown.py Process Cleaning up converted markdown Other approaches Markdown -> Gdocs Guidelines for describing workflows Make no assumptions on the user's knowledge Nothing is obvious to somebody who doesn't know Add ways to verify if a described process worked E.g., \"do this and that, if this and that is correct should see this\" Have a trouble-shooting procedure One approach is to always start from scratch Markdown vs Google Docs In general We prefer to use Markdown for technical documentation We use Google for notes from meetings and research Markdown pros Can use vim Can version control Easy to use verbatim (e.g., typing foobar ) Easy to style using pandoc Easy to embed code Easy to add Latex equations Easy to grep Google Docs pros Easy to embed figures Easy to collaborate Easy to make quick changes (instead of making a commit) Easy to publish (just make them public with proper permissions) Styling https://webapps.stackexchange.com/questions/112275/define-special-inline-styles-in-google-docs Interesting add-ons: Enable Markdown Code blocks Use darcula, size 10 def hello(): print(\"hello\") Auto-latex equations Rules of thumb Use Markdown If doc is going to be used as a public guideline If doc has mostly text, code, and formulas If there are notes from a book Use Gdoc If doc requires a lot of images that cannot be placed as text If doc is a research of an analysis Useful references Markdown cheatsheet Google guide to Markdown TODO(gp): Make sure it's compatible with our linter Style and cosmetic lints Always use markdown linter Most cosmetic lints described further can be taken care automatically by our markdown linter, so make sure to run it after implementing the changes The file is dev_scripts/lint_md.sh , see the docstrings for more details Example run: dev_scripts/lint_md.sh docs/Documentation_about_guidelines.md Do not mix manual edits and linter runs. Best practice is to run the linter and commit the changes it made as separate commit If the linter messes up the text, file an issue with examples of what the linter does incorrectly Table of content (TOC) Unfortunately both markdown and GitHub don't support automatically generating a TOC for a document To generate a table of content: Add the following tag at the top of the markdown file below the document title: <!-- toc --> Run the markdown linter in order to build TOC automatically Use nice 80 columns formatting for txt files Vim has a :gq command to reflow the comments There are plugins to take care of this for PyCharm Our markdown linter takes care of reflowing the comments as well Empty line after heading Leave an empty line after a heading to make it more visible Bad ``` # Coming through! I've big important things to do! ... and his big important wheels got STUCK! ``` Good ``` Very important title Less important text ``` Bullet lists We like using bullet list since one can represent thought process more clearly, e.g., This is thought #1 This is related to thought #1 This is thought #2 Well, that was cool! But this is even better We strictly use - instead of * , circles, etc.: Bad ``` Foo bar! hello world Baz ``` Good ``` Foo bar! hello world Baz ``` Using code style We use code style for Code Dirs (e.g., /home/users ) Command lines (e.g., git push ) When using a block of code use the write syntax highlighting Bash > git push Python python if __name__ == \"__main__\": predict_the_future() print(\"done!\") Indenting code style GitHub / pandoc seems to render incorrectly a code block unless it's indented over the previous line Bad ``` git push ``` Good > git push Embedding screenshots Avoid to use screenshots whenever possible! However, sometimes we need it (e.g., plot infographics, website inteface, etc.) To do it correctly: Place your screenshot in any comment window at GitHub This will upload the image to the GitHub cloud You DO NOT have to publish a comment, the provided link is already ready to use! Make sure your link has no not-English symbols in alt section They sometimes appear if your native PC language is not English You can avoid it by giving the picture a name in English Alternatively, you can just edit the alt section in the generated link - this will not corrupt the file Place the generated and edited link to the markdown file Improve your written English Use English spell-checker Unfortunately this is not enough Type somewhere where you can use several free choices: Grammarly LanguageTool Or other proofreading and copy-paste This is super-useful to improve your English since you see the error and the correction Otherwise you will keep making the same mistakes forever Make sure your markdown looks good You can: Check in a branch and use GitHub to render it Use Pycharm to edit, which also renders it side-by-side Compare your markdown with already published documentation Google docs style conventions Headings We add N (where N is the heading level) # before the heading name, e.g., Heading 1: # Heading 1 Heading 2: ## Heading 2 The reason is that sometimes one doesn't have the time or the patience to format things properly, so at least there is some indication of the level of the titles Avoid having multiple # separatd by a space that sometimes appear in a process of convertion of Gdocs to Markdown files Bad # # Heading 1 Good # Heading 1 Font Normal text: Font: Arial Font size: 11 Headings: Font: Arial Style: bold Font size: should be adjusted automatically when one converts \u201cNormal text\u201d to \u201cHeading N\u201d, e.g., when converting some text of size 11 to \u201cHeading 1\u201d the font sizes becomes 20 Convert between Gdocs and Markdown Gdocs -> Markdown Using convert_docx_to_markdown.py In general, we recommend using this approach This python script converts Docx to Markdown using Pandoc. Pros Removing artifacts with this python script, less manual work Best for a large document Handle figures Cons Need to move files Process Download Google document as docx Convert it to markdown using convert_docx_to_markdown.py Usage: ``` ../dev_scripts/convert_docx_to_markdown.py --docx_file Tools_Docker.docx --md_file Tools_Docker.md ``` This command should be run directly under the target output directory for the Markdown file, in order to generate correct image links. Otherwise, you'll need to manually fix the image links. File names can't contain any spaces. Therefore, use underscores _ to replace any spaces. Cleaning up converted markdown Fix some formatting manually before running the Markdown linter. Read through Style and cosmetic lints for Markdown formatting and fix some formatting based on the rules. Summary Add the following tag at the top of the markdown file below the document title: <!-- toc --> Use bullet lists to organize the whole Markdown for consistency with other docs. See Coding_Style_Guide.md or any other published Markdown format as reference Add missing ``` around code blocks. These could be missing in the original Google doc. Also adjust code block indentations if needed Remove empty lines manually :'<,'>! perl -ne 'print if /\\S/' Run the lint_md.sh Usage: > dev_scripts/lint_md.sh docs/Documentation_about_guidelines.md What the linter will do: Build TOC automatically Adjust the indentation to improve the Markdown's format (but the precondition is that you have properly adjusted the indentation levels). Remove extra empty lines under headings Adjust text layout Do not mix manual edits and linter runs If the linter messes up the text File bugs in amp with examples what the linter does incorrectly Last steps Compare the generated markdown file with the original Gdoc from top to bottom to ensure accurate rendering. Review the markdown file on GitHub to make sure it looks good, as it may slightly differ from the preview in your local markdown editor When a gdoc becomes obsolete or it's deleted Add a note at the top of a gdoc explaining what happened Example: \"Moved to /new_markdown_file.md\" Strike out the entire document Move the gdoc to the _OLD directory Other approaches Best for a large document Approach 1 - Chrome Docs to Markdown extension: Use the Docs to Markdown extension Install the extension from the G Suite marketplace User guide for the extension One needs to accept/reject all suggestions in a gdoc as the extension works poorly when a document is edited in the suggestion mode Approach 2 - Online converter: Google-docs-to-markdown/ Also need to go through Cleaning up converted markdown You might need to remove artifacts manually Markdown -> Gdocs Approach 1: Run > pandoc MyFile.md -f markdown -t odt -s -o MyFile.odt Download the template in odt format Run > pandoc code_organization.md -f markdown -t odt -s -o code_org.odt --reference-doc /Users/saggese/Downloads/Gdoc\\ -\\ Template.odt Open it with TextEdit, copy-paste to Gdoc Approach 2: Instead of copy-paste the markdown into Gdocs, you can copy the rendered markdown in a Gdoc Gdocs does a good job of maintaining the formatting, levels of the headers, the links, and so on Approach 3: https://markdownlivepreview.com/ TODO(gp): Check if the roundtrip works","title":"Documentation about guidelines"},{"location":"Documentation_about_guidelines/#documentation-about-guidelines","text":"Guidelines for describing workflows Markdown vs Google Docs In general Markdown pros Google Docs pros Rules of thumb Useful references Style and cosmetic lints Always use markdown linter Table of content (TOC) Use nice 80 columns formatting for txt files Empty line after heading Bullet lists Using code style Indenting code style Embedding screenshots Improve your written English Make sure your markdown looks good Google docs style conventions Headings Font Convert between Gdocs and Markdown Gdocs -> Markdown Using convert_docx_to_markdown.py Process Cleaning up converted markdown Other approaches Markdown -> Gdocs","title":"Documentation about guidelines"},{"location":"Documentation_about_guidelines/#guidelines-for-describing-workflows","text":"Make no assumptions on the user's knowledge Nothing is obvious to somebody who doesn't know Add ways to verify if a described process worked E.g., \"do this and that, if this and that is correct should see this\" Have a trouble-shooting procedure One approach is to always start from scratch","title":"Guidelines for describing workflows"},{"location":"Documentation_about_guidelines/#markdown-vs-google-docs","text":"","title":"Markdown vs Google Docs"},{"location":"Documentation_about_guidelines/#in-general","text":"We prefer to use Markdown for technical documentation We use Google for notes from meetings and research","title":"In general"},{"location":"Documentation_about_guidelines/#markdown-pros","text":"Can use vim Can version control Easy to use verbatim (e.g., typing foobar ) Easy to style using pandoc Easy to embed code Easy to add Latex equations Easy to grep","title":"Markdown pros"},{"location":"Documentation_about_guidelines/#google-docs-pros","text":"Easy to embed figures Easy to collaborate Easy to make quick changes (instead of making a commit) Easy to publish (just make them public with proper permissions) Styling https://webapps.stackexchange.com/questions/112275/define-special-inline-styles-in-google-docs Interesting add-ons: Enable Markdown Code blocks Use darcula, size 10 def hello(): print(\"hello\") Auto-latex equations","title":"Google Docs pros"},{"location":"Documentation_about_guidelines/#rules-of-thumb","text":"Use Markdown If doc is going to be used as a public guideline If doc has mostly text, code, and formulas If there are notes from a book Use Gdoc If doc requires a lot of images that cannot be placed as text If doc is a research of an analysis","title":"Rules of thumb"},{"location":"Documentation_about_guidelines/#useful-references","text":"Markdown cheatsheet Google guide to Markdown TODO(gp): Make sure it's compatible with our linter","title":"Useful references"},{"location":"Documentation_about_guidelines/#style-and-cosmetic-lints","text":"","title":"Style and cosmetic lints"},{"location":"Documentation_about_guidelines/#always-use-markdown-linter","text":"Most cosmetic lints described further can be taken care automatically by our markdown linter, so make sure to run it after implementing the changes The file is dev_scripts/lint_md.sh , see the docstrings for more details Example run: dev_scripts/lint_md.sh docs/Documentation_about_guidelines.md Do not mix manual edits and linter runs. Best practice is to run the linter and commit the changes it made as separate commit If the linter messes up the text, file an issue with examples of what the linter does incorrectly","title":"Always use markdown linter"},{"location":"Documentation_about_guidelines/#table-of-content-toc","text":"Unfortunately both markdown and GitHub don't support automatically generating a TOC for a document To generate a table of content: Add the following tag at the top of the markdown file below the document title: <!-- toc --> Run the markdown linter in order to build TOC automatically","title":"Table of content (TOC)"},{"location":"Documentation_about_guidelines/#use-nice-80-columns-formatting-for-txt-files","text":"Vim has a :gq command to reflow the comments There are plugins to take care of this for PyCharm Our markdown linter takes care of reflowing the comments as well","title":"Use nice 80 columns formatting for txt files"},{"location":"Documentation_about_guidelines/#empty-line-after-heading","text":"Leave an empty line after a heading to make it more visible Bad ``` # Coming through! I've big important things to do! ... and his big important wheels got STUCK! ``` Good ```","title":"Empty line after heading"},{"location":"Documentation_about_guidelines/#very-important-title","text":"Less important text ```","title":"Very important title"},{"location":"Documentation_about_guidelines/#bullet-lists","text":"We like using bullet list since one can represent thought process more clearly, e.g., This is thought #1 This is related to thought #1 This is thought #2 Well, that was cool! But this is even better We strictly use - instead of * , circles, etc.: Bad ``` Foo bar! hello world Baz ``` Good ``` Foo bar! hello world Baz ```","title":"Bullet lists"},{"location":"Documentation_about_guidelines/#using-code-style","text":"We use code style for Code Dirs (e.g., /home/users ) Command lines (e.g., git push ) When using a block of code use the write syntax highlighting Bash > git push Python python if __name__ == \"__main__\": predict_the_future() print(\"done!\")","title":"Using code style"},{"location":"Documentation_about_guidelines/#indenting-code-style","text":"GitHub / pandoc seems to render incorrectly a code block unless it's indented over the previous line Bad ``` git push ``` Good > git push","title":"Indenting code style"},{"location":"Documentation_about_guidelines/#embedding-screenshots","text":"Avoid to use screenshots whenever possible! However, sometimes we need it (e.g., plot infographics, website inteface, etc.) To do it correctly: Place your screenshot in any comment window at GitHub This will upload the image to the GitHub cloud You DO NOT have to publish a comment, the provided link is already ready to use! Make sure your link has no not-English symbols in alt section They sometimes appear if your native PC language is not English You can avoid it by giving the picture a name in English Alternatively, you can just edit the alt section in the generated link - this will not corrupt the file Place the generated and edited link to the markdown file","title":"Embedding screenshots"},{"location":"Documentation_about_guidelines/#improve-your-written-english","text":"Use English spell-checker Unfortunately this is not enough Type somewhere where you can use several free choices: Grammarly LanguageTool Or other proofreading and copy-paste This is super-useful to improve your English since you see the error and the correction Otherwise you will keep making the same mistakes forever","title":"Improve your written English"},{"location":"Documentation_about_guidelines/#make-sure-your-markdown-looks-good","text":"You can: Check in a branch and use GitHub to render it Use Pycharm to edit, which also renders it side-by-side Compare your markdown with already published documentation","title":"Make sure your markdown looks good"},{"location":"Documentation_about_guidelines/#google-docs-style-conventions","text":"","title":"Google docs style conventions"},{"location":"Documentation_about_guidelines/#headings","text":"We add N (where N is the heading level) # before the heading name, e.g., Heading 1: # Heading 1 Heading 2: ## Heading 2 The reason is that sometimes one doesn't have the time or the patience to format things properly, so at least there is some indication of the level of the titles Avoid having multiple # separatd by a space that sometimes appear in a process of convertion of Gdocs to Markdown files Bad # # Heading 1 Good # Heading 1","title":"Headings"},{"location":"Documentation_about_guidelines/#font","text":"Normal text: Font: Arial Font size: 11 Headings: Font: Arial Style: bold Font size: should be adjusted automatically when one converts \u201cNormal text\u201d to \u201cHeading N\u201d, e.g., when converting some text of size 11 to \u201cHeading 1\u201d the font sizes becomes 20","title":"Font"},{"location":"Documentation_about_guidelines/#convert-between-gdocs-and-markdown","text":"","title":"Convert between Gdocs and Markdown"},{"location":"Documentation_about_guidelines/#gdocs-markdown","text":"","title":"Gdocs -&gt; Markdown"},{"location":"Documentation_about_guidelines/#using-convert_docx_to_markdownpy","text":"In general, we recommend using this approach This python script converts Docx to Markdown using Pandoc. Pros Removing artifacts with this python script, less manual work Best for a large document Handle figures Cons Need to move files","title":"Using convert_docx_to_markdown.py"},{"location":"Documentation_about_guidelines/#process","text":"Download Google document as docx Convert it to markdown using convert_docx_to_markdown.py Usage: ``` ../dev_scripts/convert_docx_to_markdown.py --docx_file Tools_Docker.docx --md_file Tools_Docker.md ``` This command should be run directly under the target output directory for the Markdown file, in order to generate correct image links. Otherwise, you'll need to manually fix the image links. File names can't contain any spaces. Therefore, use underscores _ to replace any spaces.","title":"Process"},{"location":"Documentation_about_guidelines/#cleaning-up-converted-markdown","text":"Fix some formatting manually before running the Markdown linter. Read through Style and cosmetic lints for Markdown formatting and fix some formatting based on the rules. Summary Add the following tag at the top of the markdown file below the document title: <!-- toc --> Use bullet lists to organize the whole Markdown for consistency with other docs. See Coding_Style_Guide.md or any other published Markdown format as reference Add missing ``` around code blocks. These could be missing in the original Google doc. Also adjust code block indentations if needed Remove empty lines manually :'<,'>! perl -ne 'print if /\\S/' Run the lint_md.sh Usage: > dev_scripts/lint_md.sh docs/Documentation_about_guidelines.md What the linter will do: Build TOC automatically Adjust the indentation to improve the Markdown's format (but the precondition is that you have properly adjusted the indentation levels). Remove extra empty lines under headings Adjust text layout Do not mix manual edits and linter runs If the linter messes up the text File bugs in amp with examples what the linter does incorrectly Last steps Compare the generated markdown file with the original Gdoc from top to bottom to ensure accurate rendering. Review the markdown file on GitHub to make sure it looks good, as it may slightly differ from the preview in your local markdown editor When a gdoc becomes obsolete or it's deleted Add a note at the top of a gdoc explaining what happened Example: \"Moved to /new_markdown_file.md\" Strike out the entire document Move the gdoc to the _OLD directory","title":"Cleaning up converted markdown"},{"location":"Documentation_about_guidelines/#other-approaches","text":"Best for a large document Approach 1 - Chrome Docs to Markdown extension: Use the Docs to Markdown extension Install the extension from the G Suite marketplace User guide for the extension One needs to accept/reject all suggestions in a gdoc as the extension works poorly when a document is edited in the suggestion mode Approach 2 - Online converter: Google-docs-to-markdown/ Also need to go through Cleaning up converted markdown You might need to remove artifacts manually","title":"Other approaches"},{"location":"Documentation_about_guidelines/#markdown-gdocs","text":"Approach 1: Run > pandoc MyFile.md -f markdown -t odt -s -o MyFile.odt Download the template in odt format Run > pandoc code_organization.md -f markdown -t odt -s -o code_org.odt --reference-doc /Users/saggese/Downloads/Gdoc\\ -\\ Template.odt Open it with TextEdit, copy-paste to Gdoc Approach 2: Instead of copy-paste the markdown into Gdocs, you can copy the rendered markdown in a Gdoc Gdocs does a good job of maintaining the formatting, levels of the headers, the links, and so on Approach 3: https://markdownlivepreview.com/ TODO(gp): Check if the roundtrip works","title":"Markdown -&gt; Gdocs"},{"location":"Email/","text":"Email Mailing lists Organizing email flow Anatomy of email messages from infra Filtering emails with Gmail Notifications from GitHub GitHub pull requests GitHub issue activity Commits Gdocs TODO emails Asana Mailing lists @all is the mailing list with everybody at the company @contributors is the mailing list with every open-source contributor Organizing email flow We receive tons of emails, and the inflow is going to keep increasing At a large company you can get 10k emails per day (no kidding) The goal is to read all the emails and always be on top of it How can one do that? As usual the answer is get organized Filter emails in folders Separate emails in folders based on the action that they require (e.g., ignore, just read and be aware of it, read and respond) Read email and decide what to do about each of it: No reply needed Reply right away Follow up later (e.g., to read, reply, think about it) Use flags to distinguish what needs to be followed up later or if you are waiting for a response A possible organization in folders is: GitHub Commits in all the repos (be aware of it) Issue updates (read and respond) PRs Commits directly to master (read and review) Commits into documents dir of master (read and review) Emails generated by my GH activities (ignore and mark as read) Asana Daily activities assigned report New comment/activity New task assignment Anatomy of email messages from infra The goal is to classify emails so that we can filter email effectively Filtering emails with Gmail Personally (GP) I prefer an email client (Mozilla Thunderbird and more recently Apple Mail) rather than using Gmail web interface People are able to use it Personally I prefer to use filters on the Gmail (server) side Pros I don't get emails on my cell phone continuously The emails are organized as they arrive Folders are on the server side, so my client can simply sync Cons The Gmail interface for filtering emails is horrible The web interface is https://mail.google.com/mail/u/0/#settings/filters Note that Gmail distinguish different email accounts using different indices, e.g., https://mail.google.com/mail/u/ /#inbox Notifications from GitHub https://help.github.com/en/categories/receiving-notifications-about-activity-on-github GitHub pull requests These emails look like: Samarth KaPatel <notifications@github.com> to cryptokaizen/orange, Subscribed cryptokaizen/cmamp#4765 - Refactoring amp_path on orange - PR in cmamp - cryptokaizen/cmamp#4788 ________________________________________________________________ You can view, comment on, or merge this pull request online at: https://github.com/cryptokaizen/orange/pull/411 Commit Summary - b2b4940 orange fix - 850b2b2 amp File Changes (2 files) - M amp (2) - M dataflow_orange/system/Cx/test/test_master_pnl_real_time_observer_notebook.py (5) Patch Links: - https://github.com/cryptokaizen/orange/pull/411.patch - https://github.com/cryptokaizen/orange/pull/411.diff These emails have the words: \"You can view, comment on, or merge this pull request online at:\" in the body of the email GitHub issue activity These emails look like: Samarth KaPatel <notifications@github.com> to Review, sorrentum/sorrentum @samarth9008 requested your review on: #436 Update Signing_up_for_Sorrentum.md. __ Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because your review was requested. or: Grigorii Pomazkin <notifications@github.com> to cryptokaizen/cmamp, Mention @PomazkinG commented on this pull request. ________________________________________________________________________ In helpers/lib_tasks_pytest.py: > @@ -671,17 +671,25 @@ def run_coverage_report( # type: ignore :param aws_profile: the AWS profile to use for publishing HTML report \"\"\" # TODO(Grisha): allow user to specify which tests to run. ... obsolete, resolving __ Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you were mentioned. These emails can be recognized by the fact that have the words \"You are receiving this because\" in the email body Commits These emails look like: Grigorii Pomazkin <notifications@github.com> to cryptokaizen/cmamp, Push @PomazkinG pushed 1 commit. - 65496bb Merge branch 'master' into CmTask4707_fix_coverage_test __ View it on GitHub or unsubscribe. You are receiving this because you are subscribed to this thread. These emails can be recognized by the fact that have the words \"pushed commit\" in the email body Gdocs These emails have comments-noreply@docs.google.com or (Google Docs) in the \"subject\" field TODO emails These emails have TODO in the subject Asana These emails are received from no-reply@asana.com address These emails may contain: List of tasks assigned to you for today New activities in the tasks assigned to you New task assignment","title":"Email"},{"location":"Email/#email","text":"Mailing lists Organizing email flow Anatomy of email messages from infra Filtering emails with Gmail Notifications from GitHub GitHub pull requests GitHub issue activity Commits Gdocs TODO emails Asana","title":"Email"},{"location":"Email/#mailing-lists","text":"@all is the mailing list with everybody at the company @contributors is the mailing list with every open-source contributor","title":"Mailing lists"},{"location":"Email/#organizing-email-flow","text":"We receive tons of emails, and the inflow is going to keep increasing At a large company you can get 10k emails per day (no kidding) The goal is to read all the emails and always be on top of it How can one do that? As usual the answer is get organized Filter emails in folders Separate emails in folders based on the action that they require (e.g., ignore, just read and be aware of it, read and respond) Read email and decide what to do about each of it: No reply needed Reply right away Follow up later (e.g., to read, reply, think about it) Use flags to distinguish what needs to be followed up later or if you are waiting for a response A possible organization in folders is: GitHub Commits in all the repos (be aware of it) Issue updates (read and respond) PRs Commits directly to master (read and review) Commits into documents dir of master (read and review) Emails generated by my GH activities (ignore and mark as read) Asana Daily activities assigned report New comment/activity New task assignment","title":"Organizing email flow"},{"location":"Email/#anatomy-of-email-messages-from-infra","text":"The goal is to classify emails so that we can filter email effectively","title":"Anatomy of email messages from infra"},{"location":"Email/#filtering-emails-with-gmail","text":"Personally (GP) I prefer an email client (Mozilla Thunderbird and more recently Apple Mail) rather than using Gmail web interface People are able to use it Personally I prefer to use filters on the Gmail (server) side Pros I don't get emails on my cell phone continuously The emails are organized as they arrive Folders are on the server side, so my client can simply sync Cons The Gmail interface for filtering emails is horrible The web interface is https://mail.google.com/mail/u/0/#settings/filters Note that Gmail distinguish different email accounts using different indices, e.g., https://mail.google.com/mail/u/ /#inbox","title":"Filtering emails with Gmail"},{"location":"Email/#notifications-from-github","text":"https://help.github.com/en/categories/receiving-notifications-about-activity-on-github","title":"Notifications from GitHub"},{"location":"Email/#github-pull-requests","text":"These emails look like: Samarth KaPatel <notifications@github.com> to cryptokaizen/orange, Subscribed cryptokaizen/cmamp#4765 - Refactoring amp_path on orange - PR in cmamp - cryptokaizen/cmamp#4788 ________________________________________________________________ You can view, comment on, or merge this pull request online at: https://github.com/cryptokaizen/orange/pull/411 Commit Summary - b2b4940 orange fix - 850b2b2 amp File Changes (2 files) - M amp (2) - M dataflow_orange/system/Cx/test/test_master_pnl_real_time_observer_notebook.py (5) Patch Links: - https://github.com/cryptokaizen/orange/pull/411.patch - https://github.com/cryptokaizen/orange/pull/411.diff These emails have the words: \"You can view, comment on, or merge this pull request online at:\" in the body of the email","title":"GitHub pull requests"},{"location":"Email/#github-issue-activity","text":"These emails look like: Samarth KaPatel <notifications@github.com> to Review, sorrentum/sorrentum @samarth9008 requested your review on: #436 Update Signing_up_for_Sorrentum.md. __ Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because your review was requested. or: Grigorii Pomazkin <notifications@github.com> to cryptokaizen/cmamp, Mention @PomazkinG commented on this pull request. ________________________________________________________________________ In helpers/lib_tasks_pytest.py: > @@ -671,17 +671,25 @@ def run_coverage_report( # type: ignore :param aws_profile: the AWS profile to use for publishing HTML report \"\"\" # TODO(Grisha): allow user to specify which tests to run. ... obsolete, resolving __ Reply to this email directly, view it on GitHub, or unsubscribe. You are receiving this because you were mentioned. These emails can be recognized by the fact that have the words \"You are receiving this because\" in the email body","title":"GitHub issue activity"},{"location":"Email/#commits","text":"These emails look like: Grigorii Pomazkin <notifications@github.com> to cryptokaizen/cmamp, Push @PomazkinG pushed 1 commit. - 65496bb Merge branch 'master' into CmTask4707_fix_coverage_test __ View it on GitHub or unsubscribe. You are receiving this because you are subscribed to this thread. These emails can be recognized by the fact that have the words \"pushed commit\" in the email body","title":"Commits"},{"location":"Email/#gdocs","text":"These emails have comments-noreply@docs.google.com or (Google Docs) in the \"subject\" field","title":"Gdocs"},{"location":"Email/#todo-emails","text":"These emails have TODO in the subject","title":"TODO emails"},{"location":"Email/#asana","text":"These emails are received from no-reply@asana.com address These emails may contain: List of tasks assigned to you for today New activities in the tasks assigned to you New task assignment","title":"Asana"},{"location":"First_review_process/","text":"First Review Process Read Python Style Guide Run linter Compare your code to example code Save Reviewer time Assign Reviewers Mention the issue Resolve conversations Merge master to your branch Ask for reviews Do not use screenshots Report bugs correctly Look at examples of the first reviews We understand that receiving feedback on your code can be a difficult process, but it is an important part of our development workflow. Here we have gathered some helpful tips and resources to guide you through your first review. Read Python Style Guide Before submitting your code for review, we highly recommend that you read the Python Style Guide , which outlines the major conventions and best practices for writing Python code. Adhering to these standards will help ensure that your code is easy to read, maintain, and understand for other members of the team. Run linter Linter is a tool that checks (and tries to fix automatically) your code for syntax errors, style violations, and other issues. Run it on all the changed files to automatically catch any code issues before filing any PR or before requesting a review! To be able to run the linter, you need to you need to set up your client first since you're outside Docker: The instructions are available at Sorrentum_development_setup.md In practice you need to have run > source dev_scripts/setenv_amp.sh Run the linter with invoke command (which is abbreviated as i ) and pass all the files you need to lint in brackets after the --files option, separated by a space: ``` i lint --files \"defi/tulip/implementation/order.py defi/tulip/implementation/order_matching.py\" ``` Output example: defi/tulip/implementation/order_matching.py:14: error: Cannot find implementation or library stub for module named 'defi.dao_cross' [import] defi/tulip/implementation/order_matching.py:69: error: Need type annotation for 'buy_heap' (hint: \"buy_heap: List[<type>] = ...\") [var-annotated] defi/tulip/implementation/order_matching.py:70: error: Need type annotation for 'sell_heap' (hint: \"sell_heap: List[<type>] = ...\") [var-annotated] ... i lint has options for many workflows. E.g., you can automatically lint all the files that you touched in your PR with --branch , the files in the last commit with --last-commit . You can look at all the options with: > i lint --help Fix the lints No need to obsessively fix all of them - just crucial and obvious ones Post unresolved lints in your PR so Reviewer could see them and know which should be fixed and which are not Compare your code to example code To get an idea of what well-formatted and well-organized code looks like, we suggest taking a look at some examples of code that adhere to our standards. We try to maintain universal approaches to all the parts of the code, so when looking at a code example, check for: Code style Docstrings and comments Type hints Containing directory structure Here are some links to example code: Classes and functions: defi/tulip/implementation/order.py defi/tulip/implementation/order_matching.py Unit tests: defi/tulip/test/test_order_matching.py defi/tulip/test/test_optimize.py Scripts: dev_scripts/replace_text.py dev_scripts/lint_md.sh Save Reviewer time Assign Reviewers Make sure to select a Reviewer in a corresponding GitHub field so he/she gets notified Junior contributors should assign Team Leaders (e.g., Grisha, DanY, Samarth, ...) to review their PR Team Leaders will assign integrators (GP & Paul) themselves after all their comments are implemented Ping the assigned Reviewer in the issue if nothing happens in 24 hours If you want to keep someone notified about changes in the PR but do not want to make him/her a Reviewer, type FYI @github_name in a comment section Mention the issue Mention the corresponding issue in the PR description to ease the navigation E.g., see an example Resolve conversations When you've implemented a comment from a Reviewer, press Resolve conversation button so the Reviewer knows that you actually took care of it Merge master to your branch Before any PR review request do i git_merge_master in order to keep the code updated Resolve conflicts if there are any Do not forget to push it since this action is a commit itself Actually, a useful practice is to merge master to your branch every time you to get back to work on it This way you make sure that your branch is always using a relevant code and avoid huge merge conflicts NEVER press Squash and merge button yourself You need to merge master branch to your branch - not vice verca! This is a strictly Team Leaders and Integrators responsibility Ask for reviews When you've implemented all the comments and need another round of review: Press the circling arrows sign next to the Reviewer for the ping Remove PR_for_authors and add PR_for_reviewers label (labels desc ) Do not use screenshots Stack trace and logs are much more convenient to use for debugging Screenshots are often too small to capture both input and return logs while consuming a lot of basically useless memory The exceptions are plots and non-code information Examples: Bad Good Input: type_ = \"supply\" supply_curve1 = ddcrsede.get_supply_demand_discrete_curve( type_, supply_orders_df1 ) supply_curve1 Error: ``` NameError Traceback (most recent call last) Cell In [5], line 2 1 type_ = \"supply\" ----> 2 supply_curve1 = ddcrsede.get_supply_demand_discrete_curve( 3 type_, supply_orders_df1 4 ) 5 supply_curve1 NameError: name 'ddcrsede' is not defined ``` Report bugs correctly Whenever you face any errors put as much information about the issue as possible, e.g.,: What you are trying to achieve Command line you ran, e.g., > i lint -f defi/tulip/test/test_dao_cross_sol.py Copy-paste the error and the stack trace from the cmd line, no screenshots , e.g., Traceback (most recent call last): File \"/venv/bin/invoke\", line 8, in <module> sys.exit(program.run()) File \"/venv/lib/python3.8/site-packages/invoke/program.py\", line 373, in run self.parse_collection() ValueError: One and only one set-up config should be true: The log of the run Maybe the same run using -v DEBUG to get more info on the problem What the problem is Why the outcome is different from what you expected E.g. on how to report any issues https://github.com/sorrentum/sorrentum/issues/370#issue-1782574355 Look at examples of the first reviews It can be helpful to review some examples of previous first reviews to get an idea of what common issues are and how to address them. Here are some links to a few \"painful\" first reviews: Adding unit tests: https://github.com/sorrentum/sorrentum/pull/166 https://github.com/sorrentum/sorrentum/pull/186 Writing scripts: https://github.com/sorrentum/sorrentum/pull/267 https://github.com/sorrentum/sorrentum/pull/276","title":"First Review Process"},{"location":"First_review_process/#first-review-process","text":"Read Python Style Guide Run linter Compare your code to example code Save Reviewer time Assign Reviewers Mention the issue Resolve conversations Merge master to your branch Ask for reviews Do not use screenshots Report bugs correctly Look at examples of the first reviews We understand that receiving feedback on your code can be a difficult process, but it is an important part of our development workflow. Here we have gathered some helpful tips and resources to guide you through your first review.","title":"First Review Process"},{"location":"First_review_process/#read-python-style-guide","text":"Before submitting your code for review, we highly recommend that you read the Python Style Guide , which outlines the major conventions and best practices for writing Python code. Adhering to these standards will help ensure that your code is easy to read, maintain, and understand for other members of the team.","title":"Read Python Style Guide"},{"location":"First_review_process/#run-linter","text":"Linter is a tool that checks (and tries to fix automatically) your code for syntax errors, style violations, and other issues. Run it on all the changed files to automatically catch any code issues before filing any PR or before requesting a review! To be able to run the linter, you need to you need to set up your client first since you're outside Docker: The instructions are available at Sorrentum_development_setup.md In practice you need to have run > source dev_scripts/setenv_amp.sh Run the linter with invoke command (which is abbreviated as i ) and pass all the files you need to lint in brackets after the --files option, separated by a space: ``` i lint --files \"defi/tulip/implementation/order.py defi/tulip/implementation/order_matching.py\" ``` Output example: defi/tulip/implementation/order_matching.py:14: error: Cannot find implementation or library stub for module named 'defi.dao_cross' [import] defi/tulip/implementation/order_matching.py:69: error: Need type annotation for 'buy_heap' (hint: \"buy_heap: List[<type>] = ...\") [var-annotated] defi/tulip/implementation/order_matching.py:70: error: Need type annotation for 'sell_heap' (hint: \"sell_heap: List[<type>] = ...\") [var-annotated] ... i lint has options for many workflows. E.g., you can automatically lint all the files that you touched in your PR with --branch , the files in the last commit with --last-commit . You can look at all the options with: > i lint --help Fix the lints No need to obsessively fix all of them - just crucial and obvious ones Post unresolved lints in your PR so Reviewer could see them and know which should be fixed and which are not","title":"Run linter"},{"location":"First_review_process/#compare-your-code-to-example-code","text":"To get an idea of what well-formatted and well-organized code looks like, we suggest taking a look at some examples of code that adhere to our standards. We try to maintain universal approaches to all the parts of the code, so when looking at a code example, check for: Code style Docstrings and comments Type hints Containing directory structure Here are some links to example code: Classes and functions: defi/tulip/implementation/order.py defi/tulip/implementation/order_matching.py Unit tests: defi/tulip/test/test_order_matching.py defi/tulip/test/test_optimize.py Scripts: dev_scripts/replace_text.py dev_scripts/lint_md.sh","title":"Compare your code to example code"},{"location":"First_review_process/#save-reviewer-time","text":"","title":"Save Reviewer time"},{"location":"First_review_process/#assign-reviewers","text":"Make sure to select a Reviewer in a corresponding GitHub field so he/she gets notified Junior contributors should assign Team Leaders (e.g., Grisha, DanY, Samarth, ...) to review their PR Team Leaders will assign integrators (GP & Paul) themselves after all their comments are implemented Ping the assigned Reviewer in the issue if nothing happens in 24 hours If you want to keep someone notified about changes in the PR but do not want to make him/her a Reviewer, type FYI @github_name in a comment section","title":"Assign Reviewers"},{"location":"First_review_process/#mention-the-issue","text":"Mention the corresponding issue in the PR description to ease the navigation E.g., see an example","title":"Mention the issue"},{"location":"First_review_process/#resolve-conversations","text":"When you've implemented a comment from a Reviewer, press Resolve conversation button so the Reviewer knows that you actually took care of it","title":"Resolve conversations"},{"location":"First_review_process/#merge-master-to-your-branch","text":"Before any PR review request do i git_merge_master in order to keep the code updated Resolve conflicts if there are any Do not forget to push it since this action is a commit itself Actually, a useful practice is to merge master to your branch every time you to get back to work on it This way you make sure that your branch is always using a relevant code and avoid huge merge conflicts NEVER press Squash and merge button yourself You need to merge master branch to your branch - not vice verca! This is a strictly Team Leaders and Integrators responsibility","title":"Merge master to your branch"},{"location":"First_review_process/#ask-for-reviews","text":"When you've implemented all the comments and need another round of review: Press the circling arrows sign next to the Reviewer for the ping Remove PR_for_authors and add PR_for_reviewers label (labels desc )","title":"Ask for reviews"},{"location":"First_review_process/#do-not-use-screenshots","text":"Stack trace and logs are much more convenient to use for debugging Screenshots are often too small to capture both input and return logs while consuming a lot of basically useless memory The exceptions are plots and non-code information Examples: Bad Good Input: type_ = \"supply\" supply_curve1 = ddcrsede.get_supply_demand_discrete_curve( type_, supply_orders_df1 ) supply_curve1 Error:","title":"Do not use screenshots"},{"location":"First_review_process/#_1","text":"NameError Traceback (most recent call last) Cell In [5], line 2 1 type_ = \"supply\" ----> 2 supply_curve1 = ddcrsede.get_supply_demand_discrete_curve( 3 type_, supply_orders_df1 4 ) 5 supply_curve1 NameError: name 'ddcrsede' is not defined ```","title":"```"},{"location":"First_review_process/#report-bugs-correctly","text":"Whenever you face any errors put as much information about the issue as possible, e.g.,: What you are trying to achieve Command line you ran, e.g., > i lint -f defi/tulip/test/test_dao_cross_sol.py Copy-paste the error and the stack trace from the cmd line, no screenshots , e.g., Traceback (most recent call last): File \"/venv/bin/invoke\", line 8, in <module> sys.exit(program.run()) File \"/venv/lib/python3.8/site-packages/invoke/program.py\", line 373, in run self.parse_collection() ValueError: One and only one set-up config should be true: The log of the run Maybe the same run using -v DEBUG to get more info on the problem What the problem is Why the outcome is different from what you expected E.g. on how to report any issues https://github.com/sorrentum/sorrentum/issues/370#issue-1782574355","title":"Report bugs correctly"},{"location":"First_review_process/#look-at-examples-of-the-first-reviews","text":"It can be helpful to review some examples of previous first reviews to get an idea of what common issues are and how to address them. Here are some links to a few \"painful\" first reviews: Adding unit tests: https://github.com/sorrentum/sorrentum/pull/166 https://github.com/sorrentum/sorrentum/pull/186 Writing scripts: https://github.com/sorrentum/sorrentum/pull/267 https://github.com/sorrentum/sorrentum/pull/276","title":"Look at examples of the first reviews"},{"location":"From_zero_to_modeling/","text":"Papers to read [ ] 2018, Trading and Arbitrage in Cryptocurrency Markets, https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3171204","title":"From zero to modeling"},{"location":"General_rules_of_collaboration/","text":"General Rules of Collaboration Ask somebody if you have any doubts Ping Team Leaders when you are out of tasks Collaboration Why do we need to follow this handbook? Learning from each other Consistency and process Sync-ups All-hands meetings Technical sync-ups Ad-hoc meetings Org emails Synchronization point Morning email Communication Use the right form of communication DRY also applies to documentation Avoid write-once code and research Consistency Training period Go slowly to go faster Improve your English! Study an English grammar book Ask somebody if you have any doubts If you have doubts on how to do something you want to do: Look in the documentation and our Google drive Google search is your friend Ask your team-members Learn how to ask questions first Note that often people tell you his / her interpretation or their workaround for a problem, which might not be the best approach, so be careful and always think for yourself Don't hesitate to ask anyone, even GP & Paul Ping Team Leaders when you are out of tasks When you're close to being out of tasks or all your ongoing PRs are waiting for review and are close to being merged, feel free to ping us in the Telegram chat to ask for more issues In this way, Team Leaders can quickly assign you another issue, before you run out of work The goal is for everyone to have 2 issues to work on at the same time to avoid getting blocked on us Collaboration Why do we need to follow this handbook? Learning from each other Proper research and software engineering practices allow us to: Learn from each other Accumulate and distill the wisdom of experts Share lessons learned from our mistakes along the way Consistency and process Consistency is a crucial enabler to make teams faster Productivity increases when team members \"work in the same way\", i.e., there is a single official way of performing a task, so that it's possible and easy to: Re-use research and software components Help each other in debugging issues Add/transfer new people to teams Work on multiple projects simultaneously Learn from each other's experience and mistakes Review each other's work looking for errors and improvements ... We are not going to discuss and debate the rationale, but instead assume the above as self-evident truth Sync-ups We meet regularly every week and with different audiences to check on the progress of the many projects we work on All-hands meetings All-hands meeting on Mondays has the following goals: Summarize ongoing projects and their status Review the ZenHub board and share the achieved milestones Discuss blocking tasks across projects Discuss topics of general interest E.g., organization, process Talk about the team, hiring, customers Technical sync-ups We meet one or two times per week for each of the projects (e.g., IM, WEB3) Please check your calendar to make sure the times work and the invited participants are correct The people running the day-to-day project should update the meeting agenda in the Gdoc Try to do it one day before so that everybody knows ahead of time what we need to talk about and can come prepared Typically 2-3 issues are more than enough to fill one hour of discussion Give priority to tasks that are controversial, blocking, or finished No reason to linger on the successes or the easy stuff Send an email or tag a comment to Gdocs to broadcast the agenda It's ok to skip a meeting when the agenda is empty, or keep it short when there is not much to discuss We don't have to fill one hour every time Ad-hoc meetings Don't hesitate to ask for a quick meeting if you are unsure about: What exactly needs to be done in a GitHub Issue How to set-up something (e.g., environment, docker) Better safe than sorry Org emails GP & Paul may send emails with the subject starting with \"ORG:\" pointing to interesting docs that are of general interest and relevance Please make sure to read the docs carefully and internalize what we suggest to do and, especially, the rationale of the proposed solutions It's ok to acknowledge the email replying to all@crypto-kaizen.com Synchronization point We understand that most of the time everybody is head-down making progress on their tasks This is great! However, sometimes we need synchronization: We need to stop the progress for a bit when requested Do an urgent task Acknowledge that the task is done Go back to pushing The procedure is: One of us (e.g., GP or Paul) creates a GitHub task, with: Detailed instructions The list of all of the persons in charge of executing the task Send a ping with the link on Telegram if the task is urgent Everybody does what's asked Mark on the GitHub task your name Morning email First thing in the morning, send an email to all@crypto-kaizen.com if you are full-time or to contributors@crypto-kaizen.com if you are interns or collaborators to broadcast what you are planning to work on during that day If you are a collaborator or intern, follow the steps to join the mailing group Visit the group Click \u201cask to join group\u201d Choose the following settings Wait for the confirmation e-mail, one for the group managers will approve your request. It should look like this: The goal is: Think about what you are going to work on for the day, so you have a clear plan Let Team Leaders know that you're going work today and what is your workload Make sure people blocked on your tasks know that / whether you are working on those tasks Broadcast if you are blocked or if you don't have tasks The process is: Stick to using TODO (all caps) as the email subject, and the suggested format, so that it's easy to filter emails and see what everybody is doing today and what was going on in the previous days Specify how many hours are you going to work today At Monday we also specify an estimation for a week No need to be too specific, give just an approximation List all the tasks you're going to work during the day in priority order Add a hyperlink to the corresponding GitHub issue to each task in order to ease the navigation Communicate realistic ETAs No reason to be optimistic: complex things take time to be done correctly Use a date in the usual format, e.g. 2023-05-10 Add \"today\", \"yesterday\", \"tomorrow\", \"end of week\" so that it's easier to parse If your original ETA needs to be updated (e.g., you thought that you would have finished a task by yesterday, but it's taking longer) keep the older ETA and add the new one, e.g., ``` Create matching service #261 ETA: today (2023-05-25) Original ETA: yesterday (2023-05-24) Reason: it was more complex than what we thought or Reason: I had to context switch to ... ``` Full example: ``` Subject: TODO Hi everybody! This week I'm going to work ~40 hours Today I'm going to work ~8 hours on: Create matching service #261 ETA: today (2023-05-25) Original ETA: yesterday (2023-05-24) Reason: it was more complex than what we thought SorrTask154: Create linear supply/demand curves #177 Implement requested changes and lead to merge ETA: tomorrow (2023-05-26) Background tasks Help Toma with creating TWAP/VWAP external adapter Publish documentation about Jupyter notebooks Technical sync-up with GP Blocked on SorrTaskXYZ: Fix the world before it explodes #666 I'm running out of tasks! ``` Example image: Communication Use the right form of communication GitHub This is a major form of communication about technical details, so if you have any questions about any particular issue or PR, discuss it there first, e.g.: Clarify issue specs Ask for help with debugging PR reviews Asana Is concerned with all the non-technical issues in general, e.g.: Work organization tasks Marketing and funding On-boarding process Telegram This is our messenger for tight interaction (like a debug session) or immediacy (e.g., \"are you ready for the sync up?\") Please, avoid discussing anything that can be discussed at GitHub or Asana You often need to reference some comments and ideas in other places like issues or messages and it is impossible to reference a Telegram message outside of it It is much easier for all reviewers to catch up with all the thinking process if it is logged at one place - Telegram is never a place for this Jupyter notebooks Generally used to implement and describe research in detail Master notebooks are intended to be used as tools for demonstrative analysis with visible stats and plots Markdown files Document instructions, process, design closely related to code Notes that need to be close to the code itself Documents that need to be authoritative and long-term (e.g., reviewed, tracked carefully) Google docs Document research in a descriptive way Explain what are the results independently on how they were reached Emails TODO s primarily Rarely used for any other purpose Exceptions are to send non-urgent information to everybody There should be little replication among these forms of documentation It's not ok to file a bug and then ping on Telegram unless it's urgent Google Form When you want to ask a question anonymously use https://forms.gle/KMQgobqbyxhoTR9n6 The question will be discussed at the all hands meeting DRY also applies to documentation DRY ! Do not Repeat Yourself E.g., it's not a good idea to cut & paste pieces of Gdocs in a GitHub bug, rather just point to the relevant session on Gdocs from the GitHub bug Avoid write-once code and research Code and research is: Written once by a few people Read many times by many people Therefore it is essential to invest in the process of writing it heavily Consistency Coding/research across our group is done with consistent procedures, code layout, and naming conventions Training period When you start working with us, you need to go through a period of training in following the procedures and conventions described in this handbook We understand that this is a painful process for you: You need to change your old habits for new habits that you might disagree with, or not comprehend You need to rework your code/notebooks that are already correctly working until it adheres to the new conventions Understand that this is also a painful process for the reviewers: On top of their usual workload, they need to: Invest time to explain to you how we do things Answer your questions Try to convey the sense of why these procedures are important In a few words, nobody enjoys this process, and yet it is necessary, mandatory, and even beneficial The process acquaintance can take several days if you are open and patient, but months if you resist or treat it as an afterthought Our suggestion is to accept these rules as the existence of gravity Go slowly to go faster Once you reach proficiency, you will be moving much faster and make up for the invested time In fact, everyone will be much quicker, because everyone will be able to look at any part of the codebase or any notebook and get oriented quickly Vacations/OOTO time We use vacation calendar to announce time off If you are a part of @all mailing group you should be able to access this calendar with your company email Create an event in it, whenever you have planned time off in order to let your colleagues know in advance Improve your English! Make sure you have English checker in all your tools: Pycharm: you can use this plugin Vim: set spell Google docs: Grammarly GitHub and web: Grammarly Email client: TBD These tools are going to help you improve since you can see the mistake as you go Feel free to use Google Translate when you are not sure about a word or a phrase What's the point of doing an excellent job if you can't communicate it? Study an English grammar book I used this when I learned English (late in life at 25 starting from no-English-whatsoever, so you can do it too)","title":"General Rules of Collaboration"},{"location":"General_rules_of_collaboration/#general-rules-of-collaboration","text":"Ask somebody if you have any doubts Ping Team Leaders when you are out of tasks Collaboration Why do we need to follow this handbook? Learning from each other Consistency and process Sync-ups All-hands meetings Technical sync-ups Ad-hoc meetings Org emails Synchronization point Morning email Communication Use the right form of communication DRY also applies to documentation Avoid write-once code and research Consistency Training period Go slowly to go faster Improve your English! Study an English grammar book","title":"General Rules of Collaboration"},{"location":"General_rules_of_collaboration/#ask-somebody-if-you-have-any-doubts","text":"If you have doubts on how to do something you want to do: Look in the documentation and our Google drive Google search is your friend Ask your team-members Learn how to ask questions first Note that often people tell you his / her interpretation or their workaround for a problem, which might not be the best approach, so be careful and always think for yourself Don't hesitate to ask anyone, even GP & Paul","title":"Ask somebody if you have any doubts"},{"location":"General_rules_of_collaboration/#ping-team-leaders-when-you-are-out-of-tasks","text":"When you're close to being out of tasks or all your ongoing PRs are waiting for review and are close to being merged, feel free to ping us in the Telegram chat to ask for more issues In this way, Team Leaders can quickly assign you another issue, before you run out of work The goal is for everyone to have 2 issues to work on at the same time to avoid getting blocked on us","title":"Ping Team Leaders when you are out of tasks"},{"location":"General_rules_of_collaboration/#collaboration","text":"","title":"Collaboration"},{"location":"General_rules_of_collaboration/#why-do-we-need-to-follow-this-handbook","text":"","title":"Why do we need to follow this handbook?"},{"location":"General_rules_of_collaboration/#learning-from-each-other","text":"Proper research and software engineering practices allow us to: Learn from each other Accumulate and distill the wisdom of experts Share lessons learned from our mistakes along the way","title":"Learning from each other"},{"location":"General_rules_of_collaboration/#consistency-and-process","text":"Consistency is a crucial enabler to make teams faster Productivity increases when team members \"work in the same way\", i.e., there is a single official way of performing a task, so that it's possible and easy to: Re-use research and software components Help each other in debugging issues Add/transfer new people to teams Work on multiple projects simultaneously Learn from each other's experience and mistakes Review each other's work looking for errors and improvements ... We are not going to discuss and debate the rationale, but instead assume the above as self-evident truth","title":"Consistency and process"},{"location":"General_rules_of_collaboration/#sync-ups","text":"We meet regularly every week and with different audiences to check on the progress of the many projects we work on","title":"Sync-ups"},{"location":"General_rules_of_collaboration/#all-hands-meetings","text":"All-hands meeting on Mondays has the following goals: Summarize ongoing projects and their status Review the ZenHub board and share the achieved milestones Discuss blocking tasks across projects Discuss topics of general interest E.g., organization, process Talk about the team, hiring, customers","title":"All-hands meetings"},{"location":"General_rules_of_collaboration/#technical-sync-ups","text":"We meet one or two times per week for each of the projects (e.g., IM, WEB3) Please check your calendar to make sure the times work and the invited participants are correct The people running the day-to-day project should update the meeting agenda in the Gdoc Try to do it one day before so that everybody knows ahead of time what we need to talk about and can come prepared Typically 2-3 issues are more than enough to fill one hour of discussion Give priority to tasks that are controversial, blocking, or finished No reason to linger on the successes or the easy stuff Send an email or tag a comment to Gdocs to broadcast the agenda It's ok to skip a meeting when the agenda is empty, or keep it short when there is not much to discuss We don't have to fill one hour every time","title":"Technical sync-ups"},{"location":"General_rules_of_collaboration/#ad-hoc-meetings","text":"Don't hesitate to ask for a quick meeting if you are unsure about: What exactly needs to be done in a GitHub Issue How to set-up something (e.g., environment, docker) Better safe than sorry","title":"Ad-hoc meetings"},{"location":"General_rules_of_collaboration/#org-emails","text":"GP & Paul may send emails with the subject starting with \"ORG:\" pointing to interesting docs that are of general interest and relevance Please make sure to read the docs carefully and internalize what we suggest to do and, especially, the rationale of the proposed solutions It's ok to acknowledge the email replying to all@crypto-kaizen.com","title":"Org emails"},{"location":"General_rules_of_collaboration/#synchronization-point","text":"We understand that most of the time everybody is head-down making progress on their tasks This is great! However, sometimes we need synchronization: We need to stop the progress for a bit when requested Do an urgent task Acknowledge that the task is done Go back to pushing The procedure is: One of us (e.g., GP or Paul) creates a GitHub task, with: Detailed instructions The list of all of the persons in charge of executing the task Send a ping with the link on Telegram if the task is urgent Everybody does what's asked Mark on the GitHub task your name","title":"Synchronization point"},{"location":"General_rules_of_collaboration/#morning-email","text":"First thing in the morning, send an email to all@crypto-kaizen.com if you are full-time or to contributors@crypto-kaizen.com if you are interns or collaborators to broadcast what you are planning to work on during that day If you are a collaborator or intern, follow the steps to join the mailing group Visit the group Click \u201cask to join group\u201d Choose the following settings Wait for the confirmation e-mail, one for the group managers will approve your request. It should look like this: The goal is: Think about what you are going to work on for the day, so you have a clear plan Let Team Leaders know that you're going work today and what is your workload Make sure people blocked on your tasks know that / whether you are working on those tasks Broadcast if you are blocked or if you don't have tasks The process is: Stick to using TODO (all caps) as the email subject, and the suggested format, so that it's easy to filter emails and see what everybody is doing today and what was going on in the previous days Specify how many hours are you going to work today At Monday we also specify an estimation for a week No need to be too specific, give just an approximation List all the tasks you're going to work during the day in priority order Add a hyperlink to the corresponding GitHub issue to each task in order to ease the navigation Communicate realistic ETAs No reason to be optimistic: complex things take time to be done correctly Use a date in the usual format, e.g. 2023-05-10 Add \"today\", \"yesterday\", \"tomorrow\", \"end of week\" so that it's easier to parse If your original ETA needs to be updated (e.g., you thought that you would have finished a task by yesterday, but it's taking longer) keep the older ETA and add the new one, e.g., ``` Create matching service #261 ETA: today (2023-05-25) Original ETA: yesterday (2023-05-24) Reason: it was more complex than what we thought or Reason: I had to context switch to ... ``` Full example: ``` Subject: TODO Hi everybody! This week I'm going to work ~40 hours Today I'm going to work ~8 hours on: Create matching service #261 ETA: today (2023-05-25) Original ETA: yesterday (2023-05-24) Reason: it was more complex than what we thought SorrTask154: Create linear supply/demand curves #177 Implement requested changes and lead to merge ETA: tomorrow (2023-05-26) Background tasks Help Toma with creating TWAP/VWAP external adapter Publish documentation about Jupyter notebooks Technical sync-up with GP Blocked on SorrTaskXYZ: Fix the world before it explodes #666 I'm running out of tasks! ``` Example image:","title":"Morning email"},{"location":"General_rules_of_collaboration/#communication","text":"","title":"Communication"},{"location":"General_rules_of_collaboration/#use-the-right-form-of-communication","text":"GitHub This is a major form of communication about technical details, so if you have any questions about any particular issue or PR, discuss it there first, e.g.: Clarify issue specs Ask for help with debugging PR reviews Asana Is concerned with all the non-technical issues in general, e.g.: Work organization tasks Marketing and funding On-boarding process Telegram This is our messenger for tight interaction (like a debug session) or immediacy (e.g., \"are you ready for the sync up?\") Please, avoid discussing anything that can be discussed at GitHub or Asana You often need to reference some comments and ideas in other places like issues or messages and it is impossible to reference a Telegram message outside of it It is much easier for all reviewers to catch up with all the thinking process if it is logged at one place - Telegram is never a place for this Jupyter notebooks Generally used to implement and describe research in detail Master notebooks are intended to be used as tools for demonstrative analysis with visible stats and plots Markdown files Document instructions, process, design closely related to code Notes that need to be close to the code itself Documents that need to be authoritative and long-term (e.g., reviewed, tracked carefully) Google docs Document research in a descriptive way Explain what are the results independently on how they were reached Emails TODO s primarily Rarely used for any other purpose Exceptions are to send non-urgent information to everybody There should be little replication among these forms of documentation It's not ok to file a bug and then ping on Telegram unless it's urgent Google Form When you want to ask a question anonymously use https://forms.gle/KMQgobqbyxhoTR9n6 The question will be discussed at the all hands meeting","title":"Use the right form of communication"},{"location":"General_rules_of_collaboration/#dry-also-applies-to-documentation","text":"DRY ! Do not Repeat Yourself E.g., it's not a good idea to cut & paste pieces of Gdocs in a GitHub bug, rather just point to the relevant session on Gdocs from the GitHub bug","title":"DRY also applies to documentation"},{"location":"General_rules_of_collaboration/#avoid-write-once-code-and-research","text":"Code and research is: Written once by a few people Read many times by many people Therefore it is essential to invest in the process of writing it heavily","title":"Avoid write-once code and research"},{"location":"General_rules_of_collaboration/#consistency","text":"Coding/research across our group is done with consistent procedures, code layout, and naming conventions","title":"Consistency"},{"location":"General_rules_of_collaboration/#training-period","text":"When you start working with us, you need to go through a period of training in following the procedures and conventions described in this handbook We understand that this is a painful process for you: You need to change your old habits for new habits that you might disagree with, or not comprehend You need to rework your code/notebooks that are already correctly working until it adheres to the new conventions Understand that this is also a painful process for the reviewers: On top of their usual workload, they need to: Invest time to explain to you how we do things Answer your questions Try to convey the sense of why these procedures are important In a few words, nobody enjoys this process, and yet it is necessary, mandatory, and even beneficial The process acquaintance can take several days if you are open and patient, but months if you resist or treat it as an afterthought Our suggestion is to accept these rules as the existence of gravity","title":"Training period"},{"location":"General_rules_of_collaboration/#go-slowly-to-go-faster","text":"Once you reach proficiency, you will be moving much faster and make up for the invested time In fact, everyone will be much quicker, because everyone will be able to look at any part of the codebase or any notebook and get oriented quickly","title":"Go slowly to go faster"},{"location":"General_rules_of_collaboration/#vacationsooto-time","text":"We use vacation calendar to announce time off If you are a part of @all mailing group you should be able to access this calendar with your company email Create an event in it, whenever you have planned time off in order to let your colleagues know in advance","title":"Vacations/OOTO time"},{"location":"General_rules_of_collaboration/#improve-your-english","text":"Make sure you have English checker in all your tools: Pycharm: you can use this plugin Vim: set spell Google docs: Grammarly GitHub and web: Grammarly Email client: TBD These tools are going to help you improve since you can see the mistake as you go Feel free to use Google Translate when you are not sure about a word or a phrase What's the point of doing an excellent job if you can't communicate it?","title":"Improve your English!"},{"location":"General_rules_of_collaboration/#study-an-english-grammar-book","text":"I used this when I learned English (late in life at 25 starting from no-English-whatsoever, so you can do it too)","title":"Study an English grammar book"},{"location":"GitHub_ZenHub_workflows/","text":"GitHub/ZenHub workflows Introduction Concepts Epic Master Epics Sub-epics Issue Label Pipeline PR Issue workflows Filing a new issue Updating an issue Closing an issue PR workflows PR labels Filing a new PR General tips Filing process Review Addressing comment Coverage reports in PRs - discussion Introduction In the following we use the abbreviations below: GH = GitHub ZH = ZenHub PR = Pull Request RP = Responsible party (Team Leader) Everything we work on comes as a GH task We file tasks and then prioritize and distribute the workload We try to always work on high priority (aka, P0) tasks Issues vs bugs vs tasks We call GH Issues \"issues\", and \"tasks\", (sometimes \"tickets\") interchangeably. We avoid to call them bugs since many times we use GH to track ideas, activities, and improvements, and not only defects in the code The best names are \"tasks\" and \"issues\" ZenHub for project management We use ZH as project management layer on top of GH Please install the ZH extension for GH. This is going to make your life easier Concepts Epic An Epic pulls together Issues that are somehow related by their topic We distinguish Master Epics (e.g., WEB3 ) and sub-Epics (e.g., WEB3 - DaoCross v0.1 ) See the current list of Epics on GH here We maintain all the information about what the Epic is about in its description Master Epics Master Epics are long-running Epics (i.e., projects) E.g., WEB3 Each issue should belong to at least one Epic: either a sub-epic or a master Epic There is no need to add an issue to a Master Epic if it is already added to a sub-epic Sub-epics Master Epics can be broken down into smaller Epics (=sub-epics) E.g., WEB3 - DaoCross v0.1 Their titles should follow the pattern: XYZ - ABC v* , where: XYZ - master Epic title ABC - sub-epic title v* - version Epics and sub-epics are typed as EPIC - Sub-Epic , i.e., the master epic is capitalized and the sub-epics are capitalized lower-case Sub-epics should have a short title and a smaller scope Some sub-epics are related to short term milestones or releases (e.g., WEB3 - DaoCross - v0.1 ), other sub-epics are for long-running activities (e.g., WEB3 - Good First Issue ) Sub-epics should belong to a Master Epic in ZH A sub-epic can be moved to Done/Pre-prod only if all issues nested in it are moved to Done/Pre-prod Issue Issue is a piece of work to be done. Issues are combined into Epics by topic An issue has certain characteristics, i.e. labels An issue has a progress status, i.e. ZH pipeline (e.g., Product backlog (P1) , In progress , Done/Pre-prod ) PRs are linked to work needed to complete an issue Label Labels are attributes of an issue (or PR), e.g., good first issue , PR_for_reviewers , duplicate , etc. See the current list of labels and their descriptions here Pipeline A ZH Pipeline represents the \"progress\" status of an Issue in our process We have the following Pipelines on the ZH board: New Issues Any new GH Issue with unclear Epic / Pipeline goes here Icebox (P2) Low priority, unprioritized issues Product Backlog (P1) Issues of medium priority at the moment Background Tasks one can do in background, e.g. reading, updating documentation, etc. Sprint backlog (P0) Issues of high priority at the moment In Progress Issues that we are currently working on Review/QA Issues opened for review and testing Code is ready to be deployed pending feedback Done/Pre-prod Issues that are done and are waiting for closing Epics Both Master Epics and Sub-epics Closed Issues that are done and don't need a follow-up GP/RPs are responsible for closing PR A pull request is an event where a contributor asks to review code they want to merge into a project Issue workflows Filing a new issue Use an informative description (typically an action \"Do this and that\") We don't use a period at the end of the title If it is a \u201cserious\u201d problem (bug) put as much information about the Issue as possible, e.g.,: What you are trying to achieve Command line you ran, e.g., > i lint -f defi/tulip/test/test_dao_cross_sol.py Copy-paste the error and the stack trace from the cmd line, no screenshots, e.g., Traceback (most recent call last): File \"/venv/bin/invoke\", line 8, in <module> sys.exit(program.run()) File \"/venv/lib/python3.8/site-packages/invoke/program.py\", line 373, in run self.parse_collection() ValueError: One and only one set-up config should be true: The log of the run Maybe the same run using -v DEBUG to get more info on the problem What the problem is Why the outcome is different from what you expected Use check boxes for \"small\" actions that need to be tracked in the issue (not worth their own bug) An issue should be closed only after all the checkboxes have been addressed We use the FYI @... syntax to add \"watchers\" E.g., FYI @cryptomtc so that he receives notifications for this issue Authors and assignees receive all the emails in any case In general everybody should be subscribed to receiving all the notifications and you can quickly go through them to know what's happening around you Assign an Issue to the right person for re-routing There should be a single assignee to a bug so we know who needs to do the work Assign GP/current RPs if not sure Assign an Issue to one of the pipelines, ideally based on the urgency If you are not sure, leave it unassigned but @tag GP / RPs to make sure we can take care of it Assign an Issue to the right Epic and Label Use Blocking label when an issue needs to be handled immediately, i.e. it prevents you from making progress If you are unsure then you can leave it empty, but @tag GP / RPs to make sure we can re-route and improve the Epics/Labels Updating an issue When you start working on an Issue, move it to the In Progress pipeline on ZH Try to use In Progress only for Issues you are actively working on A rule of thumb is that you should not have more than 2-3 In Progress Issues Give priority to Issues that are close to being completed, rather than starting a new Issue Update an Issue on GH often, like at least once a day of work Show the progress to the team with quick updates Update your Issue with pointers to gdocs, PRs, notebooks If you have questions, post them on the bug and tag people Once the task, in your opinion, is done, move an issue to Review/QA pipeline so that GP/RPs can review it If we decide to stop the work, add a Paused label and move it back to the backlog, e.g., Sprint backlog (P0) , Product backlog (P1) , Icebox (P2) Closing an issue A task is closed when PR has been reviewed and merged into master When, in your opinion, there is no more work to be done on your side on an Issue, please move it to the Done/Pre-prod or Review/QA pipeline, but do not close it GP/RPs will close it after review If you made specific assumptions, or if there are loose ends, etc., add a TODO(user) or file a follow-up bug Done means that something is DONE, not 99% done It means that the code is tested, readable and usable by other teammates Together we can decide that 99% done is good enough, but it should be a conscious decision and not come as a surprise PR workflows PR labels PR_for_authors There are changes to be addressed by an author of a PR PR_for_reviewers PR is ready for review by RPs PR_for_integrators PR is ready for the final round of review by GP, i.e. close to merge Filing a new PR General tips Implement a feature in a branch (not master), once it is ready for review push it and file a PR via GH interface We have invoke tasks to automate some of these tasks: ``` i git_create_branch -i 828 i git_create_branch -b Cmamp723_hello_world i gh_create_pr ``` If you want to make sure you are going in a right direction or just to confirm the interfaces you can also file a PR to discuss Mark PR as draft if it is not ready, use the convert to draft button Filing process Add a description to help reviewers to understand what it is about and what you want the focus to be Add a pointer in the description to the issue that PR is related to - this will ease the GH navigation for you and reviewers Leave the assignee field empty This will be done by RPs Add reviewers to the reviewers list For optional review just do @FYI person_name in the description Add a corresponding label Usually the first label in the filed PR is PR_for_reviewers If it is urgent/blocking, use the Blocking label Make sure that the corresponding tests pass Always lint before asking for a review Link a PR to an issue via ZH plugin feature # TODO(Dan): Add a pic with example. If the output is a notebook: Publish a notebook, see here Attach a cmd line to open a published notebook, see here Review A reviewer should check the code: Architecture Conformity with specs Code style conventions Interfaces Mistakes Readability There are 2 possible outcomes of a review: There are changes to be addressed by author A reviewer leaves comments to the code Marks PR as PR_for_authors A PR is ready to be merged: Pass it to integrators and mark it as PR_for_integrators Usually is placed by RPs after they approve PR Addressing comment If the reviewer's comment is clear to the author and agreed upon: The author addresses the comment with a code change and after changing the code (everywhere the comment it applies) marks it as RESOLVED on the GH interface Here we trust the authors to do a good job and to not skip / lose comments If the comment needs further discussion, the author adds a note explaining why he/she disagrees and the discussion continues until consensus is reached Once all comments are addressed: Re-request the review Mark it as PR_for_reviewers Coverage reports in PRs - discussion We should start posting coverage reports in PRs. The suggested process is: PR\u2019s author posts coverage stats before (from master) and after the changes in the format below. The report should contain only the files that were touched in a PR. We have run_coverage_report invoke TODO(*): Enable for Sorrentum and add usage examples. Maybe we can automate it somehow, e.g., with GH actions. But we need to start from something. Name Stmts Miss Branch BrPart Cover ------------------------------------------------------------------------- oms/locates.py 7 7 2 0 0% oms/oms_utils.py 34 34 6 0 0% oms/tasks.py 3 3 0 0 0% oms/oms_lib_tasks.py 64 39 2 0 38% oms/order.py 101 30 22 0 64% oms/test/oms_db_helper.py 29 11 2 0 65% oms/api.py 154 47 36 2 70% oms/broker.py 200 31 50 9 81% oms/pnl_simulator.py 326 42 68 8 83% oms/place_orders.py 121 8 18 6 90% oms/portfolio.py 309 21 22 0 92% oms/oms_db.py 47 0 10 3 95% oms/broker_example.py 23 0 4 1 96% oms/mr_market.py 55 1 10 1 97% oms/__init__.py 0 0 0 0 100% oms/call_optimizer.py 31 0 0 0 100% oms/devops/__init__.py 0 0 0 0 100% oms/devops/docker_scripts/__init__.py 0 0 0 0 100% oms/order_example.py 26 0 0 0 100% oms/portfolio_example.py 32 0 0 0 100% ------------------------------------------------------------------------- TOTAL 1562 274 252 30 80% PR\u2019s author also sends a link to S3 with the full html report so that a reviewer can check that the new lines added are covered by the tests","title":"GitHub/ZenHub workflows"},{"location":"GitHub_ZenHub_workflows/#githubzenhub-workflows","text":"Introduction Concepts Epic Master Epics Sub-epics Issue Label Pipeline PR Issue workflows Filing a new issue Updating an issue Closing an issue PR workflows PR labels Filing a new PR General tips Filing process Review Addressing comment Coverage reports in PRs - discussion","title":"GitHub/ZenHub workflows"},{"location":"GitHub_ZenHub_workflows/#introduction","text":"In the following we use the abbreviations below: GH = GitHub ZH = ZenHub PR = Pull Request RP = Responsible party (Team Leader) Everything we work on comes as a GH task We file tasks and then prioritize and distribute the workload We try to always work on high priority (aka, P0) tasks Issues vs bugs vs tasks We call GH Issues \"issues\", and \"tasks\", (sometimes \"tickets\") interchangeably. We avoid to call them bugs since many times we use GH to track ideas, activities, and improvements, and not only defects in the code The best names are \"tasks\" and \"issues\" ZenHub for project management We use ZH as project management layer on top of GH Please install the ZH extension for GH. This is going to make your life easier","title":"Introduction"},{"location":"GitHub_ZenHub_workflows/#concepts","text":"","title":"Concepts"},{"location":"GitHub_ZenHub_workflows/#epic","text":"An Epic pulls together Issues that are somehow related by their topic We distinguish Master Epics (e.g., WEB3 ) and sub-Epics (e.g., WEB3 - DaoCross v0.1 ) See the current list of Epics on GH here We maintain all the information about what the Epic is about in its description","title":"Epic"},{"location":"GitHub_ZenHub_workflows/#master-epics","text":"Master Epics are long-running Epics (i.e., projects) E.g., WEB3 Each issue should belong to at least one Epic: either a sub-epic or a master Epic There is no need to add an issue to a Master Epic if it is already added to a sub-epic","title":"Master Epics"},{"location":"GitHub_ZenHub_workflows/#sub-epics","text":"Master Epics can be broken down into smaller Epics (=sub-epics) E.g., WEB3 - DaoCross v0.1 Their titles should follow the pattern: XYZ - ABC v* , where: XYZ - master Epic title ABC - sub-epic title v* - version Epics and sub-epics are typed as EPIC - Sub-Epic , i.e., the master epic is capitalized and the sub-epics are capitalized lower-case Sub-epics should have a short title and a smaller scope Some sub-epics are related to short term milestones or releases (e.g., WEB3 - DaoCross - v0.1 ), other sub-epics are for long-running activities (e.g., WEB3 - Good First Issue ) Sub-epics should belong to a Master Epic in ZH A sub-epic can be moved to Done/Pre-prod only if all issues nested in it are moved to Done/Pre-prod","title":"Sub-epics"},{"location":"GitHub_ZenHub_workflows/#issue","text":"Issue is a piece of work to be done. Issues are combined into Epics by topic An issue has certain characteristics, i.e. labels An issue has a progress status, i.e. ZH pipeline (e.g., Product backlog (P1) , In progress , Done/Pre-prod ) PRs are linked to work needed to complete an issue","title":"Issue"},{"location":"GitHub_ZenHub_workflows/#label","text":"Labels are attributes of an issue (or PR), e.g., good first issue , PR_for_reviewers , duplicate , etc. See the current list of labels and their descriptions here","title":"Label"},{"location":"GitHub_ZenHub_workflows/#pipeline","text":"A ZH Pipeline represents the \"progress\" status of an Issue in our process We have the following Pipelines on the ZH board: New Issues Any new GH Issue with unclear Epic / Pipeline goes here Icebox (P2) Low priority, unprioritized issues Product Backlog (P1) Issues of medium priority at the moment Background Tasks one can do in background, e.g. reading, updating documentation, etc. Sprint backlog (P0) Issues of high priority at the moment In Progress Issues that we are currently working on Review/QA Issues opened for review and testing Code is ready to be deployed pending feedback Done/Pre-prod Issues that are done and are waiting for closing Epics Both Master Epics and Sub-epics Closed Issues that are done and don't need a follow-up GP/RPs are responsible for closing","title":"Pipeline"},{"location":"GitHub_ZenHub_workflows/#pr","text":"A pull request is an event where a contributor asks to review code they want to merge into a project","title":"PR"},{"location":"GitHub_ZenHub_workflows/#issue-workflows","text":"","title":"Issue workflows"},{"location":"GitHub_ZenHub_workflows/#filing-a-new-issue","text":"Use an informative description (typically an action \"Do this and that\") We don't use a period at the end of the title If it is a \u201cserious\u201d problem (bug) put as much information about the Issue as possible, e.g.,: What you are trying to achieve Command line you ran, e.g., > i lint -f defi/tulip/test/test_dao_cross_sol.py Copy-paste the error and the stack trace from the cmd line, no screenshots, e.g., Traceback (most recent call last): File \"/venv/bin/invoke\", line 8, in <module> sys.exit(program.run()) File \"/venv/lib/python3.8/site-packages/invoke/program.py\", line 373, in run self.parse_collection() ValueError: One and only one set-up config should be true: The log of the run Maybe the same run using -v DEBUG to get more info on the problem What the problem is Why the outcome is different from what you expected Use check boxes for \"small\" actions that need to be tracked in the issue (not worth their own bug) An issue should be closed only after all the checkboxes have been addressed We use the FYI @... syntax to add \"watchers\" E.g., FYI @cryptomtc so that he receives notifications for this issue Authors and assignees receive all the emails in any case In general everybody should be subscribed to receiving all the notifications and you can quickly go through them to know what's happening around you Assign an Issue to the right person for re-routing There should be a single assignee to a bug so we know who needs to do the work Assign GP/current RPs if not sure Assign an Issue to one of the pipelines, ideally based on the urgency If you are not sure, leave it unassigned but @tag GP / RPs to make sure we can take care of it Assign an Issue to the right Epic and Label Use Blocking label when an issue needs to be handled immediately, i.e. it prevents you from making progress If you are unsure then you can leave it empty, but @tag GP / RPs to make sure we can re-route and improve the Epics/Labels","title":"Filing a new issue"},{"location":"GitHub_ZenHub_workflows/#updating-an-issue","text":"When you start working on an Issue, move it to the In Progress pipeline on ZH Try to use In Progress only for Issues you are actively working on A rule of thumb is that you should not have more than 2-3 In Progress Issues Give priority to Issues that are close to being completed, rather than starting a new Issue Update an Issue on GH often, like at least once a day of work Show the progress to the team with quick updates Update your Issue with pointers to gdocs, PRs, notebooks If you have questions, post them on the bug and tag people Once the task, in your opinion, is done, move an issue to Review/QA pipeline so that GP/RPs can review it If we decide to stop the work, add a Paused label and move it back to the backlog, e.g., Sprint backlog (P0) , Product backlog (P1) , Icebox (P2)","title":"Updating an issue"},{"location":"GitHub_ZenHub_workflows/#closing-an-issue","text":"A task is closed when PR has been reviewed and merged into master When, in your opinion, there is no more work to be done on your side on an Issue, please move it to the Done/Pre-prod or Review/QA pipeline, but do not close it GP/RPs will close it after review If you made specific assumptions, or if there are loose ends, etc., add a TODO(user) or file a follow-up bug Done means that something is DONE, not 99% done It means that the code is tested, readable and usable by other teammates Together we can decide that 99% done is good enough, but it should be a conscious decision and not come as a surprise","title":"Closing an issue"},{"location":"GitHub_ZenHub_workflows/#pr-workflows","text":"","title":"PR workflows"},{"location":"GitHub_ZenHub_workflows/#pr-labels","text":"PR_for_authors There are changes to be addressed by an author of a PR PR_for_reviewers PR is ready for review by RPs PR_for_integrators PR is ready for the final round of review by GP, i.e. close to merge","title":"PR labels"},{"location":"GitHub_ZenHub_workflows/#filing-a-new-pr","text":"","title":"Filing a new PR"},{"location":"GitHub_ZenHub_workflows/#general-tips","text":"Implement a feature in a branch (not master), once it is ready for review push it and file a PR via GH interface We have invoke tasks to automate some of these tasks: ``` i git_create_branch -i 828 i git_create_branch -b Cmamp723_hello_world i gh_create_pr ``` If you want to make sure you are going in a right direction or just to confirm the interfaces you can also file a PR to discuss Mark PR as draft if it is not ready, use the convert to draft button","title":"General tips"},{"location":"GitHub_ZenHub_workflows/#filing-process","text":"Add a description to help reviewers to understand what it is about and what you want the focus to be Add a pointer in the description to the issue that PR is related to - this will ease the GH navigation for you and reviewers Leave the assignee field empty This will be done by RPs Add reviewers to the reviewers list For optional review just do @FYI person_name in the description Add a corresponding label Usually the first label in the filed PR is PR_for_reviewers If it is urgent/blocking, use the Blocking label Make sure that the corresponding tests pass Always lint before asking for a review Link a PR to an issue via ZH plugin feature # TODO(Dan): Add a pic with example. If the output is a notebook: Publish a notebook, see here Attach a cmd line to open a published notebook, see here","title":"Filing process"},{"location":"GitHub_ZenHub_workflows/#review","text":"A reviewer should check the code: Architecture Conformity with specs Code style conventions Interfaces Mistakes Readability There are 2 possible outcomes of a review: There are changes to be addressed by author A reviewer leaves comments to the code Marks PR as PR_for_authors A PR is ready to be merged: Pass it to integrators and mark it as PR_for_integrators Usually is placed by RPs after they approve PR","title":"Review"},{"location":"GitHub_ZenHub_workflows/#addressing-comment","text":"If the reviewer's comment is clear to the author and agreed upon: The author addresses the comment with a code change and after changing the code (everywhere the comment it applies) marks it as RESOLVED on the GH interface Here we trust the authors to do a good job and to not skip / lose comments If the comment needs further discussion, the author adds a note explaining why he/she disagrees and the discussion continues until consensus is reached Once all comments are addressed: Re-request the review Mark it as PR_for_reviewers","title":"Addressing comment"},{"location":"GitHub_ZenHub_workflows/#coverage-reports-in-prs-discussion","text":"We should start posting coverage reports in PRs. The suggested process is: PR\u2019s author posts coverage stats before (from master) and after the changes in the format below. The report should contain only the files that were touched in a PR. We have run_coverage_report invoke TODO(*): Enable for Sorrentum and add usage examples. Maybe we can automate it somehow, e.g., with GH actions. But we need to start from something. Name Stmts Miss Branch BrPart Cover ------------------------------------------------------------------------- oms/locates.py 7 7 2 0 0% oms/oms_utils.py 34 34 6 0 0% oms/tasks.py 3 3 0 0 0% oms/oms_lib_tasks.py 64 39 2 0 38% oms/order.py 101 30 22 0 64% oms/test/oms_db_helper.py 29 11 2 0 65% oms/api.py 154 47 36 2 70% oms/broker.py 200 31 50 9 81% oms/pnl_simulator.py 326 42 68 8 83% oms/place_orders.py 121 8 18 6 90% oms/portfolio.py 309 21 22 0 92% oms/oms_db.py 47 0 10 3 95% oms/broker_example.py 23 0 4 1 96% oms/mr_market.py 55 1 10 1 97% oms/__init__.py 0 0 0 0 100% oms/call_optimizer.py 31 0 0 0 100% oms/devops/__init__.py 0 0 0 0 100% oms/devops/docker_scripts/__init__.py 0 0 0 0 100% oms/order_example.py 26 0 0 0 100% oms/portfolio_example.py 32 0 0 0 100% ------------------------------------------------------------------------- TOTAL 1562 274 252 30 80% PR\u2019s author also sends a link to S3 with the full html report so that a reviewer can check that the new lines added are covered by the tests","title":"Coverage reports in PRs - discussion"},{"location":"Git_workflow_and_best_practices/","text":"Git workflow and best practices Before you start Readings Workflow Best Practices Do not check in large data files Branch workflow best practices Branches are cheap Always work in a branch Keep different changes in separate branches Pull request (PR) best practices Workflow diagram Deleting a branch How-to and troubleshooting Do not mess up your branch Analyzing commits Show files modified in a commit Conflicts Getting the conflicting files Accepting \"theirs\" How to get out of a messy/un-mergeable branch Reverting Reverting the last local commit Branching Checking what work has been done in a branch Checking if you need to merge master into your feature branch Comparing the difference of a directory among branches Merging master Rebasing Merging pull requests Submodules Adding a submodule Working in a submodule Updating a submodule to the latest commit To check if supermodule and amp are in sync Roll forward git submodules pointers: To clean all the repos Pull a branch without checkout To force updating all the submodules Before you start GitHub is the place where we keep our code git is the tool (program) for version control We interact with GitHub via git Use public key for authorization You can add a new public key here GH -> Personal settings -> SSH keys More details about what is public key you can find in ssh.md Readings Read at least the first 3 chapters of Git book Read about Git Submodules We use Git submodules to compose and share code about repos Workflow Run git fetch ``` # Fetch all the data from origin. git fetch # List all the branches. git branch -r origin/HEAD -> origin/master origin/PTask274 ... ``` Checkout master and pull You want to branch from the latest version of master to avoid a merge: `` # Checkout the master` branch. > git checkout master Make sure your local master is in sync with the remote. git pull --rebase - Alternatively, and especially if you have local changes to move to a new branch, run git checkout master i git_pull ``` Name a branch after its corresponding issue The canonical name for a new feature branch is obtained by running i gh_issue_title : ``` > i gh_issue_title -i 274 INFO: > cmd='/Users/saggese/src/venv/amp.client_venv/bin/invoke gh_issue_title -i 274' report_memory_usage=False report_cpu_usage=False ## gh_issue_title: issue_id='274', repo_short_name='current' ## gh_login: 07:35:54 - INFO lib_tasks_gh.py gh_login:48 account='sorrentum' export GIT_SSH_COMMAND='ssh -i /Users/saggese/.ssh/id_rsa.sorrentum.github' gh auth login --with-token </Users/saggese/.ssh/github_pat.sorrentum.txt Copied to system clipboard: CmTask274_Update_names: https://github.com/sorrentum/sorrentum/pull/274 - Before running verify that GitHub cli `gh` works gh --version gh version 2.29.0 (2023-05-10) https://github.com/cli/cli/releases/tag/v2.29.0 `` - The name is CmTask274_Update_names - To use multiple branches for a given task, append a numeral to the name, e.g., CmTask274_Update_names_v02` Create and checkout the branch ``` git branch my_feature git checkout my_feature ``` Alternatively, you can create and checkout in one command with > git checkout -b my_feature From this point on, you commit only in the branch and changes to master will not affect your branch If the branch already exists, check out the branch by executing > git checkout my_feature Commit your work early and often Commits on your feature branch do not affect master. Checkpoint your work regularly by committing: > git status On branch my_feature > git add ... > git commit [my_feature 820b296] My feature is awesome! Commits stay local (not seen on GitHub) until you explicitly tell git to \"upload\" the commits through git push (see next) Push your feature branch changes upstream When you commit, commits are local (not seen on GitHub) When you want your code to be pushed to the server (e.g., to back up or to share the changes with someone else), you need to push the branch upstream > git push -u origin my_feature ... 30194fc..820b296 my_feature -> my_feature Branch 'my_feature' set up to track remote branch 'my_feature' from 'origin'. Note that -u tells git to set the upstream of this branch to origin This operation is needed only the first time you create the branch and not for each git push Merge master into your feature branch regularly Merge master into your feature branch at least once a day, if the branch stays around that long: # Get the .git from the server > git fetch # Make your master up to origin/master. > git checkout master > git pull # Merge master into my_feature branch. > git checkout my_feature > git merge master A simpler flow which should be equivalent TODO(gp): Verify that ``` # Get the .git from the server git fetch # Merge master into my_feature branch. git checkout my_feature git merge origin/master ``` Repeat Steps 4-7 as needed Request a review of your work by making a pull request (PR) Verify that your work is ready for a review by going through this checklist: The PR is self-contained The latest master has been merged into the feature branch All files in the PR have been linted with linter.py All tests pass If your work is ready for review, make a pull request Use the GitHub UI (for now; we may replace with a script). Go to the branch on the web interface and push \"Compare & pull request\" Make sure that GP and Paul are assigned as reviewers, as well as anyone else who may be interested Make sure that GP and Paul are assigned as assignees Follow up on all comments and mark as resolved any requested changes that you resolve Best Practices Do not check in large data files Avoid checking in large data files The reason is that large files bloat the repo Once a large file is checked in, it never goes away Therefore, DO NOT CHECK IN DATA FILES IN EXCESS OF 500K If in doubt (even on a branch), ask first! Sometimes is makes sense to check in some representative data for unit tests BUT, larger tests should obtain their data from s3 or MongoDB Branch workflow best practices Branches are cheap One of the advantages of working with Git is that branches are cheap master is sacred In an ideal world master branch is sacred (see Platinum rule of Git) Development should never be done directly on master Changes to master should only happen by pull-request or merge One should avoid working in master except in rare cases, e.g., a simple urgent bug-fix needed to unblock people master should be always never broken (all tests are passing and it is deployable) Always work in a branch Generally it is best to be the sole contributor to your branch If you need to collaborate with somebody on a branch, remember that the golden rule of rebase still applies to this \"public\" branch: \"do not rebase pushed commits\" It is ok to open multiple branches for a given task if needed E.g., if you have multiple chunks of work or multiple people are working on orthogonal changes It might be that the task is too big and needs to be broken in smaller bugs All the rules that apply to master apply also to a branch E.g., commit often, use meaningful commit messages. We are ok with a little looser attitude in your branch E.g., it might be ok to not run unit tests before each commit, but be careful! Use a branch even if working on a research notebook Try to avoid modifying notebooks in multiple branches simultaneously, since notebook merges can be painful Working in a branch in this case facilitates review Working in a branch protects the codebase from accidental pushes of code changes outside of the notebook (e.g., hacks to get the notebook working that need to be cleaned up) Keep different changes in separate branches It is easier for you to keep work sane and separated Cons of multiple conceptual changes in the same branches You are testing / debugging all at once which might make your life more difficult Reviewing unrelated changes slows down the review process Packaging unrelated changes together that means no change gets merged until all of the changes are accepted Pull request (PR) best practices Make sure your PR is coherent It may not need to do everything the Task requires, but the PR should be self-contained and not break anything If you absolutely need changes under review to keep going, create the new branch from the old branch rather than from master (less ideal) Try to avoid branching from branches This creates also dependencies on the order of committing branches You end up with a spiderweb of branches Frequent small PRs are easier to review You will also experience faster review turnaround Reviewers like working on smaller changes more than working on larger ones PR review time does not scale linearly with lines changed (may be more like exponential) Merging changes frequently means other people can more easily see how the code is progressing earlier on in the process, and give you feedback E.g., \"here it is a much simpler way of doing this\", or even better \"you don't need to write any code, just do <this_and_that>\" Merged changes are tested in the Jenkins build Workflow diagram Deleting a branch You can run the script dev_scripts/git/git_branch.sh to get all the branches together with some information, e.g., last commit and creator E.g., let's assume we believe that PTask354_INFRA_Populate_S3_bucket is obsolete and we want to delete it: Get master up to date > git checkout master > git fetch > git pull Merge master into the target branch Pull and merge > git checkout PTask354_INFRA_Populate_S3_bucket > git pull > git merge master Resolve conflicts > git commit > git pull Ask Git if the branch is merged One approach is to ask Git if all the changes in master are also in the branch ``` git branch PTask354_INFRA_Populate_S3_bucket --no-merged PTask354_INFRA_Populate_S3_bucket ``` Note that Git is very strict here, e.g., PTask354_INFRA_Populate_S3_bucket is not completely merged since I've moved code \"manually\" (not only through git cherry-pick, git merge ) One approach is to just merge PTask354_INFRA_Populate_S3_bucket into master and run git branch again Manually check if there is any textual difference Another approach is to check what the differences are between the branch and origin/master ``` git log master..HEAD 6465b0c saggese, 25 seconds ago : Merge branch 'master' into PTask354_INFRA_Populate_S3_bucket (HEAD -> PTask354_INFRA_Populate_S3_bucket, origin/PTask354_INFRA_Populate_S3_bucket) git log HEAD..master ``` Here we see that there are no textual differences So we can either merge the branch into master or just kill directly Kill-kill-kill! To delete both the local and remote branch you can do ``` git branch -d PTask354_INFRA_Populate_S3_bucket git push origin --delete PTask354_INFRA_Populate_S3_bucket ``` How-to and troubleshooting Do not mess up your branch If you are working in a branch, before doing git push make sure the branch is not broken (e.g., from a mistake in merge / rebase mess) A way to check that the branch is sane is the following: Make sure that you don't have extra commits in your branch: The difference between your branch and master bash > git fetch > git checkout &lt;BRANCH> > git log origin/master..HEAD shows only commits made by you or, if you are not the only one working on the branch, only commits belonging to the branch with the same PTaskXYZ E.g., if George is working on PTask275 and sees that something funny is going on: a379826 Ringo, 3 weeks ago : LemTask54: finish dataflow through cross-validation 33a46b2 George, 2 weeks ago : PTask275 Move class attributes docstrings to init, change logging Make sure the files modified in your branch are only the file you expect to be modified > git fetch > git checkout &lt;BRANCH> > git diff --name-only master..HEAD If you see that there is a problem, don't push upstream (because the branch will be broken for everybody) and ask a Git expert Analyzing commits Show files modified in a commit You can see the files modified in a given commit hash with: ``` git show --name-only $HASH ``` E.g., ``` git show --name-only 39a9e335298a3fe604896fa19296d20829801cf2 commit 39a9e335298a3fe604896fa19296d20829801cf2 Author: Julia <julia@...> Date: Fri Sep 27 11:43:41 2019 PTask274 lint vendors/cme/utils.py vendors/first_rate/utils.py ``` Conflicts Getting the conflicting files To see the files in conflicts git diff --name-only --diff-filter=U This is what the script git_conflict_files.sh does Accepting \"theirs\" > git checkout --theirs $FILES > git add $FILES TODO(gp): Fix this ours and theirs. The first option represents the current branch from which you executed the command before getting the conflicts, and the second option refers to the branch where the changes are coming from. ``` git show :1:README git show :2:README git show :3:README ``` Stage #1 is the common ancestor of the files, stage #2 is the target-branch version, and stage #3 is the version you are merging from. How to get out of a messy/un-mergeable branch If one screws up a branch: Rebase to master Resolve the conflicts E.g., pick the master version when needed: git checkout --theirs ...; git add ... Diff the changes in the branch vs another client at master > diff_to_vimdiff.py --dir1 $DIR1/amp --dir2 $DIR2/amp --skip_vim Saving log to file '/Users/saggese/src/...2/amp/dev_scripts/diff_to_vimdiff.py.log' 10-06_15:22 INFO : _parse_diff_output:36 : Reading '/tmp/tmp.diff_to_vimdiff.txt' # DIFF: README.md # DIFF: core/dataflow.py # DIFF: core/dataflow_core.py # DIFF: core/test/test_core.py # DIFF: dev_scripts/diff_to_vimdiff.py # ONLY: diff_to_vimdiff.py.log in $DIR1/dev_scripts # DIFF: dev_scripts/grc # ONLY: code_style.txt in $DIR2/docs/notes ... # DIFF: vendors/test/test_vendors.py Diff / merge manually the files that are different > diff_to_vimdiff.py --dir1 $DIR1/...2/amp --dir2 $DIR2/...3/amp --skip_vim > --only_diff_content # DIFF: README.md # DIFF: core/dataflow.py # DIFF: core/dataflow_core.py # DIFF: core/test/test_core.py ... Reverting Reverting the last local commit > git reset --soft HEAD~ Branching Checking what work has been done in a branch Look at all the branches available: ``` # Fetch all the data from origin. git fetch # List all the branches. git branch -r origin/HEAD -> origin/master origin/PTask274 ... ``` Go to the branch: ``` git checkout PTask274 ``` Check what are the commits that are in the current branch HEAD but not in master : ``` gll master..HEAD git log master..HEAD eb12233 Julia PTask274 verify dataset integrity ( 13 hours ago) Sat Sep 28 18:55:12 2019 (HEAD -> PTask274, origin/PTask274) ... a637594 saggese PTask274: Add tag for review ( 3 days ago) Thu Sep 26 17:13:33 2019 ``` To see the actual changes in a branch you can't do (Bad) \\ > git diff master..HEAD since git diff compares two commits and not a range of commits like git log (yes, Git has a horrible API) What you need to do is to get the first commit in the branch and the last from git log and compare them: ``` git difftool a637594..eb12233 gd a637594..eb12233 ``` Checking if you need to merge master into your feature branch You can see what commits are in master but missing in your branch with: ``` gll ..master de51a7c saggese Improve script to help diffing trees in case of difficult merges. Add notes from reviews ( 5 hours ago) Sat Oct 5 11:24:11 2019 (origin/master, origin/HEAD, master) 8acd60c saggese Add a basic end-to-end unit test for the linter ( 19 hours ago) Fri Oct 4 21:28:09 2019 \u2026 ``` You want to rebase your feature branch onto master Comparing the difference of a directory among branches This is useful if we want to focus on changes on a single dir ``` git ll master..PTask274 vendors/cme 39a9e33 Julia PTask274 lint ( 2 days ago) Fri Sep 27 11:43:41 2019 c8e7e1a Julia PTask268 modify according to review16 ( 2 days ago) Fri Sep 27 11:41:47 2019 a637594 saggese PTask274: Add tag for review ( 3 days ago) Thu Sep 26 17:13:33 2019 git diff --name-only a637594..33a46b2 -- vendors helpers helpers/csv.py vendors/cme/utils.py vendors/first_rate/Task274_verify_datasets.ipynb vendors/first_rate/Task274_verify_datasets.py vendors/first_rate/reader.py vendors/first_rate/utils.py vendors/test/test_vendors.py ``` Merging master If your branch lives long, you want to apply changes made on master to show on your branch Merge flow Assume your branch is clean E.g., everything is committed, or stashed Pull changes from master on the remote repo ``` git checkout master git pull ``` Checkout your feature branch ``` git checkout my_feature ``` Merge stuff from master to my_feature ``` git merge master --no-ff ... editor will open a message for the merge commit ... ``` In few informal words, the --no-ff option means that commits are not \"inlined\" (similar to rebase) even if possible, but a merge commit is always used The problem is that if the commits are \"inlined\" then you can't revert the change in one shot like we would do for a merge commit, but you need to revert all the inlined changes Rebasing For now, we suggest avoiding the rebase flow The reason is that rebase makes things cleaner when used properly, but can get you into deep trouble if not used properly You can rebase onto master , i.e., you re-apply your changes to master Not the other way around: that would be a disaster! ``` git checkout my_feature # See that you have that master doesn't have. git ll origin/master.. # See that master has that you don't have. git ll ..origin/master git rebase master git ll ..origin/master # Now you see that there is nothing in master you don't have git ll origin/master.. # You can see that you are ahead of master ``` Merging pull requests The procedure for manual merges is as follows Do not merge yourself unless explicitly requested by a reviewer Pull changes from remote master branch ``` git checkout master git pull ``` Merge your branch into master without fast-forward ``` git merge --no-ff my_feature ``` Push the newly merged master ``` git push ``` Delete the branch, if you are done with it: ``` git branch -d my_feature ``` Submodules Adding a submodule Following the instructions in https://git-scm.com/book/en/v2/Git-Tools-Submodules Working in a submodule When you work in a submodule, the flow should be like: Create a branch in a submodule Do your job Push the submodule branch Create a PR in the submodule when you are done Updating a submodule to the latest commit After the submodule PR is merged: Checkout the submodule in the master branch and do git pull In the main repo, create a branch like PTask1234_update_submodule From the new branch do git add &lt;submodule_name> , e.g., git add amp Commit changes, push Create a PR To check if supermodule and amp are in sync Run the script: ``` dev_scripts/git/git_submodules_are_updated.sh ``` Roll forward git submodules pointers: Run the script: ``` dev_scripts/git/git_submodules_roll_fwd.sh ``` To clean all the repos > git submodule foreach git clean -fd Pull a branch without checkout This is useful when merging master in a different branch and we don't want to checkout master just to pull ``` git fetch origin master:master ``` To force updating all the submodules Run the script > dev_scripts/git/git_submodules_pull.sh or ``` git submodule update --init --recursive` git submodule foreach git pull --autostash ```","title":"Git workflow and best practices"},{"location":"Git_workflow_and_best_practices/#git-workflow-and-best-practices","text":"Before you start Readings Workflow Best Practices Do not check in large data files Branch workflow best practices Branches are cheap Always work in a branch Keep different changes in separate branches Pull request (PR) best practices Workflow diagram Deleting a branch How-to and troubleshooting Do not mess up your branch Analyzing commits Show files modified in a commit Conflicts Getting the conflicting files Accepting \"theirs\" How to get out of a messy/un-mergeable branch Reverting Reverting the last local commit Branching Checking what work has been done in a branch Checking if you need to merge master into your feature branch Comparing the difference of a directory among branches Merging master Rebasing Merging pull requests Submodules Adding a submodule Working in a submodule Updating a submodule to the latest commit To check if supermodule and amp are in sync Roll forward git submodules pointers: To clean all the repos Pull a branch without checkout To force updating all the submodules","title":"Git workflow and best practices"},{"location":"Git_workflow_and_best_practices/#before-you-start","text":"GitHub is the place where we keep our code git is the tool (program) for version control We interact with GitHub via git Use public key for authorization You can add a new public key here GH -> Personal settings -> SSH keys More details about what is public key you can find in ssh.md","title":"Before you start"},{"location":"Git_workflow_and_best_practices/#readings","text":"Read at least the first 3 chapters of Git book Read about Git Submodules We use Git submodules to compose and share code about repos","title":"Readings"},{"location":"Git_workflow_and_best_practices/#workflow","text":"Run git fetch ``` # Fetch all the data from origin. git fetch # List all the branches. git branch -r origin/HEAD -> origin/master origin/PTask274 ... ``` Checkout master and pull You want to branch from the latest version of master to avoid a merge: `` # Checkout the master` branch. > git checkout master","title":"Workflow"},{"location":"Git_workflow_and_best_practices/#make-sure-your-local-master-is-in-sync-with-the-remote","text":"git pull --rebase - Alternatively, and especially if you have local changes to move to a new branch, run git checkout master i git_pull ``` Name a branch after its corresponding issue The canonical name for a new feature branch is obtained by running i gh_issue_title : ``` > i gh_issue_title -i 274 INFO: > cmd='/Users/saggese/src/venv/amp.client_venv/bin/invoke gh_issue_title -i 274' report_memory_usage=False report_cpu_usage=False ## gh_issue_title: issue_id='274', repo_short_name='current' ## gh_login: 07:35:54 - INFO lib_tasks_gh.py gh_login:48 account='sorrentum' export GIT_SSH_COMMAND='ssh -i /Users/saggese/.ssh/id_rsa.sorrentum.github' gh auth login --with-token </Users/saggese/.ssh/github_pat.sorrentum.txt","title":"Make sure your local master is in sync with the remote."},{"location":"Git_workflow_and_best_practices/#copied-to-system-clipboard","text":"CmTask274_Update_names: https://github.com/sorrentum/sorrentum/pull/274 - Before running verify that GitHub cli `gh` works gh --version gh version 2.29.0 (2023-05-10) https://github.com/cli/cli/releases/tag/v2.29.0 `` - The name is CmTask274_Update_names - To use multiple branches for a given task, append a numeral to the name, e.g., CmTask274_Update_names_v02` Create and checkout the branch ``` git branch my_feature git checkout my_feature ``` Alternatively, you can create and checkout in one command with > git checkout -b my_feature From this point on, you commit only in the branch and changes to master will not affect your branch If the branch already exists, check out the branch by executing > git checkout my_feature Commit your work early and often Commits on your feature branch do not affect master. Checkpoint your work regularly by committing: > git status On branch my_feature > git add ... > git commit [my_feature 820b296] My feature is awesome! Commits stay local (not seen on GitHub) until you explicitly tell git to \"upload\" the commits through git push (see next) Push your feature branch changes upstream When you commit, commits are local (not seen on GitHub) When you want your code to be pushed to the server (e.g., to back up or to share the changes with someone else), you need to push the branch upstream > git push -u origin my_feature ... 30194fc..820b296 my_feature -> my_feature Branch 'my_feature' set up to track remote branch 'my_feature' from 'origin'. Note that -u tells git to set the upstream of this branch to origin This operation is needed only the first time you create the branch and not for each git push Merge master into your feature branch regularly Merge master into your feature branch at least once a day, if the branch stays around that long: # Get the .git from the server > git fetch # Make your master up to origin/master. > git checkout master > git pull # Merge master into my_feature branch. > git checkout my_feature > git merge master A simpler flow which should be equivalent TODO(gp): Verify that ``` # Get the .git from the server git fetch # Merge master into my_feature branch. git checkout my_feature git merge origin/master ``` Repeat Steps 4-7 as needed Request a review of your work by making a pull request (PR) Verify that your work is ready for a review by going through this checklist: The PR is self-contained The latest master has been merged into the feature branch All files in the PR have been linted with linter.py All tests pass If your work is ready for review, make a pull request Use the GitHub UI (for now; we may replace with a script). Go to the branch on the web interface and push \"Compare & pull request\" Make sure that GP and Paul are assigned as reviewers, as well as anyone else who may be interested Make sure that GP and Paul are assigned as assignees Follow up on all comments and mark as resolved any requested changes that you resolve","title":"Copied to system clipboard:"},{"location":"Git_workflow_and_best_practices/#best-practices","text":"","title":"Best Practices"},{"location":"Git_workflow_and_best_practices/#do-not-check-in-large-data-files","text":"Avoid checking in large data files The reason is that large files bloat the repo Once a large file is checked in, it never goes away Therefore, DO NOT CHECK IN DATA FILES IN EXCESS OF 500K If in doubt (even on a branch), ask first! Sometimes is makes sense to check in some representative data for unit tests BUT, larger tests should obtain their data from s3 or MongoDB","title":"Do not check in large data files"},{"location":"Git_workflow_and_best_practices/#branch-workflow-best-practices","text":"","title":"Branch workflow best practices"},{"location":"Git_workflow_and_best_practices/#branches-are-cheap","text":"One of the advantages of working with Git is that branches are cheap","title":"Branches are cheap"},{"location":"Git_workflow_and_best_practices/#master-is-sacred","text":"In an ideal world master branch is sacred (see Platinum rule of Git) Development should never be done directly on master Changes to master should only happen by pull-request or merge One should avoid working in master except in rare cases, e.g., a simple urgent bug-fix needed to unblock people master should be always never broken (all tests are passing and it is deployable)","title":"master is sacred"},{"location":"Git_workflow_and_best_practices/#always-work-in-a-branch","text":"Generally it is best to be the sole contributor to your branch If you need to collaborate with somebody on a branch, remember that the golden rule of rebase still applies to this \"public\" branch: \"do not rebase pushed commits\" It is ok to open multiple branches for a given task if needed E.g., if you have multiple chunks of work or multiple people are working on orthogonal changes It might be that the task is too big and needs to be broken in smaller bugs All the rules that apply to master apply also to a branch E.g., commit often, use meaningful commit messages. We are ok with a little looser attitude in your branch E.g., it might be ok to not run unit tests before each commit, but be careful! Use a branch even if working on a research notebook Try to avoid modifying notebooks in multiple branches simultaneously, since notebook merges can be painful Working in a branch in this case facilitates review Working in a branch protects the codebase from accidental pushes of code changes outside of the notebook (e.g., hacks to get the notebook working that need to be cleaned up)","title":"Always work in a branch"},{"location":"Git_workflow_and_best_practices/#keep-different-changes-in-separate-branches","text":"It is easier for you to keep work sane and separated Cons of multiple conceptual changes in the same branches You are testing / debugging all at once which might make your life more difficult Reviewing unrelated changes slows down the review process Packaging unrelated changes together that means no change gets merged until all of the changes are accepted","title":"Keep different changes in separate branches"},{"location":"Git_workflow_and_best_practices/#pull-request-pr-best-practices","text":"Make sure your PR is coherent It may not need to do everything the Task requires, but the PR should be self-contained and not break anything If you absolutely need changes under review to keep going, create the new branch from the old branch rather than from master (less ideal) Try to avoid branching from branches This creates also dependencies on the order of committing branches You end up with a spiderweb of branches Frequent small PRs are easier to review You will also experience faster review turnaround Reviewers like working on smaller changes more than working on larger ones PR review time does not scale linearly with lines changed (may be more like exponential) Merging changes frequently means other people can more easily see how the code is progressing earlier on in the process, and give you feedback E.g., \"here it is a much simpler way of doing this\", or even better \"you don't need to write any code, just do <this_and_that>\" Merged changes are tested in the Jenkins build","title":"Pull request (PR) best practices"},{"location":"Git_workflow_and_best_practices/#workflow-diagram","text":"","title":"Workflow diagram"},{"location":"Git_workflow_and_best_practices/#deleting-a-branch","text":"You can run the script dev_scripts/git/git_branch.sh to get all the branches together with some information, e.g., last commit and creator E.g., let's assume we believe that PTask354_INFRA_Populate_S3_bucket is obsolete and we want to delete it: Get master up to date > git checkout master > git fetch > git pull Merge master into the target branch Pull and merge > git checkout PTask354_INFRA_Populate_S3_bucket > git pull > git merge master Resolve conflicts > git commit > git pull Ask Git if the branch is merged One approach is to ask Git if all the changes in master are also in the branch ``` git branch PTask354_INFRA_Populate_S3_bucket --no-merged PTask354_INFRA_Populate_S3_bucket ``` Note that Git is very strict here, e.g., PTask354_INFRA_Populate_S3_bucket is not completely merged since I've moved code \"manually\" (not only through git cherry-pick, git merge ) One approach is to just merge PTask354_INFRA_Populate_S3_bucket into master and run git branch again Manually check if there is any textual difference Another approach is to check what the differences are between the branch and origin/master ``` git log master..HEAD 6465b0c saggese, 25 seconds ago : Merge branch 'master' into PTask354_INFRA_Populate_S3_bucket (HEAD -> PTask354_INFRA_Populate_S3_bucket, origin/PTask354_INFRA_Populate_S3_bucket) git log HEAD..master ``` Here we see that there are no textual differences So we can either merge the branch into master or just kill directly Kill-kill-kill! To delete both the local and remote branch you can do ``` git branch -d PTask354_INFRA_Populate_S3_bucket git push origin --delete PTask354_INFRA_Populate_S3_bucket ```","title":"Deleting a branch"},{"location":"Git_workflow_and_best_practices/#how-to-and-troubleshooting","text":"","title":"How-to and troubleshooting"},{"location":"Git_workflow_and_best_practices/#do-not-mess-up-your-branch","text":"If you are working in a branch, before doing git push make sure the branch is not broken (e.g., from a mistake in merge / rebase mess) A way to check that the branch is sane is the following: Make sure that you don't have extra commits in your branch: The difference between your branch and master bash > git fetch > git checkout &lt;BRANCH> > git log origin/master..HEAD shows only commits made by you or, if you are not the only one working on the branch, only commits belonging to the branch with the same PTaskXYZ E.g., if George is working on PTask275 and sees that something funny is going on: a379826 Ringo, 3 weeks ago : LemTask54: finish dataflow through cross-validation 33a46b2 George, 2 weeks ago : PTask275 Move class attributes docstrings to init, change logging Make sure the files modified in your branch are only the file you expect to be modified > git fetch > git checkout &lt;BRANCH> > git diff --name-only master..HEAD If you see that there is a problem, don't push upstream (because the branch will be broken for everybody) and ask a Git expert","title":"Do not mess up your branch"},{"location":"Git_workflow_and_best_practices/#analyzing-commits","text":"","title":"Analyzing commits"},{"location":"Git_workflow_and_best_practices/#show-files-modified-in-a-commit","text":"You can see the files modified in a given commit hash with: ``` git show --name-only $HASH ``` E.g., ``` git show --name-only 39a9e335298a3fe604896fa19296d20829801cf2 commit 39a9e335298a3fe604896fa19296d20829801cf2 Author: Julia <julia@...> Date: Fri Sep 27 11:43:41 2019 PTask274 lint vendors/cme/utils.py vendors/first_rate/utils.py ```","title":"Show files modified in a commit"},{"location":"Git_workflow_and_best_practices/#conflicts","text":"","title":"Conflicts"},{"location":"Git_workflow_and_best_practices/#getting-the-conflicting-files","text":"To see the files in conflicts git diff --name-only --diff-filter=U This is what the script git_conflict_files.sh does","title":"Getting the conflicting files"},{"location":"Git_workflow_and_best_practices/#accepting-theirs","text":"> git checkout --theirs $FILES > git add $FILES TODO(gp): Fix this ours and theirs. The first option represents the current branch from which you executed the command before getting the conflicts, and the second option refers to the branch where the changes are coming from. ``` git show :1:README git show :2:README git show :3:README ``` Stage #1 is the common ancestor of the files, stage #2 is the target-branch version, and stage #3 is the version you are merging from.","title":"Accepting \"theirs\""},{"location":"Git_workflow_and_best_practices/#how-to-get-out-of-a-messyun-mergeable-branch","text":"If one screws up a branch: Rebase to master Resolve the conflicts E.g., pick the master version when needed: git checkout --theirs ...; git add ... Diff the changes in the branch vs another client at master > diff_to_vimdiff.py --dir1 $DIR1/amp --dir2 $DIR2/amp --skip_vim Saving log to file '/Users/saggese/src/...2/amp/dev_scripts/diff_to_vimdiff.py.log' 10-06_15:22 INFO : _parse_diff_output:36 : Reading '/tmp/tmp.diff_to_vimdiff.txt' # DIFF: README.md # DIFF: core/dataflow.py # DIFF: core/dataflow_core.py # DIFF: core/test/test_core.py # DIFF: dev_scripts/diff_to_vimdiff.py # ONLY: diff_to_vimdiff.py.log in $DIR1/dev_scripts # DIFF: dev_scripts/grc # ONLY: code_style.txt in $DIR2/docs/notes ... # DIFF: vendors/test/test_vendors.py Diff / merge manually the files that are different > diff_to_vimdiff.py --dir1 $DIR1/...2/amp --dir2 $DIR2/...3/amp --skip_vim > --only_diff_content # DIFF: README.md # DIFF: core/dataflow.py # DIFF: core/dataflow_core.py # DIFF: core/test/test_core.py ...","title":"How to get out of a messy/un-mergeable branch"},{"location":"Git_workflow_and_best_practices/#reverting","text":"","title":"Reverting"},{"location":"Git_workflow_and_best_practices/#reverting-the-last-local-commit","text":"> git reset --soft HEAD~","title":"Reverting the last local commit"},{"location":"Git_workflow_and_best_practices/#branching","text":"","title":"Branching"},{"location":"Git_workflow_and_best_practices/#checking-what-work-has-been-done-in-a-branch","text":"Look at all the branches available: ``` # Fetch all the data from origin. git fetch # List all the branches. git branch -r origin/HEAD -> origin/master origin/PTask274 ... ``` Go to the branch: ``` git checkout PTask274 ``` Check what are the commits that are in the current branch HEAD but not in master : ``` gll master..HEAD git log master..HEAD eb12233 Julia PTask274 verify dataset integrity ( 13 hours ago) Sat Sep 28 18:55:12 2019 (HEAD -> PTask274, origin/PTask274) ... a637594 saggese PTask274: Add tag for review ( 3 days ago) Thu Sep 26 17:13:33 2019 ``` To see the actual changes in a branch you can't do (Bad) \\ > git diff master..HEAD since git diff compares two commits and not a range of commits like git log (yes, Git has a horrible API) What you need to do is to get the first commit in the branch and the last from git log and compare them: ``` git difftool a637594..eb12233 gd a637594..eb12233 ```","title":"Checking what work has been done in a branch"},{"location":"Git_workflow_and_best_practices/#checking-if-you-need-to-merge-master-into-your-feature-branch","text":"You can see what commits are in master but missing in your branch with: ``` gll ..master de51a7c saggese Improve script to help diffing trees in case of difficult merges. Add notes from reviews ( 5 hours ago) Sat Oct 5 11:24:11 2019 (origin/master, origin/HEAD, master) 8acd60c saggese Add a basic end-to-end unit test for the linter ( 19 hours ago) Fri Oct 4 21:28:09 2019 \u2026 ``` You want to rebase your feature branch onto master","title":"Checking if you need to merge master into your feature branch"},{"location":"Git_workflow_and_best_practices/#comparing-the-difference-of-a-directory-among-branches","text":"This is useful if we want to focus on changes on a single dir ``` git ll master..PTask274 vendors/cme 39a9e33 Julia PTask274 lint ( 2 days ago) Fri Sep 27 11:43:41 2019 c8e7e1a Julia PTask268 modify according to review16 ( 2 days ago) Fri Sep 27 11:41:47 2019 a637594 saggese PTask274: Add tag for review ( 3 days ago) Thu Sep 26 17:13:33 2019 git diff --name-only a637594..33a46b2 -- vendors helpers helpers/csv.py vendors/cme/utils.py vendors/first_rate/Task274_verify_datasets.ipynb vendors/first_rate/Task274_verify_datasets.py vendors/first_rate/reader.py vendors/first_rate/utils.py vendors/test/test_vendors.py ```","title":"Comparing the difference of a directory among branches"},{"location":"Git_workflow_and_best_practices/#merging-master","text":"If your branch lives long, you want to apply changes made on master to show on your branch Merge flow Assume your branch is clean E.g., everything is committed, or stashed Pull changes from master on the remote repo ``` git checkout master git pull ``` Checkout your feature branch ``` git checkout my_feature ``` Merge stuff from master to my_feature ``` git merge master --no-ff ... editor will open a message for the merge commit ... ``` In few informal words, the --no-ff option means that commits are not \"inlined\" (similar to rebase) even if possible, but a merge commit is always used The problem is that if the commits are \"inlined\" then you can't revert the change in one shot like we would do for a merge commit, but you need to revert all the inlined changes","title":"Merging master"},{"location":"Git_workflow_and_best_practices/#rebasing","text":"For now, we suggest avoiding the rebase flow The reason is that rebase makes things cleaner when used properly, but can get you into deep trouble if not used properly You can rebase onto master , i.e., you re-apply your changes to master Not the other way around: that would be a disaster! ``` git checkout my_feature # See that you have that master doesn't have. git ll origin/master.. # See that master has that you don't have. git ll ..origin/master git rebase master git ll ..origin/master # Now you see that there is nothing in master you don't have git ll origin/master.. # You can see that you are ahead of master ```","title":"Rebasing"},{"location":"Git_workflow_and_best_practices/#merging-pull-requests","text":"The procedure for manual merges is as follows Do not merge yourself unless explicitly requested by a reviewer Pull changes from remote master branch ``` git checkout master git pull ``` Merge your branch into master without fast-forward ``` git merge --no-ff my_feature ``` Push the newly merged master ``` git push ``` Delete the branch, if you are done with it: ``` git branch -d my_feature ```","title":"Merging pull requests"},{"location":"Git_workflow_and_best_practices/#submodules","text":"","title":"Submodules"},{"location":"Git_workflow_and_best_practices/#adding-a-submodule","text":"Following the instructions in https://git-scm.com/book/en/v2/Git-Tools-Submodules","title":"Adding a submodule"},{"location":"Git_workflow_and_best_practices/#working-in-a-submodule","text":"When you work in a submodule, the flow should be like: Create a branch in a submodule Do your job Push the submodule branch Create a PR in the submodule when you are done","title":"Working in a submodule"},{"location":"Git_workflow_and_best_practices/#updating-a-submodule-to-the-latest-commit","text":"After the submodule PR is merged: Checkout the submodule in the master branch and do git pull In the main repo, create a branch like PTask1234_update_submodule From the new branch do git add &lt;submodule_name> , e.g., git add amp Commit changes, push Create a PR","title":"Updating a submodule to the latest commit"},{"location":"Git_workflow_and_best_practices/#to-check-if-supermodule-and-amp-are-in-sync","text":"Run the script: ``` dev_scripts/git/git_submodules_are_updated.sh ```","title":"To check if supermodule and amp are in sync"},{"location":"Git_workflow_and_best_practices/#roll-forward-git-submodules-pointers","text":"Run the script: ``` dev_scripts/git/git_submodules_roll_fwd.sh ```","title":"Roll forward git submodules pointers:"},{"location":"Git_workflow_and_best_practices/#to-clean-all-the-repos","text":"> git submodule foreach git clean -fd","title":"To clean all the repos"},{"location":"Git_workflow_and_best_practices/#pull-a-branch-without-checkout","text":"This is useful when merging master in a different branch and we don't want to checkout master just to pull ``` git fetch origin master:master ```","title":"Pull a branch without checkout"},{"location":"Git_workflow_and_best_practices/#to-force-updating-all-the-submodules","text":"Run the script > dev_scripts/git/git_submodules_pull.sh or ``` git submodule update --init --recursive` git submodule foreach git pull --autostash ```","title":"To force updating all the submodules"},{"location":"Glossary/","text":"Glossary Meta Definitions Meta Keep the terms in alphabetical order People can add terms that they are not clear and others can add definitions Always use \"suggestion\" mode for adding new terms or new definitions Definitions Asset A financial instrument with an associated price that changes over time Aka: symbol, name, ticker E.g., bitcoin, ethereum, Apple stock (US equity), orange futures FM (Financial Instrument) Financial instruments are monetary contracts between parties GH (GitHub) HLD (High Level Design) Is a general system design and includes the description of the System architecture and design IM (Instrument Master) A software component that associates symbolic names to assets and their prices Integrator Someone on the team that is in charge of merging code to the main line of development Aka: master OHLCV bar An open-high-low-close chart (also OHLC ) is a type of chart typically used to illustrate movements in the price of a financial instrument over time OMS (Order Management System) A software component in charge of placing and monitoring trading orders to market or broker PR (Pull Request) Request to merge code in GitHub RP (Responsible Party) Someone on the team that helps following our process Aka: tech lead ZH (ZenHub) Our tool for project management","title":"Glossary"},{"location":"Glossary/#glossary","text":"Meta Definitions","title":"Glossary"},{"location":"Glossary/#meta","text":"Keep the terms in alphabetical order People can add terms that they are not clear and others can add definitions Always use \"suggestion\" mode for adding new terms or new definitions","title":"Meta"},{"location":"Glossary/#definitions","text":"Asset A financial instrument with an associated price that changes over time Aka: symbol, name, ticker E.g., bitcoin, ethereum, Apple stock (US equity), orange futures FM (Financial Instrument) Financial instruments are monetary contracts between parties GH (GitHub) HLD (High Level Design) Is a general system design and includes the description of the System architecture and design IM (Instrument Master) A software component that associates symbolic names to assets and their prices Integrator Someone on the team that is in charge of merging code to the main line of development Aka: master OHLCV bar An open-high-low-close chart (also OHLC ) is a type of chart typically used to illustrate movements in the price of a financial instrument over time OMS (Order Management System) A software component in charge of placing and monitoring trading orders to market or broker PR (Pull Request) Request to merge code in GitHub RP (Responsible Party) Someone on the team that helps following our process Aka: tech lead ZH (ZenHub) Our tool for project management","title":"Definitions"},{"location":"Gsheet_into_pandas/","text":"Connecting Google Sheets to Pandas Installing gspread-pandas Configuring gspread-pandas Using gspread-pandas Connecting Google Sheets to Pandas In order to load a google sheet into a pandas dataframe (or the other way around), one can use a library called gspread-pandas . Installing gspread-pandas The library should be automatically installed in your conda env The detailed instructions on how to install the library are located here: Installation/Usage . Configuring gspread-pandas Client credentials need to be generated by each user independently. The instructions are provided here . The process is not complicated but it's not obvious since you need to click around in the GUI Some gotchas: Make sure to act only under your ... account. project/product/application names don't really matter. It's convenient to pick a name connected to the project so that it's easy to find later. Whenever you are given a choice of making something generally accessible or accessible only to the members of your organization, choose making it accessible only to the members of your organization. When you are given a choice of time periods to create something for, choose the longest one. When you are given a choice between OAuth client ID and Service account , choose OAuth client ID . To use the library on the server, the downloaded JSON with the credentials needs to be stored on your laptop ```bash export SRC_FILE=\"~/Downloads/client_secret_4642711342-ib06g3lbv6pa4n622qusqrjk8j58o8k6.apps.googleusercontent.com.json\" export DST_DIR=\"~/.config/gspread_pandas\" export DST_FILE=\"$DST_DIR/google_secret.json\" mkdir -p $DST_DIR mv $SRC_FILE $DST_FILE chmod 600 $DST_FILE and copy that on the server, e.g., bash ssh research \"mkdir -p $DST_DIR\" scp $SRC_FILE research:$DST_FILE ssh research \"chmod 600 $DST_FILE\" ``` Using gspread-pandas The notebook with the usage example is located at amp/core/notebooks/gsheet_into_pandas_example.ipynb . The first time the library is used, it will asks the user for an authorization code. After the authorization code is provided for the first time, it won't be asked again. The official use documentation is provided here . Don't feel stupid if you need multiple iterations to get this stuff working Clicking on GUI is always a recipe for low productivity Go command line and vim!","title":"Gsheet into pandas"},{"location":"Gsheet_into_pandas/#connecting-google-sheets-to-pandas","text":"In order to load a google sheet into a pandas dataframe (or the other way around), one can use a library called gspread-pandas .","title":"Connecting Google Sheets to Pandas"},{"location":"Gsheet_into_pandas/#installing-gspread-pandas","text":"The library should be automatically installed in your conda env The detailed instructions on how to install the library are located here: Installation/Usage .","title":"Installing gspread-pandas"},{"location":"Gsheet_into_pandas/#configuring-gspread-pandas","text":"Client credentials need to be generated by each user independently. The instructions are provided here . The process is not complicated but it's not obvious since you need to click around in the GUI Some gotchas: Make sure to act only under your ... account. project/product/application names don't really matter. It's convenient to pick a name connected to the project so that it's easy to find later. Whenever you are given a choice of making something generally accessible or accessible only to the members of your organization, choose making it accessible only to the members of your organization. When you are given a choice of time periods to create something for, choose the longest one. When you are given a choice between OAuth client ID and Service account , choose OAuth client ID . To use the library on the server, the downloaded JSON with the credentials needs to be stored on your laptop ```bash export SRC_FILE=\"~/Downloads/client_secret_4642711342-ib06g3lbv6pa4n622qusqrjk8j58o8k6.apps.googleusercontent.com.json\" export DST_DIR=\"~/.config/gspread_pandas\" export DST_FILE=\"$DST_DIR/google_secret.json\" mkdir -p $DST_DIR mv $SRC_FILE $DST_FILE chmod 600 $DST_FILE and copy that on the server, e.g., bash ssh research \"mkdir -p $DST_DIR\" scp $SRC_FILE research:$DST_FILE ssh research \"chmod 600 $DST_FILE\" ```","title":"Configuring gspread-pandas"},{"location":"Gsheet_into_pandas/#using-gspread-pandas","text":"The notebook with the usage example is located at amp/core/notebooks/gsheet_into_pandas_example.ipynb . The first time the library is used, it will asks the user for an authorization code. After the authorization code is provided for the first time, it won't be asked again. The official use documentation is provided here . Don't feel stupid if you need multiple iterations to get this stuff working Clicking on GUI is always a recipe for low productivity Go command line and vim!","title":"Using gspread-pandas"},{"location":"HTMLcov_server/","text":"HTMLcov server HTMLcov sever Proxy address to the s3 bucket Usage Admin Configuration Info S3 bucket NGINX server Proxy address to the s3 bucket http://172.30.2.44 (available only behind VPN) Usage Upload the coverage into bucket: cryptokaizen-html (located on CK AWS account), note the syntax from CLI: ``` aws s3 cp ./htmlcov s3://cryptokaizen-html/juraj_test --recursive --profile ck ``` Access the coverage via browser: http://172.30.2.44/juraj_test Nesting example Upload at /htmlcov s3://cryptokaizen-html/juraj_test2/CmTask8789 Access at http://172.30.2.44/juraj_test/CmTask8789 Admin Configuration Info S3 bucket Need to set bucket policy to only allow getObject operations from one IP: (which is a NAT GATEWAY of our VPC) IMPORTANT STEP OTHERWISE THE SITE IS VISIBLE PUBLICLY json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowReadFromNATGateway\", \"Effect\": \"Allow\", \"Principal\": \"*\", \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::cryptokaizen-html/*\", \"Condition\": { \"IpAddress\": { \"aws:SourceIp\": \"16.170.193.70/32\" } } } ] } - Public access can be blocked: - NGINX server Juraj Smeriga and GP have their public key stored on the server in case access via ssh is needed (user: ubuntu) Simple t3.nano instance (run ansible hardening playbook for additional security) Nginx installed with simple config added into /etc/nginx/sites-enabled/default inside default server configuration: bash location / { proxy_pass http://cryptokaizen-html.s3-website.eu-north-1.amazonaws.com/; } After any change run systemctl restart nginx","title":"HTMLcov server"},{"location":"HTMLcov_server/#htmlcov-server","text":"HTMLcov sever Proxy address to the s3 bucket Usage Admin Configuration Info S3 bucket NGINX server","title":"HTMLcov server"},{"location":"HTMLcov_server/#proxy-address-to-the-s3-bucket","text":"http://172.30.2.44 (available only behind VPN)","title":"Proxy address to the s3 bucket"},{"location":"HTMLcov_server/#usage","text":"Upload the coverage into bucket: cryptokaizen-html (located on CK AWS account), note the syntax from CLI: ``` aws s3 cp ./htmlcov s3://cryptokaizen-html/juraj_test --recursive --profile ck ``` Access the coverage via browser: http://172.30.2.44/juraj_test Nesting example Upload at /htmlcov s3://cryptokaizen-html/juraj_test2/CmTask8789 Access at http://172.30.2.44/juraj_test/CmTask8789","title":"Usage"},{"location":"HTMLcov_server/#admin-configuration-info","text":"","title":"Admin Configuration Info"},{"location":"HTMLcov_server/#s3-bucket","text":"Need to set bucket policy to only allow getObject operations from one IP: (which is a NAT GATEWAY of our VPC) IMPORTANT STEP OTHERWISE THE SITE IS VISIBLE PUBLICLY json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowReadFromNATGateway\", \"Effect\": \"Allow\", \"Principal\": \"*\", \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::cryptokaizen-html/*\", \"Condition\": { \"IpAddress\": { \"aws:SourceIp\": \"16.170.193.70/32\" } } } ] } - Public access can be blocked: -","title":"S3 bucket"},{"location":"HTMLcov_server/#nginx-server","text":"Juraj Smeriga and GP have their public key stored on the server in case access via ssh is needed (user: ubuntu) Simple t3.nano instance (run ansible hardening playbook for additional security) Nginx installed with simple config added into /etc/nginx/sites-enabled/default inside default server configuration: bash location / { proxy_pass http://cryptokaizen-html.s3-website.eu-north-1.amazonaws.com/; } After any change run systemctl restart nginx","title":"NGINX server"},{"location":"How_to_integrate_repos/","text":"Concepts Invariants for the integration set-up Integration process Preparation Integration Double-check the integration Run tests Concepts We have two dirs storing two forks of the same repo Files are touched (e.g., added, modified, deleted) in each forks The most problematic files are the files that are modified in both forks Files that are added or deleted in one fork, should be added / deleted also in the other fork Often we can integrate \"by directory\", i.e., finding entire directories that were touched in one branch but not in the other In this case we can simply copy the entire dir from one dir to the other Other times we need to integrate \"by file\" There are various interesting Git reference points: 1) the branch point for each fork, at which the integration branch was started 2) the last integration point for each fork, at which the repos are the same, or at least aligned Invariants for the integration workflows The user runs commands in an abs dir, e.g., /Users/saggese/src/{amp1,cmamp1} The user refers in the command line to dir_basename , which is the basename of the integration directories (e.g., amp1 , cmamp1 , sorrentum1 ) The src_dir_basename is the one where the command is issued The dst_dir_basename is assumed to be parallel to the src_dir_basename The dirs are then transformed in absolute dirs abs_src_dir Integration process Preparation Pull master Crete the integration branches ``` cd cmamp1 i integrate_create_branch --dir-basename cmamp1 cd sorrentum1 i integrate_create_branch --dir-basename sorrentum1 ``` Remove white spaces from both source and destination repos: ```bash dev_scripts/clean_up_text_files.sh git commit -am \"Remove white spaces\"; git push ``` One should still run the regressions out of paranoia since some golden outcomes can be changed > i gh_create_pr --no-draft > i gh_workflow_list Remove empty files: ```bash find . -type f -empty -print | grep -v .git | grep -v init | grep -v \".log$\" | grep -v \".txt$\" | xargs git rm ``` TODO(gp): Add this step to dev_scripts/clean_up_text_files.sh Align lib_tasks.py : ```bash vimdiff ~/src/{amp1,cmamp1}/tasks.py; diff_to_vimdiff.py --dir1 ~/src/amp1 --dir2 ~/src/cmamp1 --subdir helpers ``` Lint both dirs: ```bash cd amp1 i lint --dir-name . --only-format cd cmamp1 i lint --dir-name . --only-format ``` or at least the files touched by both repos: ```bash i integrate_files --file-direction only_files_in_src cat tmp.integrate_find_files_touched_since_last_integration.cmamp1.txt tmp.integrate_find_files_touched_since_last_integration.amp1.txt | sort | uniq >files.txt FILES=$(cat files.txt) i lint --only-format -f \"$FILES\" ``` - This should be done as a single separated PR to be reviewed separately Align lib_tasks.py : ```bash vimdiff ~/src/{amp1,cmamp1}/tasks.py; diff_to_vimdiff.py --dir1 ~/src/amp1 --dir2 ~/src/cmamp1 --subdir helpers ``` Integration Create the integration branches: ```bash cd amp1 i integrate_create_branch --dir-basename amp1 i integrate_create_branch --dir-basename sorrentum1 cd cmamp1 i integrate_create_branch --dir-basename cmamp1 ``` Check what files were modified in each fork since the last integration: ```bash i integrate_files --file-direction common_files i integrate_files --file-direction common_files --src-dir-basename cmamp1 --dst-dir-basename sorrentum1 i integrate_files --file-direction only_files_in_src i integrate_files --file-direction only_files_in_dst ``` Look for directory touched on only one branch: ```bash i integrate_files --file-direction common_files --mode \"print_dirs\" i integrate_files --file-direction only_files_in_src --mode \"print_dirs\" i integrate_files --file-direction only_files_in_dst --mode \"print_dirs\" ``` If we find dirs that are touched in one branch but not in the other we can copy / merge without running risks ```bash i integrate_diff_dirs --subdir $SUBDIR -c ``` Check which change was made in each side since the last integration ```bash # Find the integration point: i integrate_files --file-direction common_files ... last_integration_hash='813c7e763' # Diff the changes in each side from the integration point: i git_branch_diff_with -t hash -h 813c7e763 -f ... git difftool 813c7e763 ... ``` Check which files are different between the dirs: ```bash i integrate_diff_dirs ``` Diff dir by dir ```bash i integrate_diff_dirs --subdir dataflow/system ``` Copy by dir ```bash i integrate_diff_dirs --subdir market_data -c ``` Sync a dir to handle moved files Assume that there is a dir where files were moved ```bash invoke integrate_diff_dirs ... ... Only in .../cmamp1/.../alpha_numeric_data_snapshots: alpha ... Only in .../amp1/.../alpha_numeric_data_snapshots: latest ``` You can accept the cmamp1 side with: ```bash invoke integrate_rsync .../cmamp1/.../alpha_numeric_data_snapshots/ ``` This corresponds to: ```bash rsync --delete -a -r {src_dir}/ {dst_dir}/ ``` Double-check the integration Check that the regressions are passing on GH ```bash i gh_create_pr --no-draft ``` Check the files that were changed in both branches (i.e., the \"problematic ones\") since the last integration and compare them to the base in each branch ```bash cd amp1 i integrate_diff_overlapping_files --src-dir-basename \"amp1\" --dst-dir-basename \"cmamp1\" cd cmamp1 i integrate_diff_overlapping_files --src-dir-basename \"cmamp1\" --dst-dir-basename \"amp1\" ``` Read the changes to Python files: ```bash cd amp1 i git_branch_diff_with -t base --keep-extensions py cd cmamp1 i git_branch_diff_with -t base --keep-extensions py ``` Quickly scan all the changes in the branch compared to the base: ``` cd amp1 i git_branch_diff_with -t base cd cmamp1 i git_branch_diff_with -t base ``` Run tests Check amp / cmamp using GH actions: ```bash i gh_create_pr --no-draft i pytest_collect_only i gh_workflow_list ``` Check lem on dev1 ```bash # Clean everything. git reset --hard; git clean -fd; git pull; (cd amp; git reset --hard; git clean -fd; git pull) i git_pull AM_BRANCH=AmpTask1786_Integrate_20220916 (cd amp; gco $AM_BRANCH) i pytest_collect_only i pytest_buildmeister i git_branch_create -b $AM_BRANCH ``` Check lime on dev4 Check orange on dev1 Check dev_tools on dev1","title":"How to integrate repos"},{"location":"How_to_integrate_repos/#concepts","text":"We have two dirs storing two forks of the same repo Files are touched (e.g., added, modified, deleted) in each forks The most problematic files are the files that are modified in both forks Files that are added or deleted in one fork, should be added / deleted also in the other fork Often we can integrate \"by directory\", i.e., finding entire directories that were touched in one branch but not in the other In this case we can simply copy the entire dir from one dir to the other Other times we need to integrate \"by file\" There are various interesting Git reference points: 1) the branch point for each fork, at which the integration branch was started 2) the last integration point for each fork, at which the repos are the same, or at least aligned","title":"Concepts"},{"location":"How_to_integrate_repos/#invariants-for-the-integration-workflows","text":"The user runs commands in an abs dir, e.g., /Users/saggese/src/{amp1,cmamp1} The user refers in the command line to dir_basename , which is the basename of the integration directories (e.g., amp1 , cmamp1 , sorrentum1 ) The src_dir_basename is the one where the command is issued The dst_dir_basename is assumed to be parallel to the src_dir_basename The dirs are then transformed in absolute dirs abs_src_dir","title":"Invariants for the integration workflows"},{"location":"How_to_integrate_repos/#integration-process","text":"","title":"Integration process"},{"location":"How_to_integrate_repos/#preparation","text":"Pull master Crete the integration branches ``` cd cmamp1 i integrate_create_branch --dir-basename cmamp1 cd sorrentum1 i integrate_create_branch --dir-basename sorrentum1 ``` Remove white spaces from both source and destination repos: ```bash dev_scripts/clean_up_text_files.sh git commit -am \"Remove white spaces\"; git push ``` One should still run the regressions out of paranoia since some golden outcomes can be changed > i gh_create_pr --no-draft > i gh_workflow_list Remove empty files: ```bash find . -type f -empty -print | grep -v .git | grep -v init | grep -v \".log$\" | grep -v \".txt$\" | xargs git rm ``` TODO(gp): Add this step to dev_scripts/clean_up_text_files.sh Align lib_tasks.py : ```bash vimdiff ~/src/{amp1,cmamp1}/tasks.py; diff_to_vimdiff.py --dir1 ~/src/amp1 --dir2 ~/src/cmamp1 --subdir helpers ``` Lint both dirs: ```bash cd amp1 i lint --dir-name . --only-format cd cmamp1 i lint --dir-name . --only-format ``` or at least the files touched by both repos: ```bash i integrate_files --file-direction only_files_in_src cat tmp.integrate_find_files_touched_since_last_integration.cmamp1.txt tmp.integrate_find_files_touched_since_last_integration.amp1.txt | sort | uniq >files.txt FILES=$(cat files.txt) i lint --only-format -f \"$FILES\" ``` - This should be done as a single separated PR to be reviewed separately Align lib_tasks.py : ```bash vimdiff ~/src/{amp1,cmamp1}/tasks.py; diff_to_vimdiff.py --dir1 ~/src/amp1 --dir2 ~/src/cmamp1 --subdir helpers ```","title":"Preparation"},{"location":"How_to_integrate_repos/#integration","text":"Create the integration branches: ```bash cd amp1 i integrate_create_branch --dir-basename amp1 i integrate_create_branch --dir-basename sorrentum1 cd cmamp1 i integrate_create_branch --dir-basename cmamp1 ``` Check what files were modified in each fork since the last integration: ```bash i integrate_files --file-direction common_files i integrate_files --file-direction common_files --src-dir-basename cmamp1 --dst-dir-basename sorrentum1 i integrate_files --file-direction only_files_in_src i integrate_files --file-direction only_files_in_dst ``` Look for directory touched on only one branch: ```bash i integrate_files --file-direction common_files --mode \"print_dirs\" i integrate_files --file-direction only_files_in_src --mode \"print_dirs\" i integrate_files --file-direction only_files_in_dst --mode \"print_dirs\" ``` If we find dirs that are touched in one branch but not in the other we can copy / merge without running risks ```bash i integrate_diff_dirs --subdir $SUBDIR -c ``` Check which change was made in each side since the last integration ```bash # Find the integration point: i integrate_files --file-direction common_files ... last_integration_hash='813c7e763' # Diff the changes in each side from the integration point: i git_branch_diff_with -t hash -h 813c7e763 -f ... git difftool 813c7e763 ... ``` Check which files are different between the dirs: ```bash i integrate_diff_dirs ``` Diff dir by dir ```bash i integrate_diff_dirs --subdir dataflow/system ``` Copy by dir ```bash i integrate_diff_dirs --subdir market_data -c ``` Sync a dir to handle moved files Assume that there is a dir where files were moved ```bash invoke integrate_diff_dirs ... ... Only in .../cmamp1/.../alpha_numeric_data_snapshots: alpha ... Only in .../amp1/.../alpha_numeric_data_snapshots: latest ``` You can accept the cmamp1 side with: ```bash invoke integrate_rsync .../cmamp1/.../alpha_numeric_data_snapshots/ ``` This corresponds to: ```bash rsync --delete -a -r {src_dir}/ {dst_dir}/ ```","title":"Integration"},{"location":"How_to_integrate_repos/#double-check-the-integration","text":"Check that the regressions are passing on GH ```bash i gh_create_pr --no-draft ``` Check the files that were changed in both branches (i.e., the \"problematic ones\") since the last integration and compare them to the base in each branch ```bash cd amp1 i integrate_diff_overlapping_files --src-dir-basename \"amp1\" --dst-dir-basename \"cmamp1\" cd cmamp1 i integrate_diff_overlapping_files --src-dir-basename \"cmamp1\" --dst-dir-basename \"amp1\" ``` Read the changes to Python files: ```bash cd amp1 i git_branch_diff_with -t base --keep-extensions py cd cmamp1 i git_branch_diff_with -t base --keep-extensions py ``` Quickly scan all the changes in the branch compared to the base: ``` cd amp1 i git_branch_diff_with -t base cd cmamp1 i git_branch_diff_with -t base ```","title":"Double-check the integration"},{"location":"How_to_integrate_repos/#run-tests","text":"Check amp / cmamp using GH actions: ```bash i gh_create_pr --no-draft i pytest_collect_only i gh_workflow_list ``` Check lem on dev1 ```bash # Clean everything. git reset --hard; git clean -fd; git pull; (cd amp; git reset --hard; git clean -fd; git pull) i git_pull AM_BRANCH=AmpTask1786_Integrate_20220916 (cd amp; gco $AM_BRANCH) i pytest_collect_only i pytest_buildmeister i git_branch_create -b $AM_BRANCH ``` Check lime on dev4 Check orange on dev1 Check dev_tools on dev1","title":"Run tests"},{"location":"How_to_organize_your_work/","text":"Golden rules Please watch , star and fork the repo. \"Remember to treat others the way you want to be treated\" Everybody comes from a different place and different skill-level, somebody has a job, somebody has a full-time work Let's be polite, helpful, and supportive to each other I'll remind you an adage that I tell myself when my teammates drive me crazy: \"If you want to go fast, go alone; if you want to go far, go together\" While being assigned a warm-up issue: We highly value the importance of following instructions to ensure efficient task completion. We intentionally select straightforward and simple issues as the first tasks for you. So as your first step you should put primary focus on following the rules. Failure to do so can result in wasting more of our time than saving it. A pledge to put in the time you committed and not disappear If we don't find that you can meaningfully contribute, we will give you a warning and help you improve, we will suggest you how to fill in your knowledge gaps, and start not putting the time to help since we don't have enough resources If we decide that you are not ready to contribute, we will drop you, without discussions: this is our sole decision for the good of the project and the other contributors You can study and try again in 6 months Roles Tech leads Tech leads are contributors that have put more effort in Sorrentum and / or have more experience in research and development The same project (e.g., Web3, Model, Arbitrage, NLP) can have multiple tech leads One becomes a tech lead by technical excellence and dedication, and for no other reason (e.g., seniority or degree) What tech leads do: Organize the work in their specific project together with GP / Paul and the other full-time contributors Help young team members, e.g., answering their questions, showing how we do things Answer other team members questions, even in different projects Following our process, advertising it, and explaining it to new team members Understand what other people in your team are doing and coordinate with them Make sure code, notebooks, and data are reused and shared What tech leads gain from being a tech lead: Learn how to run a research / dev team Practice leadership skills (which mainly entail technical excellence and patience) Work closely with GP / Paul Material benefits (e.g., higher Sorrentum bonuses once the reward system is in place) Developers People on teams working on the same project or on related projects can: work together on the same tasks (you can coordinate on GitHub, Telegram, or do a Zoom) work together and split the work (still you need coordination) each can \"replicate\" the work so that both of you understand (no coordination) How to organize your research The GH issues are for: reporting issues (e.g., \"I am getting this error. What should I do?\") updates (e.g., \"I've made a bunch of progress and the details are in the gdoc ) Commit the code in a PR (even the same PR) so that it's easier to review the details together You can say \"this is what I'm doing, what do you think?\" E.g., create a dir like 'SorrIssueXYZ_...' under https://github.com/sorrentum/sorrentum/tree/master/sorrentum_sandbox/examples/ml_projects E.g., see the PR under https://github.com/sorrentum/sorrentum/pull/31 Work in the Docker container so that you use the standard flow and it's easy to collaborate It's ok if you want to use your different flow, but still try to commit often Save all the files in Project dir You can use Google Desktop to keep the dir synced locally on your computer so when you work on your project the data is always in sync with what others view Keep a detailed research log in a Google Doc in the Project dir Document what you are doing, take screenshot, explain the results You can make progress only if you are organized and consistent Avoid emails any time possible Communication should happen on GitHub around specific Issues Read General Rules of Collaboration for more details Use Telegram chat when you are blocked on something Try to use GitHub Issues when not urgent to provide context Don't abuse Telegram, learning what is urgent and what's not On GitHub Provide context about the problem Explain clearly what is the problem and what you are trying to do Writing good bug updates is an art that takes time When you are done with your task and you need more work ping your project chat Work with your team (especially if you are a tech lead) Answer other people questions Understand what other people in your team are doing Coordinate with them We can only make progress together as a team and not as a single person (get over being shy) About the project you choose We let you pick the project you like because we believe that one needs to be excited about something to work on that through the late night, on Sat and Sunday On the other side, in 15 years of leading, mentoring, hiring, and firing people I've never found one person not complaining about their project being \"boring\" People working on exploratory analysis want to do machine learning People want to do \"machine learning\" when 95% of the machine learning work is building data pipelines and doing exploratory analysis People running models complain about that being uninspiring and would rather to do exploratory analysis People writing code want to do machine learning (since that's the cool stuff!) Software engineers would like to be lawyers (since they want to interact with people and computers), and lawyers want to be engineers (since they can't take dealing with people anymore) The problem is that everything is interesting and boring at the same time. It depends on your mindset If you say \"this is boring\", it will be boring for sure If you look for finding interesting pieces in what you need to do, it will become suddenly super interesting The people that succeed are the ones that do their best at their job no matter what, that find joy in solving whatever problem brings value","title":"Golden rules"},{"location":"How_to_organize_your_work/#golden-rules","text":"Please watch , star and fork the repo. \"Remember to treat others the way you want to be treated\" Everybody comes from a different place and different skill-level, somebody has a job, somebody has a full-time work Let's be polite, helpful, and supportive to each other I'll remind you an adage that I tell myself when my teammates drive me crazy: \"If you want to go fast, go alone; if you want to go far, go together\" While being assigned a warm-up issue: We highly value the importance of following instructions to ensure efficient task completion. We intentionally select straightforward and simple issues as the first tasks for you. So as your first step you should put primary focus on following the rules. Failure to do so can result in wasting more of our time than saving it. A pledge to put in the time you committed and not disappear If we don't find that you can meaningfully contribute, we will give you a warning and help you improve, we will suggest you how to fill in your knowledge gaps, and start not putting the time to help since we don't have enough resources If we decide that you are not ready to contribute, we will drop you, without discussions: this is our sole decision for the good of the project and the other contributors You can study and try again in 6 months","title":"Golden rules"},{"location":"How_to_organize_your_work/#roles","text":"","title":"Roles"},{"location":"How_to_organize_your_work/#tech-leads","text":"Tech leads are contributors that have put more effort in Sorrentum and / or have more experience in research and development The same project (e.g., Web3, Model, Arbitrage, NLP) can have multiple tech leads One becomes a tech lead by technical excellence and dedication, and for no other reason (e.g., seniority or degree) What tech leads do: Organize the work in their specific project together with GP / Paul and the other full-time contributors Help young team members, e.g., answering their questions, showing how we do things Answer other team members questions, even in different projects Following our process, advertising it, and explaining it to new team members Understand what other people in your team are doing and coordinate with them Make sure code, notebooks, and data are reused and shared What tech leads gain from being a tech lead: Learn how to run a research / dev team Practice leadership skills (which mainly entail technical excellence and patience) Work closely with GP / Paul Material benefits (e.g., higher Sorrentum bonuses once the reward system is in place)","title":"Tech leads"},{"location":"How_to_organize_your_work/#developers","text":"People on teams working on the same project or on related projects can: work together on the same tasks (you can coordinate on GitHub, Telegram, or do a Zoom) work together and split the work (still you need coordination) each can \"replicate\" the work so that both of you understand (no coordination)","title":"Developers"},{"location":"How_to_organize_your_work/#how-to-organize-your-research","text":"The GH issues are for: reporting issues (e.g., \"I am getting this error. What should I do?\") updates (e.g., \"I've made a bunch of progress and the details are in the gdoc ) Commit the code in a PR (even the same PR) so that it's easier to review the details together You can say \"this is what I'm doing, what do you think?\" E.g., create a dir like 'SorrIssueXYZ_...' under https://github.com/sorrentum/sorrentum/tree/master/sorrentum_sandbox/examples/ml_projects E.g., see the PR under https://github.com/sorrentum/sorrentum/pull/31 Work in the Docker container so that you use the standard flow and it's easy to collaborate It's ok if you want to use your different flow, but still try to commit often Save all the files in Project dir You can use Google Desktop to keep the dir synced locally on your computer so when you work on your project the data is always in sync with what others view Keep a detailed research log in a Google Doc in the Project dir Document what you are doing, take screenshot, explain the results You can make progress only if you are organized and consistent Avoid emails any time possible Communication should happen on GitHub around specific Issues Read General Rules of Collaboration for more details Use Telegram chat when you are blocked on something Try to use GitHub Issues when not urgent to provide context Don't abuse Telegram, learning what is urgent and what's not On GitHub Provide context about the problem Explain clearly what is the problem and what you are trying to do Writing good bug updates is an art that takes time When you are done with your task and you need more work ping your project chat Work with your team (especially if you are a tech lead) Answer other people questions Understand what other people in your team are doing Coordinate with them We can only make progress together as a team and not as a single person (get over being shy)","title":"How to organize your research"},{"location":"How_to_organize_your_work/#about-the-project-you-choose","text":"We let you pick the project you like because we believe that one needs to be excited about something to work on that through the late night, on Sat and Sunday On the other side, in 15 years of leading, mentoring, hiring, and firing people I've never found one person not complaining about their project being \"boring\" People working on exploratory analysis want to do machine learning People want to do \"machine learning\" when 95% of the machine learning work is building data pipelines and doing exploratory analysis People running models complain about that being uninspiring and would rather to do exploratory analysis People writing code want to do machine learning (since that's the cool stuff!) Software engineers would like to be lawyers (since they want to interact with people and computers), and lawyers want to be engineers (since they can't take dealing with people anymore) The problem is that everything is interesting and boring at the same time. It depends on your mindset If you say \"this is boring\", it will be boring for sure If you look for finding interesting pieces in what you need to do, it will become suddenly super interesting The people that succeed are the ones that do their best at their job no matter what, that find joy in solving whatever problem brings value","title":"About the project you choose"},{"location":"Imports_and_packages/","text":"Imports and packages Goals of packages Circular dependency (aka import cycle, import loop) Rules for imports How to import code from unit tests Common unit test code Package/lib hierarchy and cycle prevention Anatomy of a package TODO(gp): Consolidate here any other rule from other gdoc Goals of packages The goal of creating packages is to: Simplify the import from clients Hide in which file the actual code is, so that we can reorganize the code without having to change all the client code Organize the code in related units Make it simpler to avoid import loops by enforcing that there are no import loops in any module and no import loops among modules E.g., referring to package from a different package looks like python import dataflow.core as dtfcore dtfcore.ArmaGenerator(...) Importing the specific file: python import dataflow.system.source_nodes as dtfsysonod dtfsysonod.ArmaGenerator(...) Circular dependency (aka import cycle, import loop) The simplest case of circular import is a situation when in lib A we have import B , and in lib B we have import A The presence of circular imports can be checked with an invoke i lint_detect_cycles . By default, it will run on the whole repo, which takes a couple of minutes, but it will provide the most reliable and thorough check for circular imports Rules for imports We follow rules to avoid import loops: Code inside a package should import directly a file in the same package and not use the package E.g., im_v2/common/data/client/data_frame_im_clients.py Good python import im_v2.common.data.client.base_im_clients as imvcdcbimcl Bad python import im_v2.common.data.client as icdc Code from a package should import other packages, instead of importing directly the file We don't allow any import loop that can be detected statically (i.e., by inspecting the code without executing it) This guarantees that there are no dynamic import loops, which are even more difficult to detect and disruptive We allow only imports at the module level and not inside functions We don't accept using local imports to break import loops, unless it's temporary to solve a more important problem We allow nested packages TODO(gp): Clarify the rules here We don't want to abuse packaging by creating too many of them Rationale: There is overhead in organizing and maintaining code in packages and we want to pay the overhead only if we get enough benefit from this We specify a short import in the __init__.py file for a package manually because the linter cannot do it automatically yet We use the first letters to build a short import and try to keep it less than 8 chars long, e.g., im_v2.talos.data.client -> itdcl We insert an import docstring in the __init__.py file manually and then we use the specified short import everywhere in the codebase. E.g., ```python Import as: import im_v2.talos.data.client as itdcl ``` How to import code from unit tests To avoid churning client code when code is moved among files, we allow unit tests to both: Import the package when testing code exported from the package E.g., in market_data/test/market_data_test_case.py you can import the package even if it's included python import market_data as mdata \u2026 mdata.AbstractMarketData \u2026 Import the files directly with the code and not the package E.g., python import market_data.abstract_market_data as mdabmada \u2026 mdabmada.AbstractMarketData \u2026 To justify, one can argue that unit tests are clients of the code and should import packages like any other client To justify, one can interpret that unit tests are tied to specific files, so they should be kept in sync with the low-level code and not with the public interface. In fact, we already allow unit tests to call private functions, acknowledging that unit tests are not regular clients Given that both explanations are valid, we allow both styles Common unit test code Unit tests should not import from each other If there is common code, it should go in libraries inside or outside test directories E.g., we use foobar_example.py files containing builders for mocks and examples of objects to be used by tests E.g., we use test/foobar_test_case.py or test/foobar_utils.py In other terms, test files are always leaves of the import graph Package/lib hierarchy and cycle prevention Static import cycles can be detected by the invoke lint_detect_cycles To prevent import cycles, we want to enforce that certain packages don't depend on other packages E.g., helpers should not depend on any other package, besides external libraries core should only depend on helpers dataflow should only depend on core and helpers These constraints can be expressed in terms of \"certain nodes of the import graph are sources\" or \"certain edges in the import graph are forbidden\" We also want to enforce that certain libs don't import others within a single package. For example, in helpers , the following hierarchy should be respected: hdbg (should not depend on any other helper) hintrospection , hprint (should depend only on hdbg for assertion) henv , hsystem , hio , hversio (this is the base layer to access env vars and execute commands) hgit (Git requires accessing env vars and system calls) A library can only import libs that are \"below\" it or on the same level E.g., henv can import hdbg , hprint , and hio , but it cannot import hgit While importing a lib on the same level, make sure you are not creating an import cycle In addition, keep in mind the following rules to prevent import cycles: Any import inside a function is just a temporary hack waiting to create problems Any time we can break a file into smaller pieces, we should do that since this helps control the dependencies Anatomy of a package TODO(gp): Let's use dataflow as a running example A package has a special __init__.py exporting public methods","title":"Imports and packages"},{"location":"Imports_and_packages/#imports-and-packages","text":"Goals of packages Circular dependency (aka import cycle, import loop) Rules for imports How to import code from unit tests Common unit test code Package/lib hierarchy and cycle prevention Anatomy of a package TODO(gp): Consolidate here any other rule from other gdoc","title":"Imports and packages"},{"location":"Imports_and_packages/#goals-of-packages","text":"The goal of creating packages is to: Simplify the import from clients Hide in which file the actual code is, so that we can reorganize the code without having to change all the client code Organize the code in related units Make it simpler to avoid import loops by enforcing that there are no import loops in any module and no import loops among modules E.g., referring to package from a different package looks like python import dataflow.core as dtfcore dtfcore.ArmaGenerator(...) Importing the specific file: python import dataflow.system.source_nodes as dtfsysonod dtfsysonod.ArmaGenerator(...)","title":"Goals of packages"},{"location":"Imports_and_packages/#circular-dependency-aka-import-cycle-import-loop","text":"The simplest case of circular import is a situation when in lib A we have import B , and in lib B we have import A The presence of circular imports can be checked with an invoke i lint_detect_cycles . By default, it will run on the whole repo, which takes a couple of minutes, but it will provide the most reliable and thorough check for circular imports","title":"Circular dependency (aka import cycle, import loop)"},{"location":"Imports_and_packages/#rules-for-imports","text":"We follow rules to avoid import loops: Code inside a package should import directly a file in the same package and not use the package E.g., im_v2/common/data/client/data_frame_im_clients.py Good python import im_v2.common.data.client.base_im_clients as imvcdcbimcl Bad python import im_v2.common.data.client as icdc Code from a package should import other packages, instead of importing directly the file We don't allow any import loop that can be detected statically (i.e., by inspecting the code without executing it) This guarantees that there are no dynamic import loops, which are even more difficult to detect and disruptive We allow only imports at the module level and not inside functions We don't accept using local imports to break import loops, unless it's temporary to solve a more important problem We allow nested packages TODO(gp): Clarify the rules here We don't want to abuse packaging by creating too many of them Rationale: There is overhead in organizing and maintaining code in packages and we want to pay the overhead only if we get enough benefit from this We specify a short import in the __init__.py file for a package manually because the linter cannot do it automatically yet We use the first letters to build a short import and try to keep it less than 8 chars long, e.g., im_v2.talos.data.client -> itdcl We insert an import docstring in the __init__.py file manually and then we use the specified short import everywhere in the codebase. E.g., ```python Import as: import im_v2.talos.data.client as itdcl ```","title":"Rules for imports"},{"location":"Imports_and_packages/#how-to-import-code-from-unit-tests","text":"To avoid churning client code when code is moved among files, we allow unit tests to both: Import the package when testing code exported from the package E.g., in market_data/test/market_data_test_case.py you can import the package even if it's included python import market_data as mdata \u2026 mdata.AbstractMarketData \u2026 Import the files directly with the code and not the package E.g., python import market_data.abstract_market_data as mdabmada \u2026 mdabmada.AbstractMarketData \u2026 To justify, one can argue that unit tests are clients of the code and should import packages like any other client To justify, one can interpret that unit tests are tied to specific files, so they should be kept in sync with the low-level code and not with the public interface. In fact, we already allow unit tests to call private functions, acknowledging that unit tests are not regular clients Given that both explanations are valid, we allow both styles","title":"How to import code from unit tests"},{"location":"Imports_and_packages/#common-unit-test-code","text":"Unit tests should not import from each other If there is common code, it should go in libraries inside or outside test directories E.g., we use foobar_example.py files containing builders for mocks and examples of objects to be used by tests E.g., we use test/foobar_test_case.py or test/foobar_utils.py In other terms, test files are always leaves of the import graph","title":"Common unit test code"},{"location":"Imports_and_packages/#packagelib-hierarchy-and-cycle-prevention","text":"Static import cycles can be detected by the invoke lint_detect_cycles To prevent import cycles, we want to enforce that certain packages don't depend on other packages E.g., helpers should not depend on any other package, besides external libraries core should only depend on helpers dataflow should only depend on core and helpers These constraints can be expressed in terms of \"certain nodes of the import graph are sources\" or \"certain edges in the import graph are forbidden\" We also want to enforce that certain libs don't import others within a single package. For example, in helpers , the following hierarchy should be respected: hdbg (should not depend on any other helper) hintrospection , hprint (should depend only on hdbg for assertion) henv , hsystem , hio , hversio (this is the base layer to access env vars and execute commands) hgit (Git requires accessing env vars and system calls) A library can only import libs that are \"below\" it or on the same level E.g., henv can import hdbg , hprint , and hio , but it cannot import hgit While importing a lib on the same level, make sure you are not creating an import cycle In addition, keep in mind the following rules to prevent import cycles: Any import inside a function is just a temporary hack waiting to create problems Any time we can break a file into smaller pieces, we should do that since this helps control the dependencies","title":"Package/lib hierarchy and cycle prevention"},{"location":"Imports_and_packages/#anatomy-of-a-package","text":"TODO(gp): Let's use dataflow as a running example A package has a special __init__.py exporting public methods","title":"Anatomy of a package"},{"location":"Jupyter_notebook_best_practices/","text":"Jupyter notebook best practices When to use a Jupyter notebook General structure of a notebook Description Imports Configuration Make the notebook flow clear General best practices Update calls only for Master/Gallery notebooks Convention: Rationale: Keep code that belongs together in one cell Write beautiful code, even in notebooks Show how data is transformed as you go Use keyboard shortcuts Strive for simplicity Dependencies among cells Re-execute from scratch Add comments for complex cells Do not cut & paste code Avoid \"wall-of-code\" cell Avoid data biases Avoid hardwired constants Explain where data is coming from Fix warnings Make cells idempotent Always look at the discarded data Use a progress bar Notebooks and libraries Pros Cons Recommendations for plots Use the proper y-scale Make each plot self-explanatory Avoid wall-of-text tables Use common axes to allow visual comparisons Use the right plot Useful plugins Vim bindings Table of content ExecuteTime Spellchecker AutoSaveTime Notify Jupytext Gspread When to use a Jupyter notebook A notebook can be used for various goals: Tutorial / gallery Show how some code works (e.g., functions in signal_processing.py or data_encyclopedia.ipynb ) The code should always work We might want to add unit tests for it Prototyping / one-off E.g., We prototype some code, before it becomes library code We did some one-off analysis Analysis Aka \"master\" notebooks The notebook should always work so we need to treat it as part of the code base We might want to add unit tests for it General structure of a notebook Description At the top of the notebook add a description section explaining a notebook's goal and what it does, e.g., ``` # Description This notebook was used for prototyping / debugging code that was moved in the file abc.py ``` Convert section headers and cells with description text to a Markdown format by selecting a cell and then at Jupyter interface do Cell -> Cell Type -> Markdown Imports Add a code section importing the needed libraries Autoreload modules to keep Jupyter and local code updated in real-time Standard imports, e.g. os Third-party imports, e.g. pandas Local imports from our lib It's better to put all the imports in one cell and separate different import types by 1 empty line, e.g.: ``` # Imports %load_ext autoreload %autoreload 2 import logging import os import matplotlib.pyplot as plt import pandas as pd import helpers.dbg as dbg import helpers.env as env import helpers.printing as prnt import core.explore as exp import core.signal_processing as sigp ... ``` In this way executing one cell is enough to configure the notebook Configuration You can configure the notebooks with some utils, logging, and report info on how the notebook was executed (e.g., Git commit, libs, etc.) by using the following cell: ``` # Configure logger. hdbg.init_logger(verbosity=logging.INFO) _LOG = logging.getLogger( name ) # Print system signature. _LOG.info(\"%s\", henv.get_system_signature()[0]) # Configure the notebook style. hprint.config_notebook() ``` The output of the cell looks like: INFO: > cmd='/venv/lib/python3.8/site-packages/ipykernel_launcher.py -f /home/.local/share/jupyter/runtime/kernel-a48f60fd-0f96-48b4-82d8-879385f2be91.json' WARNING: Running in Jupyter DEBUG Effective logging level=10 DEBUG Shut up 1 modules: asyncio DEBUG > (cd . && cd \"$(git rev-parse --show-toplevel)/..\" && (git rev-parse --is-inside-work-tree | grep -q true)) 2>&1 DEBUG > (git rev-parse --show-toplevel) 2>&1 ... # Packages python: 3.8.10 cvxopt: 1.3.0 cvxpy: 1.2.2 gluonnlp: ? gluonts: 0.6.7 joblib: 1.2.0 mxnet: 1.9.1 numpy: 1.23.4 pandas: 1.5.1 pyarrow: 10.0.0 scipy: 1.9.3 seaborn: 0.12.1 sklearn: 1.1.3 statsmodels: 0.13.5 Make the notebook flow clear Each notebook needs to follow a clear and logical flow, e.g: Load data Compute stats Clean data Compute stats Do analysis Show results The flow should be highlighted using headings in markdown: # Level 1 ## Level 2 ### Level 3 Use the extension for navigating the notebook (see our suggestions for Jupyter plug-ins) Keep related code and analysis close together so: Readers can understand the logical flow One could \"easily\" split the notebook in parts (e.g., when it becomes too big) You can collapse the cells and don't scroll back and forth too much General best practices Update calls only for Master/Gallery notebooks Convention: We do our best to update the calls in the Master/Gallery notebooks but we don't guarantee that the fix is correct For other notebooks we either do the fix (e.g., changing a name of a function) and tweak the call to enforce the old behavior, or even not do anything if there are too many changes Rationale: We have dozens of ad-hoc research notebooks When a piece of code is updated (e.g., ImClient ) the change should be propagated everywhere in the code base, including the notebooks This results in excessive amount of maintenance work which we want to avoid Keep code that belongs together in one cell It's often useful to keep in a cell computation that needs to be always executed together E.g., compute something and then print results In this way a single cell execution computes all data together Often computation starts in multiple cells, e.g., to inline debugging, and once we are more confident that it works correctly we can merge it in a cell (or even better in a function) Write beautiful code, even in notebooks Follow the conventions and suggestions for Python code style When prototyping with a notebook, the code can be of lower quality than code, but still needs to be readable and robust In our opinion it's just better to always do write robust and readable code: it doesn't buy much time to cut corners Show how data is transformed as you go Print a few lines of data structures (e.g., df.head(3) ) so one can see how data is transformed through the cells Use keyboard shortcuts Learn the default keyboard shortcuts to edit efficiently You can use the vim plug-in (see below) and become 3x more ninja Strive for simplicity Always make the notebook easy to be understood and run by somebody else Explain what happens Organize the code in a logical way Use decent variable names Comment the results, when possible / needed Dependencies among cells Try to avoid dependencies between cells Even better avoid any dependency between cells, e.g.: Put all the imports in one cell at the beginning, so with one cell execution you can make sure that all the imports are done Compare this approach with the case where the imports are randomly sprinkled in the notebook, then you need to go execute them one by one if you re-initialize the notebook For the same reason group functions in one cell that you can easily re-execute Re-execute from scratch Once in a while (e.g., once a day) Commit your changes Make sure you can re-execute everything from the top with Kernel -> Restart & Clean output and then Kernel -> Run all Visually verify that the results didn't change, so that there is no weird state or dependency in the code Before a commit (and definitively before a PR) do a clean run Add comments for complex cells When a cell is too long, explain in a comment what a cell does, e.g., ## Count stocks with all nans. num_nans = np.isnan(rets).sum(axis=0) num_nans /= rets.shape[0] num_nans = num_nans.sort_values(ascending=False) num_stocks_with_no_nans = (num_nans == 0.0).sum() print(\"num_stocks_with_no_nans=%s\" % hprint.perc(num_stocks_with_no_nans, rets.shape[1])) Another approach is to factor out the code in functions with clear names and simplify the flow Do not cut & paste code Cutting + paste + modify is NEVER a good idea It takes more time to clean up cut & paste code than doing right in the first place Just make a function out of the code and call it! Avoid \"wall-of-code\" cell Obvious Avoid data biases Try to compute statistics on the entire data set so that results are representative and not dependent on a particular slice of the data You can sample the data and check stability of the results If it takes too long to compute the statistics on the entire data set, report the problem and we can think of how to speed it up Avoid hardwired constants Don't use hardwired constants Try to parametrize the code Explain where data is coming from If you are using data from a file (e.g., /data/wd/RP_1yr_13_companies.pkl ), explain in a comment how the file was generated Ideally report a command line to regenerate the data The goal is for other people to be able to re-run the notebook from scratch Fix warnings A notebook should run without warnings Warnings can't be ignored since they indicate that the code is relying on a feature that will change in the future, e.g., FutureWarning: Sorting because non-concatenation axis is not aligned. A future version of pandas will change to not sort by default. To accept the future behavior, pass 'sort=False'. To retain the current behavior and silence the warning, pass 'sort=True'. Another example: after a cell execution the following warning appears: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy This is a typical pandas warning telling us that we created a view on a dataframe (e.g., by slicing) and we are modifying the underlying data through the view This is dangerous since it can create unexpected side effects and coupling between pieces of code that can be painful to debug and fix If we don't fix the issue now, the next time we create a conda environment the code might either break or (even worse) have a different behavior, i.e., silent failure It's better to fix the warning now that we can verify that the code does what we want to do, instead of fixing it later when we don't remember anymore what exactly we were doing If you have warnings in your code or notebook you can't be sure that the code is doing exactly what you think it is doing For what we know your code might be deleting your hard-disk, moving money from your account to mine, starting World War 3, ... You don't ever want to program by coincidence Typically the warnings are informative and tell us what's the issue and how to fix it, so please fix your code If it's not obvious how to interpret or fix a warning file a bug, file a bug reporting clearly a repro case and the error message Make cells idempotent Try to make a notebook cell able of being executed multiple times without changing its output value, e.g., Bad df[\"id\"] = df[\"id\"] + 1 This computation is not idempotent, since if you execute it multiple times is going to increment the column id at every iteration Good df[\"incremented_df\"] = df[\"id\"] + 1 A better approach is to always create a new \"copy\" Another example: Bad tmp = normalize(tmp) Good tmp_after_normalize = normalize(tmp) In this way it's easy to add another stage in the pipeline without changing everything Of course the names tmp_1 , tmp_2 are a horrible idea since they are not self-explanatory and adding a new stage screws up the numbering For data frames and variables is a good idea to create copies of the data along the way: df_without_1s = df[df[\"id\"] != 1].copy() Always look at the discarded data Filtering the data is a risky operation since once the data is dropped, nobody is going to go back and double check what exactly happened Everything downstream (e.g., all the results, all the conclusions, all the decisions based on those conclusions) rely on the filtering being correct Any time there is a dropna or a filtering / masking operation, e.g.: compu_data = compu_data.dropna(subset=[\"CIK\"]) or selected_metrics = [...] compu_data = compu_data[compu_data[\"item\"].apply(lambda x : x in selected_metrics)] compu_data = compu_data[compu_data[\"datadate\"].apply(date_is_quarter_end)] Always count what percentage of the rows you dropped (e.g., do a back of the envelope check that you are dropping what you would expect) import helpers.printing as hprint ... n_rows = compu_form_df.shape[0] compu_form_df = compu_form_df.drop_duplicates() n_rows_after = compu_form_df.shape[0] _LOG.debug(\"After dropping duplicates kept: %s\", hprint.perc(n_rows_after, n_rows)) Make absolutely sure you are not dropping important data E.g., has the distribution of the data changed in the way you would expect? Use a progress bar Always use progress bars (even in notebooks) so that user can see how long it will take for a certain computation. It is also possible to let tqdm automatically choose between console or notebook versions by using from tqdm.autonotebook import tqdm Notebooks and libraries It's ok to use functions in notebooks when building the analysis to leverage notebook interactivity Once the notebook is \"stable\", often it's better to move the code in a library, i.e., a python file. Make sure you add autoreload modules in Imports section %load_ext autoreload %autoreload 2 Otherwise, if you change a function in the lib, the notebook will not pull this change and use the old version of the function Pros The same notebook code can be used for different notebooks E.g., the function to read the data from disk is an obvious example More people can reuse the same code for different analyses If one changes the code in a library, Git can help tracking changes and merging, while notebooks are difficult to diff / merge Cleaning up / commenting / untangling the code can help reason carefully about the assumptions to find issues The notebook becomes more streamlined and easy to understand since now it's a sequence of functions do_this_and_that and presenting the results One can speed up / parallelize analyses with multiprocessing Notebooks are not great for this E.g., when one does the analyses on a small subset of the data and then wants to run on the entire large dataset The exploratory analysis can be moved towards modeling and then production Cons One have to scroll back and forth between notebook and the libraries to execute the cell with the functions and fix all the possible mistakes Recommendations for plots Use the proper y-scale If one value can vary from -1.0 to 1.0, force the y-scale between those limits so that the values are absolutes, unless this would squash the plot Make each plot self-explanatory Make sure that each plot has a descriptive title, x and y label Explain the set-up of a plot / analysis E.g., what is the universe of stocks used? What is the period of time? Add this information also to the plots Avoid wall-of-text tables Try to use plots summarizing the results besides the raw results in a table Use common axes to allow visual comparisons Try to use same axes for multiple graphs when possible to allow visual comparison between graphs If that's not possible or convenient make individual plots with different scales and add a plot with multiple graphs inside on the same axis (e.g., with y-log) Use the right plot Pick the right type of graph to make your point pandas , seaborn , matplotlib are your friends Useful plugins You can access the extensions menu: Edit -> nbextensions config http://localhost:XYZ/nbextensions/ Vim bindings VIM binding will change your life Table of content To see the entire logical flow of the notebook, when you use the headers properly ExecuteTime To see how long each cell takes to execute Spellchecker To improve your English! AutoSaveTime To save the code automatically every minute Notify Show a browser notification when kernel becomes idle Jupytext We use Jupytext as standard part of our development flow See documentation/general/jupytext.md Gspread Allow to read g-sheets in Jupyter Notebook First, one needs to configure Google API, just follow the instructions from here Useful links: Examples of gspread Usage Enabling Gsheet API Adding service email if it\u2019s not working","title":"Jupyter notebook best practices"},{"location":"Jupyter_notebook_best_practices/#jupyter-notebook-best-practices","text":"When to use a Jupyter notebook General structure of a notebook Description Imports Configuration Make the notebook flow clear General best practices Update calls only for Master/Gallery notebooks Convention: Rationale: Keep code that belongs together in one cell Write beautiful code, even in notebooks Show how data is transformed as you go Use keyboard shortcuts Strive for simplicity Dependencies among cells Re-execute from scratch Add comments for complex cells Do not cut & paste code Avoid \"wall-of-code\" cell Avoid data biases Avoid hardwired constants Explain where data is coming from Fix warnings Make cells idempotent Always look at the discarded data Use a progress bar Notebooks and libraries Pros Cons Recommendations for plots Use the proper y-scale Make each plot self-explanatory Avoid wall-of-text tables Use common axes to allow visual comparisons Use the right plot Useful plugins Vim bindings Table of content ExecuteTime Spellchecker AutoSaveTime Notify Jupytext Gspread","title":"Jupyter notebook best practices"},{"location":"Jupyter_notebook_best_practices/#when-to-use-a-jupyter-notebook","text":"A notebook can be used for various goals: Tutorial / gallery Show how some code works (e.g., functions in signal_processing.py or data_encyclopedia.ipynb ) The code should always work We might want to add unit tests for it Prototyping / one-off E.g., We prototype some code, before it becomes library code We did some one-off analysis Analysis Aka \"master\" notebooks The notebook should always work so we need to treat it as part of the code base We might want to add unit tests for it","title":"When to use a Jupyter notebook"},{"location":"Jupyter_notebook_best_practices/#general-structure-of-a-notebook","text":"","title":"General structure of a notebook"},{"location":"Jupyter_notebook_best_practices/#description","text":"At the top of the notebook add a description section explaining a notebook's goal and what it does, e.g., ``` # Description This notebook was used for prototyping / debugging code that was moved in the file abc.py ``` Convert section headers and cells with description text to a Markdown format by selecting a cell and then at Jupyter interface do Cell -> Cell Type -> Markdown","title":"Description"},{"location":"Jupyter_notebook_best_practices/#imports","text":"Add a code section importing the needed libraries Autoreload modules to keep Jupyter and local code updated in real-time Standard imports, e.g. os Third-party imports, e.g. pandas Local imports from our lib It's better to put all the imports in one cell and separate different import types by 1 empty line, e.g.: ``` # Imports %load_ext autoreload %autoreload 2 import logging import os import matplotlib.pyplot as plt import pandas as pd import helpers.dbg as dbg import helpers.env as env import helpers.printing as prnt import core.explore as exp import core.signal_processing as sigp ... ``` In this way executing one cell is enough to configure the notebook","title":"Imports"},{"location":"Jupyter_notebook_best_practices/#configuration","text":"You can configure the notebooks with some utils, logging, and report info on how the notebook was executed (e.g., Git commit, libs, etc.) by using the following cell: ``` # Configure logger. hdbg.init_logger(verbosity=logging.INFO) _LOG = logging.getLogger( name ) # Print system signature. _LOG.info(\"%s\", henv.get_system_signature()[0]) # Configure the notebook style. hprint.config_notebook() ``` The output of the cell looks like: INFO: > cmd='/venv/lib/python3.8/site-packages/ipykernel_launcher.py -f /home/.local/share/jupyter/runtime/kernel-a48f60fd-0f96-48b4-82d8-879385f2be91.json' WARNING: Running in Jupyter DEBUG Effective logging level=10 DEBUG Shut up 1 modules: asyncio DEBUG > (cd . && cd \"$(git rev-parse --show-toplevel)/..\" && (git rev-parse --is-inside-work-tree | grep -q true)) 2>&1 DEBUG > (git rev-parse --show-toplevel) 2>&1 ... # Packages python: 3.8.10 cvxopt: 1.3.0 cvxpy: 1.2.2 gluonnlp: ? gluonts: 0.6.7 joblib: 1.2.0 mxnet: 1.9.1 numpy: 1.23.4 pandas: 1.5.1 pyarrow: 10.0.0 scipy: 1.9.3 seaborn: 0.12.1 sklearn: 1.1.3 statsmodels: 0.13.5","title":"Configuration"},{"location":"Jupyter_notebook_best_practices/#make-the-notebook-flow-clear","text":"Each notebook needs to follow a clear and logical flow, e.g: Load data Compute stats Clean data Compute stats Do analysis Show results The flow should be highlighted using headings in markdown: # Level 1 ## Level 2 ### Level 3 Use the extension for navigating the notebook (see our suggestions for Jupyter plug-ins) Keep related code and analysis close together so: Readers can understand the logical flow One could \"easily\" split the notebook in parts (e.g., when it becomes too big) You can collapse the cells and don't scroll back and forth too much","title":"Make the notebook flow clear"},{"location":"Jupyter_notebook_best_practices/#general-best-practices","text":"","title":"General best practices"},{"location":"Jupyter_notebook_best_practices/#update-calls-only-for-mastergallery-notebooks","text":"","title":"Update calls only for Master/Gallery notebooks"},{"location":"Jupyter_notebook_best_practices/#convention","text":"We do our best to update the calls in the Master/Gallery notebooks but we don't guarantee that the fix is correct For other notebooks we either do the fix (e.g., changing a name of a function) and tweak the call to enforce the old behavior, or even not do anything if there are too many changes","title":"Convention:"},{"location":"Jupyter_notebook_best_practices/#rationale","text":"We have dozens of ad-hoc research notebooks When a piece of code is updated (e.g., ImClient ) the change should be propagated everywhere in the code base, including the notebooks This results in excessive amount of maintenance work which we want to avoid","title":"Rationale:"},{"location":"Jupyter_notebook_best_practices/#keep-code-that-belongs-together-in-one-cell","text":"It's often useful to keep in a cell computation that needs to be always executed together E.g., compute something and then print results In this way a single cell execution computes all data together Often computation starts in multiple cells, e.g., to inline debugging, and once we are more confident that it works correctly we can merge it in a cell (or even better in a function)","title":"Keep code that belongs together in one cell"},{"location":"Jupyter_notebook_best_practices/#write-beautiful-code-even-in-notebooks","text":"Follow the conventions and suggestions for Python code style When prototyping with a notebook, the code can be of lower quality than code, but still needs to be readable and robust In our opinion it's just better to always do write robust and readable code: it doesn't buy much time to cut corners","title":"Write beautiful code, even in notebooks"},{"location":"Jupyter_notebook_best_practices/#show-how-data-is-transformed-as-you-go","text":"Print a few lines of data structures (e.g., df.head(3) ) so one can see how data is transformed through the cells","title":"Show how data is transformed as you go"},{"location":"Jupyter_notebook_best_practices/#use-keyboard-shortcuts","text":"Learn the default keyboard shortcuts to edit efficiently You can use the vim plug-in (see below) and become 3x more ninja","title":"Use keyboard shortcuts"},{"location":"Jupyter_notebook_best_practices/#strive-for-simplicity","text":"Always make the notebook easy to be understood and run by somebody else Explain what happens Organize the code in a logical way Use decent variable names Comment the results, when possible / needed","title":"Strive for simplicity"},{"location":"Jupyter_notebook_best_practices/#dependencies-among-cells","text":"Try to avoid dependencies between cells Even better avoid any dependency between cells, e.g.: Put all the imports in one cell at the beginning, so with one cell execution you can make sure that all the imports are done Compare this approach with the case where the imports are randomly sprinkled in the notebook, then you need to go execute them one by one if you re-initialize the notebook For the same reason group functions in one cell that you can easily re-execute","title":"Dependencies among cells"},{"location":"Jupyter_notebook_best_practices/#re-execute-from-scratch","text":"Once in a while (e.g., once a day) Commit your changes Make sure you can re-execute everything from the top with Kernel -> Restart & Clean output and then Kernel -> Run all Visually verify that the results didn't change, so that there is no weird state or dependency in the code Before a commit (and definitively before a PR) do a clean run","title":"Re-execute from scratch"},{"location":"Jupyter_notebook_best_practices/#add-comments-for-complex-cells","text":"When a cell is too long, explain in a comment what a cell does, e.g., ## Count stocks with all nans. num_nans = np.isnan(rets).sum(axis=0) num_nans /= rets.shape[0] num_nans = num_nans.sort_values(ascending=False) num_stocks_with_no_nans = (num_nans == 0.0).sum() print(\"num_stocks_with_no_nans=%s\" % hprint.perc(num_stocks_with_no_nans, rets.shape[1])) Another approach is to factor out the code in functions with clear names and simplify the flow","title":"Add comments for complex cells"},{"location":"Jupyter_notebook_best_practices/#do-not-cut-paste-code","text":"Cutting + paste + modify is NEVER a good idea It takes more time to clean up cut & paste code than doing right in the first place Just make a function out of the code and call it!","title":"Do not cut &amp; paste code"},{"location":"Jupyter_notebook_best_practices/#avoid-wall-of-code-cell","text":"Obvious","title":"Avoid \"wall-of-code\" cell"},{"location":"Jupyter_notebook_best_practices/#avoid-data-biases","text":"Try to compute statistics on the entire data set so that results are representative and not dependent on a particular slice of the data You can sample the data and check stability of the results If it takes too long to compute the statistics on the entire data set, report the problem and we can think of how to speed it up","title":"Avoid data biases"},{"location":"Jupyter_notebook_best_practices/#avoid-hardwired-constants","text":"Don't use hardwired constants Try to parametrize the code","title":"Avoid hardwired constants"},{"location":"Jupyter_notebook_best_practices/#explain-where-data-is-coming-from","text":"If you are using data from a file (e.g., /data/wd/RP_1yr_13_companies.pkl ), explain in a comment how the file was generated Ideally report a command line to regenerate the data The goal is for other people to be able to re-run the notebook from scratch","title":"Explain where data is coming from"},{"location":"Jupyter_notebook_best_practices/#fix-warnings","text":"A notebook should run without warnings Warnings can't be ignored since they indicate that the code is relying on a feature that will change in the future, e.g., FutureWarning: Sorting because non-concatenation axis is not aligned. A future version of pandas will change to not sort by default. To accept the future behavior, pass 'sort=False'. To retain the current behavior and silence the warning, pass 'sort=True'. Another example: after a cell execution the following warning appears: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy This is a typical pandas warning telling us that we created a view on a dataframe (e.g., by slicing) and we are modifying the underlying data through the view This is dangerous since it can create unexpected side effects and coupling between pieces of code that can be painful to debug and fix If we don't fix the issue now, the next time we create a conda environment the code might either break or (even worse) have a different behavior, i.e., silent failure It's better to fix the warning now that we can verify that the code does what we want to do, instead of fixing it later when we don't remember anymore what exactly we were doing If you have warnings in your code or notebook you can't be sure that the code is doing exactly what you think it is doing For what we know your code might be deleting your hard-disk, moving money from your account to mine, starting World War 3, ... You don't ever want to program by coincidence Typically the warnings are informative and tell us what's the issue and how to fix it, so please fix your code If it's not obvious how to interpret or fix a warning file a bug, file a bug reporting clearly a repro case and the error message","title":"Fix warnings"},{"location":"Jupyter_notebook_best_practices/#make-cells-idempotent","text":"Try to make a notebook cell able of being executed multiple times without changing its output value, e.g., Bad df[\"id\"] = df[\"id\"] + 1 This computation is not idempotent, since if you execute it multiple times is going to increment the column id at every iteration Good df[\"incremented_df\"] = df[\"id\"] + 1 A better approach is to always create a new \"copy\" Another example: Bad tmp = normalize(tmp) Good tmp_after_normalize = normalize(tmp) In this way it's easy to add another stage in the pipeline without changing everything Of course the names tmp_1 , tmp_2 are a horrible idea since they are not self-explanatory and adding a new stage screws up the numbering For data frames and variables is a good idea to create copies of the data along the way: df_without_1s = df[df[\"id\"] != 1].copy()","title":"Make cells idempotent"},{"location":"Jupyter_notebook_best_practices/#always-look-at-the-discarded-data","text":"Filtering the data is a risky operation since once the data is dropped, nobody is going to go back and double check what exactly happened Everything downstream (e.g., all the results, all the conclusions, all the decisions based on those conclusions) rely on the filtering being correct Any time there is a dropna or a filtering / masking operation, e.g.: compu_data = compu_data.dropna(subset=[\"CIK\"]) or selected_metrics = [...] compu_data = compu_data[compu_data[\"item\"].apply(lambda x : x in selected_metrics)] compu_data = compu_data[compu_data[\"datadate\"].apply(date_is_quarter_end)] Always count what percentage of the rows you dropped (e.g., do a back of the envelope check that you are dropping what you would expect) import helpers.printing as hprint ... n_rows = compu_form_df.shape[0] compu_form_df = compu_form_df.drop_duplicates() n_rows_after = compu_form_df.shape[0] _LOG.debug(\"After dropping duplicates kept: %s\", hprint.perc(n_rows_after, n_rows)) Make absolutely sure you are not dropping important data E.g., has the distribution of the data changed in the way you would expect?","title":"Always look at the discarded data"},{"location":"Jupyter_notebook_best_practices/#use-a-progress-bar","text":"Always use progress bars (even in notebooks) so that user can see how long it will take for a certain computation. It is also possible to let tqdm automatically choose between console or notebook versions by using from tqdm.autonotebook import tqdm","title":"Use a progress bar"},{"location":"Jupyter_notebook_best_practices/#notebooks-and-libraries","text":"It's ok to use functions in notebooks when building the analysis to leverage notebook interactivity Once the notebook is \"stable\", often it's better to move the code in a library, i.e., a python file. Make sure you add autoreload modules in Imports section %load_ext autoreload %autoreload 2 Otherwise, if you change a function in the lib, the notebook will not pull this change and use the old version of the function","title":"Notebooks and libraries"},{"location":"Jupyter_notebook_best_practices/#pros","text":"The same notebook code can be used for different notebooks E.g., the function to read the data from disk is an obvious example More people can reuse the same code for different analyses If one changes the code in a library, Git can help tracking changes and merging, while notebooks are difficult to diff / merge Cleaning up / commenting / untangling the code can help reason carefully about the assumptions to find issues The notebook becomes more streamlined and easy to understand since now it's a sequence of functions do_this_and_that and presenting the results One can speed up / parallelize analyses with multiprocessing Notebooks are not great for this E.g., when one does the analyses on a small subset of the data and then wants to run on the entire large dataset The exploratory analysis can be moved towards modeling and then production","title":"Pros"},{"location":"Jupyter_notebook_best_practices/#cons","text":"One have to scroll back and forth between notebook and the libraries to execute the cell with the functions and fix all the possible mistakes","title":"Cons"},{"location":"Jupyter_notebook_best_practices/#recommendations-for-plots","text":"","title":"Recommendations for plots"},{"location":"Jupyter_notebook_best_practices/#use-the-proper-y-scale","text":"If one value can vary from -1.0 to 1.0, force the y-scale between those limits so that the values are absolutes, unless this would squash the plot","title":"Use the proper y-scale"},{"location":"Jupyter_notebook_best_practices/#make-each-plot-self-explanatory","text":"Make sure that each plot has a descriptive title, x and y label Explain the set-up of a plot / analysis E.g., what is the universe of stocks used? What is the period of time? Add this information also to the plots","title":"Make each plot self-explanatory"},{"location":"Jupyter_notebook_best_practices/#avoid-wall-of-text-tables","text":"Try to use plots summarizing the results besides the raw results in a table","title":"Avoid wall-of-text tables"},{"location":"Jupyter_notebook_best_practices/#use-common-axes-to-allow-visual-comparisons","text":"Try to use same axes for multiple graphs when possible to allow visual comparison between graphs If that's not possible or convenient make individual plots with different scales and add a plot with multiple graphs inside on the same axis (e.g., with y-log)","title":"Use common axes to allow visual comparisons"},{"location":"Jupyter_notebook_best_practices/#use-the-right-plot","text":"Pick the right type of graph to make your point pandas , seaborn , matplotlib are your friends","title":"Use the right plot"},{"location":"Jupyter_notebook_best_practices/#useful-plugins","text":"You can access the extensions menu: Edit -> nbextensions config http://localhost:XYZ/nbextensions/","title":"Useful plugins"},{"location":"Jupyter_notebook_best_practices/#vim-bindings","text":"VIM binding will change your life","title":"Vim bindings"},{"location":"Jupyter_notebook_best_practices/#table-of-content","text":"To see the entire logical flow of the notebook, when you use the headers properly","title":"Table of content"},{"location":"Jupyter_notebook_best_practices/#executetime","text":"To see how long each cell takes to execute","title":"ExecuteTime"},{"location":"Jupyter_notebook_best_practices/#spellchecker","text":"To improve your English!","title":"Spellchecker"},{"location":"Jupyter_notebook_best_practices/#autosavetime","text":"To save the code automatically every minute","title":"AutoSaveTime"},{"location":"Jupyter_notebook_best_practices/#notify","text":"Show a browser notification when kernel becomes idle","title":"Notify"},{"location":"Jupyter_notebook_best_practices/#jupytext","text":"We use Jupytext as standard part of our development flow See documentation/general/jupytext.md","title":"Jupytext"},{"location":"Jupyter_notebook_best_practices/#gspread","text":"Allow to read g-sheets in Jupyter Notebook First, one needs to configure Google API, just follow the instructions from here Useful links: Examples of gspread Usage Enabling Gsheet API Adding service email if it\u2019s not working","title":"Gspread"},{"location":"Reading_List/","text":"Reading List Git Bash / Linux Coding Data analysis SRE Git Short tutorial Pro Git book To achieve mastery Bash / Linux Short tutorial Missing semester of CS Coding The Pragmatic Programmer Aka the Black Book Reading and (really) understanding this is equivalent to accumulate 20 years of coding It will change your life The Joel Test We should probably listen to the guy that started StackOverflow Today the 12 steps are obvious, but in 2000 these simple ideas were revolutionary And, yes you are correct noticing that Joel is holding the table tennis racquet incorrectly in the picture Data analysis Python for Data Analysis Reading is not enough: you should have tried all the examples of the book Remember: whatever you want to do, there is a more effective pandas way to do it in one line SRE Site Reliability Engineering \"Members of the SRE team explain how their engagement with the entire software lifecycle has enabled Google to build, deploy, monitor, and maintain some of the largest software systems in the world.\" An outstanding reference drawing on a wealth of experience","title":"Reading List"},{"location":"Reading_List/#reading-list","text":"Git Bash / Linux Coding Data analysis SRE","title":"Reading List"},{"location":"Reading_List/#git","text":"Short tutorial Pro Git book To achieve mastery","title":"Git"},{"location":"Reading_List/#bash-linux","text":"Short tutorial Missing semester of CS","title":"Bash / Linux"},{"location":"Reading_List/#coding","text":"The Pragmatic Programmer Aka the Black Book Reading and (really) understanding this is equivalent to accumulate 20 years of coding It will change your life The Joel Test We should probably listen to the guy that started StackOverflow Today the 12 steps are obvious, but in 2000 these simple ideas were revolutionary And, yes you are correct noticing that Joel is holding the table tennis racquet incorrectly in the picture","title":"Coding"},{"location":"Reading_List/#data-analysis","text":"Python for Data Analysis Reading is not enough: you should have tried all the examples of the book Remember: whatever you want to do, there is a more effective pandas way to do it in one line","title":"Data analysis"},{"location":"Reading_List/#sre","text":"Site Reliability Engineering \"Members of the SRE team explain how their engagement with the entire software lifecycle has enabled Google to build, deploy, monitor, and maintain some of the largest software systems in the world.\" An outstanding reference drawing on a wealth of experience","title":"SRE"},{"location":"Receive_crypto_payment/","text":"Choose a crypto wallet type Public / private keys What do people in the team use Beginner Advanced Choose a crypto currency to receive money in Send a crypto wallet address Do a test transfer Cash out crypto Bank account Cash Choose a crypto wallet type Https://academy.binance.com/en/articles/crypto-wallet-types-explained Https://www.blockchain-council.org/blockchain/types-of-crypto-wallets-explained/ Public / private keys See the explanation here TLDR: Send a public key to receive a transaction, see it as a wallet address Private key is needed to prove ownership or spend the funds associated with your public address, see it as a password What do people in the team use Beginner Mobile / desktop hot wallet Exodus Pros: Easy to set up Has a mobile app which is easy to use Built-in exchange (e.g., allows to exchange BTC to ETH inside the app) Allows to sell crypto and receive fiat money to a bank account via the app, see here Customer support Cons: A swap may include a spread As any hot wallet is vulnerable to hackers attacks It is not recommended to store significant amount of money Advanced If you are experienced and / or own a lot of crypto you might want to have a cold wallet on your laptop, holding the bulk of your bitcoin and transfer among different website on-line wallets. https://bitcoin.org/en/choose-your-wallet Choose a crypto currency to receive money in One can either receive a payment in BTC or in one of the Stablecoins (USDT is a preference) People in the team prefer to use Stablecoins since Stablecoins pursue price stability Send a crypto wallet address Send your crypto wallet address to GP together with an invoice via email. E.g., Hi GP, Please find my invoice for the period [a, b] attached. I would like to receive the money in USDT ERC20. My crypto wallet address is: 0x5ce3d650703f745B9C0cf20E322204b00bF59205 Do a test transfer For the first time GP sends a test transfer (e.g., 100$) just to confirm that a wallet address provided works. After a wallet address is \"verified\" send the full amount (exluding the test transfer amount). Cash out crypto Bank account There are some 3rd party services that buy crypto money from you and send fiat money to your bank account E.g., Built-in Exodus crypto withdrawal service Coinbase cash out serivce Binance P2P Cash There are some crypto ATMs in the USA, see here","title":"Receive crypto payment"},{"location":"Receive_crypto_payment/#choose-a-crypto-wallet-type","text":"Https://academy.binance.com/en/articles/crypto-wallet-types-explained Https://www.blockchain-council.org/blockchain/types-of-crypto-wallets-explained/","title":"Choose a crypto wallet type"},{"location":"Receive_crypto_payment/#public-private-keys","text":"See the explanation here TLDR: Send a public key to receive a transaction, see it as a wallet address Private key is needed to prove ownership or spend the funds associated with your public address, see it as a password","title":"Public / private keys"},{"location":"Receive_crypto_payment/#what-do-people-in-the-team-use","text":"","title":"What do people in the team use"},{"location":"Receive_crypto_payment/#beginner","text":"Mobile / desktop hot wallet Exodus Pros: Easy to set up Has a mobile app which is easy to use Built-in exchange (e.g., allows to exchange BTC to ETH inside the app) Allows to sell crypto and receive fiat money to a bank account via the app, see here Customer support Cons: A swap may include a spread As any hot wallet is vulnerable to hackers attacks It is not recommended to store significant amount of money","title":"Beginner"},{"location":"Receive_crypto_payment/#advanced","text":"If you are experienced and / or own a lot of crypto you might want to have a cold wallet on your laptop, holding the bulk of your bitcoin and transfer among different website on-line wallets. https://bitcoin.org/en/choose-your-wallet","title":"Advanced"},{"location":"Receive_crypto_payment/#choose-a-crypto-currency-to-receive-money-in","text":"One can either receive a payment in BTC or in one of the Stablecoins (USDT is a preference) People in the team prefer to use Stablecoins since Stablecoins pursue price stability","title":"Choose a crypto currency to receive money in"},{"location":"Receive_crypto_payment/#send-a-crypto-wallet-address","text":"Send your crypto wallet address to GP together with an invoice via email. E.g., Hi GP, Please find my invoice for the period [a, b] attached. I would like to receive the money in USDT ERC20. My crypto wallet address is: 0x5ce3d650703f745B9C0cf20E322204b00bF59205","title":"Send a crypto wallet address"},{"location":"Receive_crypto_payment/#do-a-test-transfer","text":"For the first time GP sends a test transfer (e.g., 100$) just to confirm that a wallet address provided works. After a wallet address is \"verified\" send the full amount (exluding the test transfer amount).","title":"Do a test transfer"},{"location":"Receive_crypto_payment/#cash-out-crypto","text":"","title":"Cash out crypto"},{"location":"Receive_crypto_payment/#bank-account","text":"There are some 3rd party services that buy crypto money from you and send fiat money to your bank account E.g., Built-in Exodus crypto withdrawal service Coinbase cash out serivce Binance P2P","title":"Bank account"},{"location":"Receive_crypto_payment/#cash","text":"There are some crypto ATMs in the USA, see here","title":"Cash"},{"location":"Scrum_methodology/","text":"Scrum Methodology Roles Goal of Scrum methodology Metaphor for the roles in terms of a race car ScrumMaster ProductOwner DevTeam Artifacts Product backlog Product backlog items Complexity of PBI High-priority vs lower-priority tasks Sprint backlog The burndown The meetings Planning meeting Part one of sprint planning meeting Part two of sprint planning meeting Daily scrum Daily scrum: questions What the daily scrum is not Sprint review Sprint retrospective Sprint retrospective: questions From \"Lacey, The Scrum Field Guide: Practical Advice for Your First Year, 2012\" Roles Goal of Scrum methodology Work in the interests of customers and stakeholders to turn the vision into a working product Metaphor for the roles in terms of a race car ProductOwner = driver DevTeam = engine ScrumMaster = lubricants and sensors ScrumMaster Identify when the team is not performing to its ability Assist in correcting the issues Notice non-verbal cues Is comfortable with conflict Can build trust and earn respect ProductOwner Represent the customers Point the car in the correct direction Adjust the car direction to stay on course Make decisions about official release Ultimately he is responsible for success or failure of the projects Decide: What is developed When it is developed Whether the product meets expectations DevTeam Aka Team, Development team, Core team Developers, testers, architects, designers Cross-functionality is a good thing The ideal team size is 6 plus / minus 2 Artifacts Product backlog = master list of all features and functionalities needed to implement the vision into the product The ProductOwner keeps the backlog: Prioritized Up to date Clear The backlog is never complete: Items are added and removed Reordered based on priority, value, or risk Product backlog items Aka PBI E.g., bugs, features, enhancements, non-functional requirements Complexity of PBI ProductOwner and the DevTeam estimate the size of each task The complexity of each task can be expressed in different ways: Points T-shirt size (S, M, L, XL) High-priority vs lower-priority tasks High-priority stories should be small and clear So they can be brought into the sprint Lower-priority items can be large and fuzzy Bigger stories are decomposed into smaller chunks Sprint backlog = output of the planning meeting List of tasks that need to complete during the sprint Sprint backlog tasks have an estimate in hours The DevTeam keeps the sprint backlog up to date During a sprint New tasks are discovered Tasks are adjusted (in terms of description or estimated hours) Tasks are marked as done The burndown Communicate how much work is remaining and what is the team velocity It is updated at the end of each day Plot the number of hours remaining (y-axis) against the number of days remaining (x-axis) The meetings Planning meeting Each sprint begins with a sprint planning attended by the team, ScrumMaster, ProductOwner Typically one needs two hours per number of weeks to plan the sprint For a 1-month sprint, 8 hours of meeting For 2-week sprint, 4 hours of meeting Part one of sprint planning meeting Review of potential product backlog items for the sprint ProductOwner describes what the goal of the meeting is DevTeam asks questions to drive away ambiguity Outcome is one-sentence description of the desired outcome of the sprint Part two of sprint planning meeting Many DevTeams discuss how to implement the tasks The ProductOwner doesn't need to be present The ScrumMaster can be present facilitating the process The DevTeam discusses and decides the implementation of the tasks Decompose backlog items into work tasks Estimate tasks in terms of hours Daily scrum Aka daily stand-up Give the DevTeam the opportunity to sync daily, at the same time, and at the same place Daily scrum: questions The 3 most frequent questions are: What have you accomplished since the last meeting? What will you accomplish today? What obstacles are in your way? What the daily scrum is not The daily scrum is not a deep-dive problem-solving meeting Any other issues need to be taken offline It is not a status report meeting to the ScrumMaster The purpose is for the DevTeam members to talk to each other The ProductOwner is in \"listen-only\" mode Sprint review On the last day of the sprint, the DevTeam holds a sprint review Everybody should join ScrumMaster ProductOwner DevTeam Customers, key stakeholders Executives DevTeam Recaps the goal of the sprint Presents the work done Customers Review the progress made on the project Accept changes Ask for changes Sprint retrospective After the sprint review, the retrospective is a way to identify how to improve process and execution Sprint retrospective: questions What went well during the sprint? What could be improved in the next sprint?","title":"Scrum Methodology"},{"location":"Scrum_methodology/#scrum-methodology","text":"Roles Goal of Scrum methodology Metaphor for the roles in terms of a race car ScrumMaster ProductOwner DevTeam Artifacts Product backlog Product backlog items Complexity of PBI High-priority vs lower-priority tasks Sprint backlog The burndown The meetings Planning meeting Part one of sprint planning meeting Part two of sprint planning meeting Daily scrum Daily scrum: questions What the daily scrum is not Sprint review Sprint retrospective Sprint retrospective: questions From \"Lacey, The Scrum Field Guide: Practical Advice for Your First Year, 2012\"","title":"Scrum Methodology"},{"location":"Scrum_methodology/#roles","text":"","title":"Roles"},{"location":"Scrum_methodology/#goal-of-scrum-methodology","text":"Work in the interests of customers and stakeholders to turn the vision into a working product","title":"Goal of Scrum methodology"},{"location":"Scrum_methodology/#metaphor-for-the-roles-in-terms-of-a-race-car","text":"ProductOwner = driver DevTeam = engine ScrumMaster = lubricants and sensors","title":"Metaphor for the roles in terms of a race car"},{"location":"Scrum_methodology/#scrummaster","text":"Identify when the team is not performing to its ability Assist in correcting the issues Notice non-verbal cues Is comfortable with conflict Can build trust and earn respect","title":"ScrumMaster"},{"location":"Scrum_methodology/#productowner","text":"Represent the customers Point the car in the correct direction Adjust the car direction to stay on course Make decisions about official release Ultimately he is responsible for success or failure of the projects Decide: What is developed When it is developed Whether the product meets expectations","title":"ProductOwner"},{"location":"Scrum_methodology/#devteam","text":"Aka Team, Development team, Core team Developers, testers, architects, designers Cross-functionality is a good thing The ideal team size is 6 plus / minus 2","title":"DevTeam"},{"location":"Scrum_methodology/#artifacts","text":"","title":"Artifacts"},{"location":"Scrum_methodology/#product-backlog","text":"= master list of all features and functionalities needed to implement the vision into the product The ProductOwner keeps the backlog: Prioritized Up to date Clear The backlog is never complete: Items are added and removed Reordered based on priority, value, or risk","title":"Product backlog"},{"location":"Scrum_methodology/#product-backlog-items","text":"Aka PBI E.g., bugs, features, enhancements, non-functional requirements","title":"Product backlog items"},{"location":"Scrum_methodology/#complexity-of-pbi","text":"ProductOwner and the DevTeam estimate the size of each task The complexity of each task can be expressed in different ways: Points T-shirt size (S, M, L, XL)","title":"Complexity of PBI"},{"location":"Scrum_methodology/#high-priority-vs-lower-priority-tasks","text":"High-priority stories should be small and clear So they can be brought into the sprint Lower-priority items can be large and fuzzy Bigger stories are decomposed into smaller chunks","title":"High-priority vs lower-priority tasks"},{"location":"Scrum_methodology/#sprint-backlog","text":"= output of the planning meeting List of tasks that need to complete during the sprint Sprint backlog tasks have an estimate in hours The DevTeam keeps the sprint backlog up to date During a sprint New tasks are discovered Tasks are adjusted (in terms of description or estimated hours) Tasks are marked as done","title":"Sprint backlog"},{"location":"Scrum_methodology/#the-burndown","text":"Communicate how much work is remaining and what is the team velocity It is updated at the end of each day Plot the number of hours remaining (y-axis) against the number of days remaining (x-axis)","title":"The burndown"},{"location":"Scrum_methodology/#the-meetings","text":"","title":"The meetings"},{"location":"Scrum_methodology/#planning-meeting","text":"Each sprint begins with a sprint planning attended by the team, ScrumMaster, ProductOwner Typically one needs two hours per number of weeks to plan the sprint For a 1-month sprint, 8 hours of meeting For 2-week sprint, 4 hours of meeting","title":"Planning meeting"},{"location":"Scrum_methodology/#part-one-of-sprint-planning-meeting","text":"Review of potential product backlog items for the sprint ProductOwner describes what the goal of the meeting is DevTeam asks questions to drive away ambiguity Outcome is one-sentence description of the desired outcome of the sprint","title":"Part one of sprint planning meeting"},{"location":"Scrum_methodology/#part-two-of-sprint-planning-meeting","text":"Many DevTeams discuss how to implement the tasks The ProductOwner doesn't need to be present The ScrumMaster can be present facilitating the process The DevTeam discusses and decides the implementation of the tasks Decompose backlog items into work tasks Estimate tasks in terms of hours","title":"Part two of sprint planning meeting"},{"location":"Scrum_methodology/#daily-scrum","text":"Aka daily stand-up Give the DevTeam the opportunity to sync daily, at the same time, and at the same place","title":"Daily scrum"},{"location":"Scrum_methodology/#daily-scrum-questions","text":"The 3 most frequent questions are: What have you accomplished since the last meeting? What will you accomplish today? What obstacles are in your way?","title":"Daily scrum: questions"},{"location":"Scrum_methodology/#what-the-daily-scrum-is-not","text":"The daily scrum is not a deep-dive problem-solving meeting Any other issues need to be taken offline It is not a status report meeting to the ScrumMaster The purpose is for the DevTeam members to talk to each other The ProductOwner is in \"listen-only\" mode","title":"What the daily scrum is not"},{"location":"Scrum_methodology/#sprint-review","text":"On the last day of the sprint, the DevTeam holds a sprint review Everybody should join ScrumMaster ProductOwner DevTeam Customers, key stakeholders Executives DevTeam Recaps the goal of the sprint Presents the work done Customers Review the progress made on the project Accept changes Ask for changes","title":"Sprint review"},{"location":"Scrum_methodology/#sprint-retrospective","text":"After the sprint review, the retrospective is a way to identify how to improve process and execution","title":"Sprint retrospective"},{"location":"Scrum_methodology/#sprint-retrospective-questions","text":"What went well during the sprint? What could be improved in the next sprint?","title":"Sprint retrospective: questions"},{"location":"Signing_up_for_Sorrentum/","text":"Signing up to the project Look around the repo and make sure that you are really interested in what we are doing, and you have time to contribute Ponder on the IMPORTANT note about committing to contribute to the project here Please fork, star, watch the Sorrentum repo so that GitHub promotes our repo (this will help us promote our effort) Fill out the Contributor Info form . It's meant to just get basic contact info and technical skills about you Don\u2019t worry: by working with us, you will quickly become a coding machine Communication with the project teams happens through: GitHub on the repo like in any open-source development Telegram IM channel for higher bandwidth discussion Through the Sorrentum mailing list Accept the invite from GitHub that we will send you Again please don't forget to fork, star, watch the Sorrentum repo so that GitHub promotes our repo Accept the invite to join the Google Drive that we will send you Subscribe to the Telegram channel Send a request, you will be added to the group after admin's approval On-boarding Tasks Once the invitation is accepted, an issue will be created by the title On-board \\<YOUR FIRST NAME LAST NAME\\> Aka <YOUR GITHUB_HANDLE> . If not assinged, assign it to yourself and go through the particular checklist from the issue one-by-one, marking each item as done when it's actually done .","title":"Signing up to the project"},{"location":"Signing_up_for_Sorrentum/#signing-up-to-the-project","text":"Look around the repo and make sure that you are really interested in what we are doing, and you have time to contribute Ponder on the IMPORTANT note about committing to contribute to the project here Please fork, star, watch the Sorrentum repo so that GitHub promotes our repo (this will help us promote our effort) Fill out the Contributor Info form . It's meant to just get basic contact info and technical skills about you Don\u2019t worry: by working with us, you will quickly become a coding machine Communication with the project teams happens through: GitHub on the repo like in any open-source development Telegram IM channel for higher bandwidth discussion Through the Sorrentum mailing list Accept the invite from GitHub that we will send you Again please don't forget to fork, star, watch the Sorrentum repo so that GitHub promotes our repo Accept the invite to join the Google Drive that we will send you Subscribe to the Telegram channel Send a request, you will be added to the group after admin's approval On-boarding Tasks Once the invitation is accepted, an issue will be created by the title On-board \\<YOUR FIRST NAME LAST NAME\\> Aka <YOUR GITHUB_HANDLE> . If not assinged, assign it to yourself and go through the particular checklist from the issue one-by-one, marking each item as done when it's actually done .","title":"Signing up to the project"},{"location":"Sorrentum_development_setup/","text":"Supporting OS Getting a first issue to work on (aka warm-up issue) How to contribute code (short version) Cloning the code Sorrentum Dev Docker container (aka dev container, cmamp container) Supporting OS We support Mac x86, Apple Silicon and Linux Ubuntu If you are using Windows, Install VMWare software Reference video for installing ubuntu on VMWare software Make sure you set up your git and github Install docker on your Ubuntu VM Getting a first issue to work on (aka warm-up issue) The goal is to get comfortable with the development system We mark certain bugs as \"good-as-first-bug\" Typical warm-up issues are: Write unit tests Copy-paste-modify (ideally refactor and then change) code Simple refactoring How to contribute code (short version) Make sure you are familiar with our coding style Create a branch of your assigned issues/bugs E.g., for a GitHub issue with the name \"Expose the linter container to Sorrentum contributors #63\", the branch name should be SorrTask63_Expose_the_linter_container_to_Sorrentum_contributors This step is automated through the invoke flow (see docs for more info) Run the linter on your code before pushing. Do git commit and git push together so the latest changes are readily visible Make sure your branch is up-to-date with the master branch Create a Pull Request (PR) from your branch Add your assigned reviewers for your PR so that they are informed of your PR After being reviewed, the PR will be merged to the master branch by your reviewers Cloning the code All the source code should go under ~/src (e.g., /Users/<YOUR_USER>/src on a Mac PC) The path to the local repo folder should look like this ~/src/{REPO_NAME}{IDX} where IDX is an integer REPO_NAME is a name of the repository To clone the repo, use the cloning command described in the Github official documentation Example of cloning command: ``` # Sometimes it does not work. git clone git@github.com:sorrentum/sorrentum.git ~/src/sorrentum1 # Alternative command. git clone https://github.com/sorrentum/sorrentum.git ~/src/sorrentum1 ``` Sorrentum Dev Docker container (aka dev container, cmamp container) Get familiar with Docker if you are not (e.g., https://docs.docker.com/get-started/overview/) We work in a Docker container that has all the required dependencies installed You can use PyCharm / VS code on your laptop to edit code, but you want to run code inside the dev container since this makes sure everyone is running with the same system, and it makes it easy to share code and reproduce problems Install Docker Desktop on your PC Links: Mac Linux Windows Check the installation by running: ``` docker pull hello-world Using default tag: latest latest: Pulling from library/hello-world Digest: sha256:fc6cf906cbfa013e80938cdf0bb199fbdbb86d6e3e013783e5a766f50f5dbce0 Status: Image is up to date for hello-world:latest docker.io/library/hello-world:latest ``` Common problems with Docker Mac DNS problem, try step 5 from the article and repeat the cmd below: ``` docker pull hello-world Error response from daemon: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers) ``` Linux sudo problem, see here for the solution ``` docker pull hello-world Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.40/containers/json: dial unix /var/run/docker.sock: connect: permission denied ``` Thin environment Build the thin environment; this is done once ``` source dev_scripts/client_setup/build.sh ``` Activate the thin environment; make sure it is always activated ``` source dev_scripts/setenv_amp.sh ``` If you see output like this, your environment is successfully built! If not and you encounter any issues, please post them under your designated on-boarding issue alias sp='echo '\\''source ~/.profile'\\''; source ~/.profile' alias vi='/usr/bin/vi' alias vim='/usr/bin/vi' alias vimdiff='/usr/bin/vi -d' alias vip='vim -c \"source ~/.vimrc_priv\"' alias w='which' ==> SUCCESS <== Docker image Pull the latest cmamp image; this is done once ``` i docker_pull or docker pull sorrentum/cmamp:latest ``` Pull the latest dev_tools image; this is done onec ``` i docker_pull_dev_tools or docker pull sorrentum/dev_tools:prod ``` Git Get the latest version of master ``` # To update your feature branch with the latest changes from master run # the cmd below from a feature branch, i.e. not from master. i git_merge_master # If you are on master just pull the remote changes. i git_pull ``` Basic Docker commands Start a Docker container ``` i docker_bash ``` Ignore all the warnings that do not prevent you from running the tests, e.g., WARNING: The AM_AWS_ACCESS_KEY_ID variable is not set. Defaulting to a blank string. WARNING: The AM_AWS_DEFAULT_REGION variable is not set. Defaulting to a blank string. WARNING: The AM_AWS_SECRET_ACCESS_KEY variable is not set. Defaulting to a blank string. WARNING: The AM_FORCE_TEST_FAIL variable is not set. Defaulting to a blank string. WARNING: The CK_AWS_ACCESS_KEY_ID variable is not set. Defaulting to a blank string. WARNING: The CK_AWS_DEFAULT_REGION variable is not set. Defaulting to a blank string. WARNING: The CK_AWS_SECRET_ACCESS_KEY variable is not set. Defaulting to a blank string. WARNING: The CK_TELEGRAM_TOKEN variable is not set. Defaulting to a blank string. ``` This code is not in sync with the container: code_version='1.4.1' != container_version='1.4.0' You need to: - merge origin/master into your branch with invoke git_merge_master - pull the latest container with invoke docker_pull ``` Start a Jupyter server ``` i docker_jupyter ``` To open a Jupyter notebook in a local web-browser: In the output from the cmd above find an assigned port, e.g., [I 14:52:26.824 NotebookApp] http://0044e866de8d:10091/ -> port is 10091 Add the port to the link like so: http://localhost:10091/ or http://127.0.0.1:10091 Copy-paste the link into a web-browser and update the page","title":"Sorrentum development setup"},{"location":"Sorrentum_development_setup/#supporting-os","text":"We support Mac x86, Apple Silicon and Linux Ubuntu If you are using Windows, Install VMWare software Reference video for installing ubuntu on VMWare software Make sure you set up your git and github Install docker on your Ubuntu VM","title":"Supporting OS"},{"location":"Sorrentum_development_setup/#getting-a-first-issue-to-work-on-aka-warm-up-issue","text":"The goal is to get comfortable with the development system We mark certain bugs as \"good-as-first-bug\" Typical warm-up issues are: Write unit tests Copy-paste-modify (ideally refactor and then change) code Simple refactoring","title":"Getting a first issue to work on (aka warm-up issue)"},{"location":"Sorrentum_development_setup/#how-to-contribute-code-short-version","text":"Make sure you are familiar with our coding style Create a branch of your assigned issues/bugs E.g., for a GitHub issue with the name \"Expose the linter container to Sorrentum contributors #63\", the branch name should be SorrTask63_Expose_the_linter_container_to_Sorrentum_contributors This step is automated through the invoke flow (see docs for more info) Run the linter on your code before pushing. Do git commit and git push together so the latest changes are readily visible Make sure your branch is up-to-date with the master branch Create a Pull Request (PR) from your branch Add your assigned reviewers for your PR so that they are informed of your PR After being reviewed, the PR will be merged to the master branch by your reviewers","title":"How to contribute code (short version)"},{"location":"Sorrentum_development_setup/#cloning-the-code","text":"All the source code should go under ~/src (e.g., /Users/<YOUR_USER>/src on a Mac PC) The path to the local repo folder should look like this ~/src/{REPO_NAME}{IDX} where IDX is an integer REPO_NAME is a name of the repository To clone the repo, use the cloning command described in the Github official documentation Example of cloning command: ``` # Sometimes it does not work. git clone git@github.com:sorrentum/sorrentum.git ~/src/sorrentum1 # Alternative command. git clone https://github.com/sorrentum/sorrentum.git ~/src/sorrentum1 ```","title":"Cloning the code"},{"location":"Sorrentum_development_setup/#sorrentum-dev-docker-container-aka-dev-container-cmamp-container","text":"Get familiar with Docker if you are not (e.g., https://docs.docker.com/get-started/overview/) We work in a Docker container that has all the required dependencies installed You can use PyCharm / VS code on your laptop to edit code, but you want to run code inside the dev container since this makes sure everyone is running with the same system, and it makes it easy to share code and reproduce problems Install Docker Desktop on your PC Links: Mac Linux Windows Check the installation by running: ``` docker pull hello-world Using default tag: latest latest: Pulling from library/hello-world Digest: sha256:fc6cf906cbfa013e80938cdf0bb199fbdbb86d6e3e013783e5a766f50f5dbce0 Status: Image is up to date for hello-world:latest docker.io/library/hello-world:latest ``` Common problems with Docker Mac DNS problem, try step 5 from the article and repeat the cmd below: ``` docker pull hello-world Error response from daemon: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers) ``` Linux sudo problem, see here for the solution ``` docker pull hello-world Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.40/containers/json: dial unix /var/run/docker.sock: connect: permission denied ``` Thin environment Build the thin environment; this is done once ``` source dev_scripts/client_setup/build.sh ``` Activate the thin environment; make sure it is always activated ``` source dev_scripts/setenv_amp.sh ``` If you see output like this, your environment is successfully built! If not and you encounter any issues, please post them under your designated on-boarding issue alias sp='echo '\\''source ~/.profile'\\''; source ~/.profile' alias vi='/usr/bin/vi' alias vim='/usr/bin/vi' alias vimdiff='/usr/bin/vi -d' alias vip='vim -c \"source ~/.vimrc_priv\"' alias w='which' ==> SUCCESS <== Docker image Pull the latest cmamp image; this is done once ``` i docker_pull or docker pull sorrentum/cmamp:latest ``` Pull the latest dev_tools image; this is done onec ``` i docker_pull_dev_tools or docker pull sorrentum/dev_tools:prod ``` Git Get the latest version of master ``` # To update your feature branch with the latest changes from master run # the cmd below from a feature branch, i.e. not from master. i git_merge_master # If you are on master just pull the remote changes. i git_pull ``` Basic Docker commands Start a Docker container ``` i docker_bash ``` Ignore all the warnings that do not prevent you from running the tests, e.g., WARNING: The AM_AWS_ACCESS_KEY_ID variable is not set. Defaulting to a blank string. WARNING: The AM_AWS_DEFAULT_REGION variable is not set. Defaulting to a blank string. WARNING: The AM_AWS_SECRET_ACCESS_KEY variable is not set. Defaulting to a blank string. WARNING: The AM_FORCE_TEST_FAIL variable is not set. Defaulting to a blank string. WARNING: The CK_AWS_ACCESS_KEY_ID variable is not set. Defaulting to a blank string. WARNING: The CK_AWS_DEFAULT_REGION variable is not set. Defaulting to a blank string. WARNING: The CK_AWS_SECRET_ACCESS_KEY variable is not set. Defaulting to a blank string. WARNING: The CK_TELEGRAM_TOKEN variable is not set. Defaulting to a blank string. ``` This code is not in sync with the container: code_version='1.4.1' != container_version='1.4.0' You need to: - merge origin/master into your branch with invoke git_merge_master - pull the latest container with invoke docker_pull ``` Start a Jupyter server ``` i docker_jupyter ``` To open a Jupyter notebook in a local web-browser: In the output from the cmd above find an assigned port, e.g., [I 14:52:26.824 NotebookApp] http://0044e866de8d:10091/ -> port is 10091 Add the port to the link like so: http://localhost:10091/ or http://127.0.0.1:10091 Copy-paste the link into a web-browser and update the page","title":"Sorrentum Dev Docker container (aka dev container, cmamp container)"},{"location":"Telegram/","text":"Telegram General Secret vs Regular chats Secret Regular Username Google meet room General We use Telegram for Discussions that need tight interaction (like a debug session) immediacy (e.g., \"are you ready for the sync up?\") Github Actions notifications from Telegram bots E.g., regressions fail in one of our repos Secret vs Regular chats Secret We use secret chats for private one-on-one communication We prefer to send all the sensitive information using encrypted chats A Secret chat uses end-to-end encryption which means that even Telegram does not have access to a conversion How to start secret chat - see here Limitations: Telegram desktop version does not support secret chats so one uses secret chats only using his/her phone You cannot add multiple people to a secret chat, only one-on-one communication is allowed All secret chats in Telegram are device-specific and are not part of the Telegram cloud. This means you can only access messages in a secret chat from their device of origin. Regular We use regular chats for General discussions within a team Group chats We do not share sensitive information via regular chats Username We ask everyone to set a username so that is easier to find a person See the instructions here Google meet room It is always nice to pin a google meeting room in a chat We usually use -> as an invitation to join a google meet room pinned in a chat","title":"Telegram"},{"location":"Telegram/#telegram","text":"General Secret vs Regular chats Secret Regular Username Google meet room","title":"Telegram"},{"location":"Telegram/#general","text":"We use Telegram for Discussions that need tight interaction (like a debug session) immediacy (e.g., \"are you ready for the sync up?\") Github Actions notifications from Telegram bots E.g., regressions fail in one of our repos","title":"General"},{"location":"Telegram/#secret-vs-regular-chats","text":"","title":"Secret vs Regular chats"},{"location":"Telegram/#secret","text":"We use secret chats for private one-on-one communication We prefer to send all the sensitive information using encrypted chats A Secret chat uses end-to-end encryption which means that even Telegram does not have access to a conversion How to start secret chat - see here Limitations: Telegram desktop version does not support secret chats so one uses secret chats only using his/her phone You cannot add multiple people to a secret chat, only one-on-one communication is allowed All secret chats in Telegram are device-specific and are not part of the Telegram cloud. This means you can only access messages in a secret chat from their device of origin.","title":"Secret"},{"location":"Telegram/#regular","text":"We use regular chats for General discussions within a team Group chats We do not share sensitive information via regular chats","title":"Regular"},{"location":"Telegram/#username","text":"We ask everyone to set a username so that is easier to find a person See the instructions here","title":"Username"},{"location":"Telegram/#google-meet-room","text":"It is always nice to pin a google meeting room in a chat We usually use -> as an invitation to join a google meet room pinned in a chat","title":"Google meet room"},{"location":"Tools-PyCharm/","text":"Running PyCharm remotely There are multiple ways to develop on a remote server using PyCharm VNC approach PyCharm runs locally on the server using a \"virtual screen\" Your laptop interacts with a VNC server to get the GUI locally Pros: Everything works You can run anything like you are local on the server, since you are in practice just using a virtual screen Cons: Without enough bandwidth it's slow and not snappy enough X11 approach Same as VNC, but instead of sending bitmaps through VNC, a \"compressed\" version of the GUI is sent to the local computer directly Pros: Maybe faster than VNC PyCharm window is like a native window on your laptop Cons: X11 is old crap developed long time again and not really supported any more One needs to tunnel X11 traffic, set things up, and so on PyCharm Gateway New client-server architecture for PyCharm A \"headless\" PyCharm runs on the server A GUI client PyCharm runs on your laptop Pros It's as fast as possible, probably as fast as running locally Cons Need a PyCharm pro license (not a problem, we have money) It's not super polished: kind of beta, but it will get better and better PyCharm Remote Set-up This is described below in PyCharm - Advanced tip and tricks Edit locally and then PyCharm moves the files back and forth Pros Only requires ssh Cons You can't run / debug remotely Current situation Approach 1) seems to require lots of memory and CPU and it's not really fast. Approach 2) works but it's a pain to set-up and slow. We want to try with 3) TODO(gp): @Juraj pls a short tutorial on how to install TODO(gp): @Juraj understand if it works, if it's fast, and if it requires less memory How to run our cmamp container directly from PyCharm PyCharm allows to run commands directly inside a container See https://www.jetbrains.com/help/pycharm/using-docker-as-a-remote-interpreter.html In fact when we do i docker_bash we launch a container and run bash inside it, but PyCharm can do the same thing TODO(gp): @Juraj Let's both try this. There are some notes below about it How to review a PR inside Pycharm CTRL + SHIFT + A -> View Pull Request {width=\"3.612198162729659in\" height=\"4.932292213473316in\"} How to edit remote code You need to use a certain local directory (e.g., /Users/saggese/src/commodity_research1) and a remote directory (e.g., /wd/saggese/src/commodity_research1) They need to be synced at the same git branch (e.g., master or AmpTask1112_Audit_amp_Docker_system_03) Set-up Deployment {width=\"4.810558836395451in\" height=\"3.51540791776028in\"} {width=\"3.9114588801399823in\" height=\"1.6807655293088364in\"} The deployment options are {width=\"3.123700787401575in\" height=\"2.78830271216098in\"} You can see what file is changed in the file transfer window: {width=\"6.5in\" height=\"1.2083333333333333in\"} pycharm Develop on one node, sync, run on the server Run local application with venv Database Run application inside Docker Run application remotely inside Docker General ssh config File | Settings | Tools | SSH Configurations {width=\"3.802863079615048in\" height=\"1.4626399825021872in\"} Once setup, ssh config can be used for all tools in PyCharm. Remote Interpreter DataGrip Deployment Etc. DB connection via ssh Note: PyCharm Professional DataGrip is used as an example. There are numerous open source alternatives such as Beaver . Config below should apply to them also. To add a new data source in DataGrip, go to the database section in the lower left corner. {width=\"2.7734372265966756in\" height=\"2.7734372265966756in\"} Then pick your desired data source from the dropdown in the upper right corner. {width=\"4.164062773403325in\" height=\"2.028646106736658in\"} You will be presented with a dummy config that needs to be replaced with proper data as shown below. {width=\"4.158922790901137in\" height=\"3.0320844269466316in\"} Before that is done, be sure that proper ssh info is added in SSH/SSL section. {width=\"2.776042213473316in\" height=\"0.598069772528434in\"} Deployment with remote repository (through sync) Note: Before setting up deployment, pull the cmamp repo on EC2 instance and use the same name as on your local machine (example: cmamp1). Always try to keep both repos in sync via git. For more subtle and simpler changes use File | Reload All From Disk . This will upload changes to the remote repo. Tools | Deployment | Configuration {width=\"4.893396762904637in\" height=\"2.0781255468066493in\"} {width=\"4.838542213473316in\" height=\"1.1398490813648294in\"} Tools | Deployment | Options {width=\"4.671411854768154in\" height=\"3.2864588801399823in\"} Uncheck \"Skip external changes\" and check \"Delete remote files\" Tools | Deployment | Automatic Upload Check it Tools | Deployment | Browse Remote Host {width=\"5.394192913385827in\" height=\"1.765625546806649in\"} PUDB - remote debugging - ToDo How to run tests inside a container https://www.jetbrains.com/help/pycharm/using-docker-compose-as-a-remote-interpreter.html#docker-compose-remote Note that the \"start SSH session...\" action is available only in PyCharm Professional Edition, while the terminal itself is available in both Professional and Community editions. Installing PyCharm Professional Windows Download the installer using this link Run the installer and follow the wizard steps. To run PyCharm, find it in the Windows Start menu or use the desktop shortcut. macOS There are separate disk images for Intel and Apple Silicon processors. Download the image, based on your processor using this link Mount the image and drag the PyCharm app to the Applications folder. Run the PyCharm app from the Applications directory, Launchpad, or Spotlight. Linux **Using tar archives ** Download the tar archive using this link Unpack the pycharm-*.tar.gz file to a different folder, if your current Download folder doesn't support file execution: ``` tar xzf pycharm-*.tar.gz -C `` ``` The recommended installation location according to the filesystem hierarchy standard (FHS) is /opt . To install PyCharm into this directory, enter the following command: > sudo tar xzf pycharm-\\*.tar.gz -C /opt/ Switch to the bin subdirectory: ``` cd /pycharm-*/bin # E.g., cd /opt/pycharm-*/bin ``` Run pycharm.sh from the bin subdirectory ``` sh pycharm.sh ``` Using snap packages For Ubuntu 16.04 and later, you can use snap packages to install PyCharm. ``` sudo snap install pycharm-professional --classic # or sudo snap install pycharm-community --classic ``` Run in the Terminalu ``` pycharm-professional # or pycharm-community # or pycharm-educational ``` Connecting via PyCharm gateway (SSH) The first thing you need to do is sign up for a free trial license or use it if it already have Then make sure you have a VPN connection to our VPC Click on Connect via SSH Into Username: write < > Example: richard Into Host: write < > Example: 172.30.2.136 Mark the Specify private key check box and locate the private key from the zip which was sent in the onboarding process. Example: crypto.pub Leave Port: 22 as it is. Click on Check Connection and Continue. Select IDE version: PyCharm Py 213.6777.x Locate your directory. Example: /data/richard Click on Download and Start IDE. Connecting via VNC Make sure you have a VPN connection. Installing VNC Install VNC using this link: https://www.realvnc.com/en/connect/download/viewer/windows/ Sysadmin has sent you: os_password.txt your username $USER a key crypto.pub that looks like: -----BEGIN OPENSSH PRIVATE KEY----- b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAABlwAAAAdzc2gtcn NhAAAAAwEAAQAAAYEA0IQsLy1lL3bhPT+43sht2/m9tqZm8sEQrXMAVtfm4ji/LXMr7094 ... hakqVTlQ2sr0YTAAAAHnNhZ2dlc2VAZ3BtYWMuZmlvcy1yb3V0ZXIuaG9tZQECAwQ= -----END OPENSSH PRIVATE KEY----- Let's say you are connected via VNC. Login into the OS. Run pycharm.sh using terminal (should be there) ``` bash /opt/pycharm-community-2021.2.3/bin/pycharm.sh ``` Configuration Reflow Set the reflow to reindent {width=\"6.5in\" height=\"4.486111111111111in\"} Some recommended plug-ins vim Grazie Wrap-to-column GitHub Copilot","title":"Running PyCharm remotely"},{"location":"Tools-PyCharm/#running-pycharm-remotely","text":"There are multiple ways to develop on a remote server using PyCharm VNC approach PyCharm runs locally on the server using a \"virtual screen\" Your laptop interacts with a VNC server to get the GUI locally Pros: Everything works You can run anything like you are local on the server, since you are in practice just using a virtual screen Cons: Without enough bandwidth it's slow and not snappy enough X11 approach Same as VNC, but instead of sending bitmaps through VNC, a \"compressed\" version of the GUI is sent to the local computer directly Pros: Maybe faster than VNC PyCharm window is like a native window on your laptop Cons: X11 is old crap developed long time again and not really supported any more One needs to tunnel X11 traffic, set things up, and so on PyCharm Gateway New client-server architecture for PyCharm A \"headless\" PyCharm runs on the server A GUI client PyCharm runs on your laptop Pros It's as fast as possible, probably as fast as running locally Cons Need a PyCharm pro license (not a problem, we have money) It's not super polished: kind of beta, but it will get better and better PyCharm Remote Set-up This is described below in PyCharm - Advanced tip and tricks Edit locally and then PyCharm moves the files back and forth Pros Only requires ssh Cons You can't run / debug remotely","title":"Running PyCharm remotely"},{"location":"Tools-PyCharm/#current-situation","text":"Approach 1) seems to require lots of memory and CPU and it's not really fast. Approach 2) works but it's a pain to set-up and slow. We want to try with 3) TODO(gp): @Juraj pls a short tutorial on how to install TODO(gp): @Juraj understand if it works, if it's fast, and if it requires less memory","title":"Current situation"},{"location":"Tools-PyCharm/#how-to-run-our-cmamp-container-directly-from-pycharm","text":"PyCharm allows to run commands directly inside a container See https://www.jetbrains.com/help/pycharm/using-docker-as-a-remote-interpreter.html In fact when we do i docker_bash we launch a container and run bash inside it, but PyCharm can do the same thing TODO(gp): @Juraj Let's both try this. There are some notes below about it","title":"How to run our cmamp container directly from PyCharm"},{"location":"Tools-PyCharm/#how-to-review-a-pr-inside-pycharm","text":"CTRL + SHIFT + A -> View Pull Request {width=\"3.612198162729659in\" height=\"4.932292213473316in\"}","title":"How to review a PR inside Pycharm"},{"location":"Tools-PyCharm/#how-to-edit-remote-code","text":"You need to use a certain local directory (e.g., /Users/saggese/src/commodity_research1) and a remote directory (e.g., /wd/saggese/src/commodity_research1) They need to be synced at the same git branch (e.g., master or AmpTask1112_Audit_amp_Docker_system_03) Set-up Deployment {width=\"4.810558836395451in\" height=\"3.51540791776028in\"} {width=\"3.9114588801399823in\" height=\"1.6807655293088364in\"} The deployment options are {width=\"3.123700787401575in\" height=\"2.78830271216098in\"} You can see what file is changed in the file transfer window: {width=\"6.5in\" height=\"1.2083333333333333in\"} pycharm Develop on one node, sync, run on the server Run local application with venv Database Run application inside Docker Run application remotely inside Docker","title":"How to edit remote code"},{"location":"Tools-PyCharm/#general-ssh-config","text":"File | Settings | Tools | SSH Configurations {width=\"3.802863079615048in\" height=\"1.4626399825021872in\"} Once setup, ssh config can be used for all tools in PyCharm. Remote Interpreter DataGrip Deployment Etc.","title":"General ssh config"},{"location":"Tools-PyCharm/#db-connection-via-ssh","text":"Note: PyCharm Professional DataGrip is used as an example. There are numerous open source alternatives such as Beaver . Config below should apply to them also. To add a new data source in DataGrip, go to the database section in the lower left corner. {width=\"2.7734372265966756in\" height=\"2.7734372265966756in\"} Then pick your desired data source from the dropdown in the upper right corner. {width=\"4.164062773403325in\" height=\"2.028646106736658in\"} You will be presented with a dummy config that needs to be replaced with proper data as shown below. {width=\"4.158922790901137in\" height=\"3.0320844269466316in\"} Before that is done, be sure that proper ssh info is added in SSH/SSL section. {width=\"2.776042213473316in\" height=\"0.598069772528434in\"}","title":"DB connection via ssh"},{"location":"Tools-PyCharm/#deployment-with-remote-repository-through-sync","text":"Note: Before setting up deployment, pull the cmamp repo on EC2 instance and use the same name as on your local machine (example: cmamp1). Always try to keep both repos in sync via git. For more subtle and simpler changes use File | Reload All From Disk . This will upload changes to the remote repo. Tools | Deployment | Configuration {width=\"4.893396762904637in\" height=\"2.0781255468066493in\"} {width=\"4.838542213473316in\" height=\"1.1398490813648294in\"} Tools | Deployment | Options {width=\"4.671411854768154in\" height=\"3.2864588801399823in\"} Uncheck \"Skip external changes\" and check \"Delete remote files\" Tools | Deployment | Automatic Upload Check it Tools | Deployment | Browse Remote Host {width=\"5.394192913385827in\" height=\"1.765625546806649in\"}","title":"Deployment with remote repository (through sync)"},{"location":"Tools-PyCharm/#pudb-remote-debugging-todo","text":"","title":"PUDB - remote debugging - ToDo"},{"location":"Tools-PyCharm/#how-to-run-tests-inside-a-container","text":"https://www.jetbrains.com/help/pycharm/using-docker-compose-as-a-remote-interpreter.html#docker-compose-remote Note that the \"start SSH session...\" action is available only in PyCharm Professional Edition, while the terminal itself is available in both Professional and Community editions.","title":"How to run tests inside a container"},{"location":"Tools-PyCharm/#installing-pycharm-professional","text":"","title":"Installing PyCharm Professional"},{"location":"Tools-PyCharm/#windows","text":"Download the installer using this link Run the installer and follow the wizard steps. To run PyCharm, find it in the Windows Start menu or use the desktop shortcut.","title":"Windows"},{"location":"Tools-PyCharm/#macos","text":"There are separate disk images for Intel and Apple Silicon processors. Download the image, based on your processor using this link Mount the image and drag the PyCharm app to the Applications folder. Run the PyCharm app from the Applications directory, Launchpad, or Spotlight.","title":"macOS"},{"location":"Tools-PyCharm/#linux","text":"**Using tar archives ** Download the tar archive using this link Unpack the pycharm-*.tar.gz file to a different folder, if your current Download folder doesn't support file execution: ``` tar xzf pycharm-*.tar.gz -C `` ``` The recommended installation location according to the filesystem hierarchy standard (FHS) is /opt . To install PyCharm into this directory, enter the following command: > sudo tar xzf pycharm-\\*.tar.gz -C /opt/ Switch to the bin subdirectory: ``` cd /pycharm-*/bin # E.g., cd /opt/pycharm-*/bin ``` Run pycharm.sh from the bin subdirectory ``` sh pycharm.sh ``` Using snap packages For Ubuntu 16.04 and later, you can use snap packages to install PyCharm. ``` sudo snap install pycharm-professional --classic # or sudo snap install pycharm-community --classic ``` Run in the Terminalu ``` pycharm-professional # or pycharm-community # or pycharm-educational ```","title":"Linux"},{"location":"Tools-PyCharm/#connecting-via-pycharm-gateway-ssh","text":"The first thing you need to do is sign up for a free trial license or use it if it already have Then make sure you have a VPN connection to our VPC Click on Connect via SSH Into Username: write < > Example: richard Into Host: write < > Example: 172.30.2.136 Mark the Specify private key check box and locate the private key from the zip which was sent in the onboarding process. Example: crypto.pub Leave Port: 22 as it is. Click on Check Connection and Continue. Select IDE version: PyCharm Py 213.6777.x Locate your directory. Example: /data/richard Click on Download and Start IDE.","title":"Connecting via PyCharm gateway (SSH)"},{"location":"Tools-PyCharm/#connecting-via-vnc","text":"Make sure you have a VPN connection. Installing VNC Install VNC using this link: https://www.realvnc.com/en/connect/download/viewer/windows/ Sysadmin has sent you: os_password.txt your username $USER a key crypto.pub that looks like: -----BEGIN OPENSSH PRIVATE KEY----- b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAABlwAAAAdzc2gtcn NhAAAAAwEAAQAAAYEA0IQsLy1lL3bhPT+43sht2/m9tqZm8sEQrXMAVtfm4ji/LXMr7094 ... hakqVTlQ2sr0YTAAAAHnNhZ2dlc2VAZ3BtYWMuZmlvcy1yb3V0ZXIuaG9tZQECAwQ= -----END OPENSSH PRIVATE KEY----- Let's say you are connected via VNC. Login into the OS. Run pycharm.sh using terminal (should be there) ``` bash /opt/pycharm-community-2021.2.3/bin/pycharm.sh ```","title":"Connecting via VNC"},{"location":"Tools-PyCharm/#configuration","text":"","title":"Configuration"},{"location":"Tools-PyCharm/#reflow","text":"Set the reflow to reindent {width=\"6.5in\" height=\"4.486111111111111in\"}","title":"Reflow"},{"location":"Tools-PyCharm/#some-recommended-plug-ins","text":"vim Grazie Wrap-to-column GitHub Copilot","title":"Some recommended plug-ins"},{"location":"Type_hints/","text":"Type hints Why we use type hints What to annotate with type hints Conventions Empty return Invoke tasks Annotation for kwargs Any np.array and np.ndarray Handling the annoying Incompatible types in assignment Handling the annoying \"None\" has no attribute Disabling mypy errors What to do when you don't know what to do Library without types Inferring types using unit tests Why we use type hints We use Python 3 type hints to: Improve documentation Allow mypy to perform static checking of the code, looking for bugs Enforce the type checks at run-time, through automatic assertions (not implemented yet) What to annotate with type hints We expect all new library code (i.e., that is not in a notebook) to have type annotations We annotate the function signature We don't annotate the variables inside a function unless mypy reports that it can't infer the type We strive to get no errors / warnings from the linter, including mypy Conventions Empty return Return -> None if your function doesn't return Pros: mypy checks functions only when there is at least an annotation: so using -> None enables mypy to do type checking It reminds us that we need to use type hints Cons: None is the default value and so it might seem redundant Invoke tasks For some reason invoke does not like type hints, so we Omit type hints for invoke tasks, i.e. functions with the @task decorator Put # type: ignore so that mypy does not complain Example: python @task def run_qa_tests( # type: ignore ctx, stage=\"dev\", version=\"\", ): Annotation for kwargs We use kwargs: Any and not kwargs: Dict[str, Any] * always binds to a Tuple , and ** always binds to a Dict[str, Any] . Because of this restriction, type hints only need you to define the types of the contained arguments. The type checker automatically adds the Tuple[_, ...] and Dict[str, _] container types. Reference article Any Any type hint = no type hint We try to avoid it everywhere when possible np.array and np.ndarray If you get something like the following lint: bash dataflow/core/nodes/sklearn_models.py:537:[amp_mypy] error: Function \"numpy.core.multiarray.array\" is not valid as a type [valid-type] Then the problem is probably that a parameter that the lint is related to has been typed as np.array while it should be typed as np.ndarray : python `x_vals: np.array` -> `x_vals: np.ndarray` Handling the annoying Incompatible types in assignment mypy assigns a single type to each variable for its entire scope The problem is in common idioms where we use the same variable to store different representations of the same data python output : str = ... output = output.split(\"\\n\") ... # Process output. ... output = \"\\n\".join(output) Unfortunately the proper solution is to use different variables python output : str = ... output_as_array = output.split(\"\\n\") ... # Process output. ... output = \"\\n\".join(output_as_array) Another case could be: python from typing import Optional def test_func(arg: bool): ... var: Optional[bool] = ... dbg.dassert_is_not(var, None) test_func(arg=var) Sometimes mypy doesn't pick up the None check, and warns that the function expects a bool rather than an Optional[bool] . In that case, the solution is to explicitly use typing.cast on the argument when passing it in, note that typing.cast has no runtime effects and is purely for type checking. Here're the relevant docs So the solution would be: python from typing import cast ... ... test_func(arg=cast(bool, var)) Handling the annoying \"None\" has no attribute In some model classes self._model parameter is being assigned to None in ctor and being set after calling set_fit_state method The problem is that statically it's not possible to understand that someone will call set_fit_state before using self._model , so when a model's method is applied: python self._model = self._model.fit(...) the following lint appears: bash dataflow/core/nodes/sklearn_models.py:155:[amp_mypy] error: \"None\" has no attribute \"fit\" A solution is to Type hint when assigning the model parameter in ctor: python self._model: Optional[sklearn.base.BaseEstimator] = None Cast a type to the model parameter after asserting that it is not None : python hdbg.dassert_is_not(self._model, None) self._model = cast(sklearn.base.BaseEstimator, self._model) Disabling mypy errors If mypy reports an error and you don't understand why, please ping one of the python experts asking for help If you are sure that you understand why mypy reports and error and that you can override it, you disable this error When you want to disable an error reported by mypy : Add a comment reporting the mypy error Explain why this is not a problem Add # type: ignore with two spaces as usual for the inline comment Example python # mypy: Cannot find module named 'pyannotate_runtime' # pyannotate is not always installed from pyannotate_runtime import collect_types # type: ignore What to do when you don't know what to do Go to the mypy official cheat sheet Use reveal_type To find out what type mypy infers for an expression anywhere in your program, wrap it in reveal_type() mypy will print an error message with the type; remove it again before running the code See the official mypy documentation Library without types mypy is unhappy when a library doesn't have types Lots of libraries are starting to add type hints now that python 2 has been deprecated bash *.py:14: error: No library stub file for module 'sklearn.model_selection' [mypy] You can go in mypy.ini and add the library (following the alphabetical order) to the list Note that you need to ensure that different copies of mypy.ini in different sub projects are equal ```bash vimdiff mypy.ini amp/mypy.ini or cp mypy.ini amp/mypy.ini ``` Inferring types using unit tests Sometimes it is possible to infer types directly from unit tests. We have used this flow to annotate the code when we switched to Python3 and it worked fine although there were various mistakes. We still prefer to annotate by hand based on what the code is intended to do, rather than automatically infer it from how the code behaves. Install pyannotate bash > pip install pyannotate To enable collecting type hints run bash > export PYANNOTATE=True Run pytest , e.g., on a subset of unit tests: Run pytest , e.g., on a subset of unit tests like helpers : bash > pytest helpers A file type_info.json is generated Annotate the code with the inferred types: bash > pyannotate -w --type-info type_info.json . --py3","title":"Type hints"},{"location":"Type_hints/#type-hints","text":"Why we use type hints What to annotate with type hints Conventions Empty return Invoke tasks Annotation for kwargs Any np.array and np.ndarray Handling the annoying Incompatible types in assignment Handling the annoying \"None\" has no attribute Disabling mypy errors What to do when you don't know what to do Library without types Inferring types using unit tests","title":"Type hints"},{"location":"Type_hints/#why-we-use-type-hints","text":"We use Python 3 type hints to: Improve documentation Allow mypy to perform static checking of the code, looking for bugs Enforce the type checks at run-time, through automatic assertions (not implemented yet)","title":"Why we use type hints"},{"location":"Type_hints/#what-to-annotate-with-type-hints","text":"We expect all new library code (i.e., that is not in a notebook) to have type annotations We annotate the function signature We don't annotate the variables inside a function unless mypy reports that it can't infer the type We strive to get no errors / warnings from the linter, including mypy","title":"What to annotate with type hints"},{"location":"Type_hints/#conventions","text":"","title":"Conventions"},{"location":"Type_hints/#empty-return","text":"Return -> None if your function doesn't return Pros: mypy checks functions only when there is at least an annotation: so using -> None enables mypy to do type checking It reminds us that we need to use type hints Cons: None is the default value and so it might seem redundant","title":"Empty return"},{"location":"Type_hints/#invoke-tasks","text":"For some reason invoke does not like type hints, so we Omit type hints for invoke tasks, i.e. functions with the @task decorator Put # type: ignore so that mypy does not complain Example: python @task def run_qa_tests( # type: ignore ctx, stage=\"dev\", version=\"\", ):","title":"Invoke tasks"},{"location":"Type_hints/#annotation-for-kwargs","text":"We use kwargs: Any and not kwargs: Dict[str, Any] * always binds to a Tuple , and ** always binds to a Dict[str, Any] . Because of this restriction, type hints only need you to define the types of the contained arguments. The type checker automatically adds the Tuple[_, ...] and Dict[str, _] container types. Reference article","title":"Annotation for kwargs"},{"location":"Type_hints/#any","text":"Any type hint = no type hint We try to avoid it everywhere when possible","title":"Any"},{"location":"Type_hints/#nparray-and-npndarray","text":"If you get something like the following lint: bash dataflow/core/nodes/sklearn_models.py:537:[amp_mypy] error: Function \"numpy.core.multiarray.array\" is not valid as a type [valid-type] Then the problem is probably that a parameter that the lint is related to has been typed as np.array while it should be typed as np.ndarray : python `x_vals: np.array` -> `x_vals: np.ndarray`","title":"np.array and np.ndarray"},{"location":"Type_hints/#handling-the-annoying-incompatible-types-in-assignment","text":"mypy assigns a single type to each variable for its entire scope The problem is in common idioms where we use the same variable to store different representations of the same data python output : str = ... output = output.split(\"\\n\") ... # Process output. ... output = \"\\n\".join(output) Unfortunately the proper solution is to use different variables python output : str = ... output_as_array = output.split(\"\\n\") ... # Process output. ... output = \"\\n\".join(output_as_array) Another case could be: python from typing import Optional def test_func(arg: bool): ... var: Optional[bool] = ... dbg.dassert_is_not(var, None) test_func(arg=var) Sometimes mypy doesn't pick up the None check, and warns that the function expects a bool rather than an Optional[bool] . In that case, the solution is to explicitly use typing.cast on the argument when passing it in, note that typing.cast has no runtime effects and is purely for type checking. Here're the relevant docs So the solution would be: python from typing import cast ... ... test_func(arg=cast(bool, var))","title":"Handling the annoying Incompatible types in assignment"},{"location":"Type_hints/#handling-the-annoying-none-has-no-attribute","text":"In some model classes self._model parameter is being assigned to None in ctor and being set after calling set_fit_state method The problem is that statically it's not possible to understand that someone will call set_fit_state before using self._model , so when a model's method is applied: python self._model = self._model.fit(...) the following lint appears: bash dataflow/core/nodes/sklearn_models.py:155:[amp_mypy] error: \"None\" has no attribute \"fit\" A solution is to Type hint when assigning the model parameter in ctor: python self._model: Optional[sklearn.base.BaseEstimator] = None Cast a type to the model parameter after asserting that it is not None : python hdbg.dassert_is_not(self._model, None) self._model = cast(sklearn.base.BaseEstimator, self._model)","title":"Handling the annoying \"None\" has no attribute"},{"location":"Type_hints/#disabling-mypy-errors","text":"If mypy reports an error and you don't understand why, please ping one of the python experts asking for help If you are sure that you understand why mypy reports and error and that you can override it, you disable this error When you want to disable an error reported by mypy : Add a comment reporting the mypy error Explain why this is not a problem Add # type: ignore with two spaces as usual for the inline comment Example python # mypy: Cannot find module named 'pyannotate_runtime' # pyannotate is not always installed from pyannotate_runtime import collect_types # type: ignore","title":"Disabling mypy errors"},{"location":"Type_hints/#what-to-do-when-you-dont-know-what-to-do","text":"Go to the mypy official cheat sheet Use reveal_type To find out what type mypy infers for an expression anywhere in your program, wrap it in reveal_type() mypy will print an error message with the type; remove it again before running the code See the official mypy documentation","title":"What to do when you don't know what to do"},{"location":"Type_hints/#library-without-types","text":"mypy is unhappy when a library doesn't have types Lots of libraries are starting to add type hints now that python 2 has been deprecated bash *.py:14: error: No library stub file for module 'sklearn.model_selection' [mypy] You can go in mypy.ini and add the library (following the alphabetical order) to the list Note that you need to ensure that different copies of mypy.ini in different sub projects are equal ```bash vimdiff mypy.ini amp/mypy.ini or cp mypy.ini amp/mypy.ini ```","title":"Library without types"},{"location":"Type_hints/#inferring-types-using-unit-tests","text":"Sometimes it is possible to infer types directly from unit tests. We have used this flow to annotate the code when we switched to Python3 and it worked fine although there were various mistakes. We still prefer to annotate by hand based on what the code is intended to do, rather than automatically infer it from how the code behaves. Install pyannotate bash > pip install pyannotate To enable collecting type hints run bash > export PYANNOTATE=True Run pytest , e.g., on a subset of unit tests: Run pytest , e.g., on a subset of unit tests like helpers : bash > pytest helpers A file type_info.json is generated Annotate the code with the inferred types: bash > pyannotate -w --type-info type_info.json . --py3","title":"Inferring types using unit tests"},{"location":"Unit_tests/","text":"Unit tests Running unit tests Using invoke Docker image stage and version Specifying pytest options Save test output to file Show the tests but do not run Skip submodules Compute tests coverage Timeout Rerunning timeout-ed tests Compute tests coverage An example coverage session An example with customized pytest-cov html run Generate coverage report with invoke Common usage Publishing HTML report on S3 Running pytest directly Usage and Invocations reference Custom pytest options behaviors Enable logging Update golden outcomes Incremental test mode Debugging Notebooks Running tests on GH Actions How to run a single test on GH Action Guidelines about writing unit tests What is a unit test? Why is unit testing important? Unit testing tips Tip #1: Test one thing Tip #2: Keep tests self-contained Tip #3: Only specify data related to what is being tested Tip #4: Test realistic corner cases Tip #5: Test a typical scenario Tip #6: Test executable scripts end-to-end Conventions Naming and placement conventions Our framework to test using input / output data Use text and not pickle files as input check_string vs self.assertEqual Use self.assert_equal How to split unit test code in files Skeleton for unit test Hierarchical TestCase approach Use the appropriate self.assert* Do not use hdbg.dassert Interesting testing functions Use setUp/tearDown Update test tags Mocking Refs Common usage samples Philosophy about mocking Some general suggestions about testing Test from the outside-in We don't need to test all the assertions Use strings to compare output instead of data structures Use self.check_string() for things that we care about not changing (or are too big to have as strings in the code) Each test method should test a single test case Each test should be crystal clear on how it is different from the others In general you want to budget the time to write unit tests Write skeleton of unit tests and ask for a review if you are not sure how what to test Object patch with return value Path patch with multiple return values Ways of calling patch and patch.object Mock object state after test run Mock common external calls in hunitest.TestCase class Mocks with specs Caveats Running unit tests Before any PR (and ideally after every commit) we want to run all the unit tests to make sure we didn't introduce no new bugs We use pytest and unittest as testing framework We have different test set lists: fast Tests that are quick to execute (typically < 5 secs per test class) We want to run these tests before / after every commit / PR to make sure things are not horrible broken slow Tests that we don't want to run all the times because they are: Slow (typically < 20 seconds per test) Related to pieces of code that don't change often External APIs we don't want to hit continuously superslow Tests that run long workload, e.g., running a production model Using invoke invoke is a task execution which allows to execute some typical workflows, e.g. run the tests ```bash # Run only fast tests: i run_fast_tests # Run only slow tests: i run_slow_tests # Run only superslow tests: i run_superslow_tests To see the options use --help option, e.g. i --help run_fast_tests : Usage: inv[oke] [--core-opts] run_fast_tests [--options] [other tasks here ...] Docstring: Run fast tests. :param stage: select a specific stage for the Docker image :param pytest_opts: option for pytest :param skip_submodules: ignore all the dir inside a submodule :param coverage: enable coverage computation :param collect_only: do not run tests but show what will be executed :param tee_to_file: save output of pytest in tmp.pytest.log :param kwargs: kwargs for ctx.run Options: -c, --coverage -k, --skip-submodules -o, --collect-only -p STRING, --pytest-opts=STRING -s STRING, --stage=STRING -t, --tee-to-file -v STRING, --version=STRING ``` Docker image stage and version To select a specific stage for Docker image use the --stage option. E.g., this might be useful when a user wants to run regressions on the local Docker image to verify that nothing is broken before promoting it to dev image. ```bash i run_fast_tests --stage local ``` To run the tests on the specific version of a Docker image, use the --version option. E.g., this might be useful when releasing a new version of an image. ```bash i run_fast_tests --stage local --version 1.0.4 ``` Specifying pytest options With the option --pytest-opts it is possible to pass any pytest option to invoke . E.g., if a user want to run the tests in the debug mode to show the output ```bash i run_fast_tests -s --dbg ``` Save test output to file To save the output of pytest to tmp.pytest.log use the --tee-to-file option. ```bash i run_fast_tests --tee-to-file ``` Show the tests but do not run To show (but not run) the tests that will be executed, use the --collect-only . ```bash i run_fast_test --collect-only ``` Skip submodules To skip running tests in submodules use the --skip-submodules option. This is useful for repos with submodules, e.g., dev_tools where cmamp is a submodule. Using this option, only tests in dev_tools but not in cmamp are run ```bash cd dev_tools1 i run_fast_tests --skip-submodules ``` Compute tests coverage To compute tests coverage use the --coverage option ```bash i run_fast_tests --coverage ``` Timeout We use the pytest-timeout package to limit durations of fast, slow, and superslow tests The timeout durations for each test type are listed here The timeout restricts only running time of the test method, not setUp and tearDown time Rerunning timeout-ed tests Running tests can take different amount of time depending on workload and machine Because of this, we rerun failing tests using pytest-rerunfailures pytest-rerunfailures is not completely compatible with pytest-timeout . This is why we have to add the -o timeout_func_only=true flag to pytest-timeout . See https://github.com/pytest-dev/pytest-rerunfailures/issues/99 for more information We rerun timeouted fast tests twice and timeouted slow and superslow tests once There is a way to provide a rerun delay for individual tests. However, we can\u2019t use it for now due to #693 (comment) Compute tests coverage The documentation for coverage is here . Run a set of unit tests enabling coverage: ```bash # Run the coverage for a single test: i run_fast_tests --coverage -p oms/test/test_broker.py::TestSimulatedBroker1 # Run coverage for an entire module like oms : i run_fast_tests --coverage -p oms ``` This generates and run a pytest command inside Docker like: bash docker> /venv/bin/pytest -m \"not slow and not superslow\" oms/test/test_broker.py::TestSimulatedBroker1 --cov=. --cov-branch --cov-report term-missing --cov-report html Which generates: A default coverage report A binary .coverage file that contains the coverage information An htmlcov dir with a browsable code output to inspect the coverage for the files One can post-process the coverage report in different ways using the command coverage inside a docker container, since the code was run (as always) inside the Docker container that contains all the dependencies. ```bash coverage -h Coverage.py, version 5.5 with C extension Measure, collect, and report on code coverage in Python programs. usage: coverage [options] [args] Commands: annotate Annotate source files with execution information. combine Combine a number of data files. debug Display information about the internals of coverage.py erase Erase previously collected coverage data. help Get help on using coverage.py. html Create an HTML report. json Create a JSON report of coverage results. report Report coverage stats on modules. run Run a Python program and measure code execution. xml Create an XML report of coverage results. Use \"coverage help \" for detailed help on any command. Full documentation is at https://coverage.readthedocs.io ``` ```bash coverage report -h Usage: coverage report [options] [modules] Report coverage statistics on modules. Options: --contexts=REGEX1,REGEX2,... Only display data from lines covered in the given contexts. Accepts Python regexes, which must be quoted. --fail-under=MIN Exit with a status of 2 if the total coverage is less than MIN. -i, --ignore-errors Ignore errors while reading source files. --include=PAT1,PAT2,... Include only files whose paths match one of these patterns. Accepts shell-style wildcards, which must be quoted. --omit=PAT1,PAT2,... Omit files whose paths match one of these patterns. Accepts shell-style wildcards, which must be quoted. --precision=N Number of digits after the decimal point to display for reported coverage percentages. --sort=COLUMN Sort the report by the named column: name, stmts, miss, branch, brpart, or cover. Default is name. -m, --show-missing Show line numbers of statements in each module that weren't executed. --skip-covered Skip files with 100% coverage. --no-skip-covered Disable --skip-covered. --skip-empty Skip files with no code. --debug=OPTS Debug options, separated by commas. [env: COVERAGE_DEBUG] -h, --help Get help on this command. --rcfile=RCFILE Specify configuration file. By default '.coveragerc', 'setup.cfg', 'tox.ini', and 'pyproject.toml' are tried. [env: COVERAGE_RCFILE] ``` ```bash i docker_bash # Report the coverage for all the files under oms using the workload above (i.e., the fast tests under oms/test/test_broker.py::TestSimulatedBroker1 ) docker> coverage report --include=\"oms/*\" Name Stmts Miss Branch BrPart Cover oms/ init .py 0 0 0 0 100% oms/api.py 154 47 36 2 70% oms/broker.py 200 31 50 9 81% oms/broker_example.py 23 0 4 1 96% oms/call_optimizer.py 31 0 0 0 100% oms/devops/ init .py 0 0 0 0 100% oms/devops/docker_scripts/ init .py 0 0 0 0 100% oms/locates.py 7 7 2 0 0% oms/mr_market.py 55 1 10 1 97% oms/oms_db.py 47 0 10 3 95% oms/oms_lib_tasks.py 64 39 2 0 38% oms/oms_utils.py 34 34 6 0 0% oms/order.py 101 30 22 0 64% oms/order_example.py 26 0 0 0 100% oms/place_orders.py 121 8 18 6 90% oms/pnl_simulator.py 326 42 68 8 83% oms/portfolio.py 309 21 22 0 92% oms/portfolio_example.py 32 0 0 0 100% oms/tasks.py 3 3 0 0 0% oms/test/oms_db_helper.py 29 11 2 0 65% oms/test/test_api.py 132 25 12 0 83% oms/test/test_broker.py 33 5 4 0 86% oms/test/test_mocked_portfolio.py 0 0 0 0 100% oms/test/test_mr_market.py 46 0 2 0 100% oms/test/test_oms_db.py 114 75 14 0 38% oms/test/test_order.py 24 0 4 0 100% oms/test/test_place_orders.py 77 0 4 0 100% oms/test/test_pnl_simulator.py 235 6 16 0 98% oms/test/test_portfolio.py 135 0 6 0 100% TOTAL 2358 385 314 30 82% ``` To exclude the test files, which could inflate the coverage ```bash coverage report --include=\"oms/ \" --omit=\" /test_*.py\" Name Stmts Miss Branch BrPart Cover oms/ init .py 0 0 0 0 100% oms/api.py 154 47 36 2 70% oms/broker.py 200 31 50 9 81% oms/broker_example.py 23 0 4 1 96% oms/call_optimizer.py 31 0 0 0 100% oms/devops/ init .py 0 0 0 0 100% oms/devops/docker_scripts/ init .py 0 0 0 0 100% oms/locates.py 7 7 2 0 0% oms/mr_market.py 55 1 10 1 97% oms/oms_db.py 47 0 10 3 95% oms/oms_lib_tasks.py 64 39 2 0 38% oms/oms_utils.py 34 34 6 0 0% oms/order.py 101 30 22 0 64% oms/order_example.py 26 0 0 0 100% oms/place_orders.py 121 8 18 6 90% oms/pnl_simulator.py 326 42 68 8 83% oms/portfolio.py 309 21 22 0 92% oms/portfolio_example.py 32 0 0 0 100% oms/tasks.py 3 3 0 0 0% oms/test/oms_db_helper.py 29 11 2 0 65% TOTAL 1562 274 252 30 80% ``` To open the line coverage, from outside Docker go with your browser to htmlcov/index.html . The htmlcov is re-written with every coverage run with the --cov-report html option. If you move out index.html from htmlcov dir some html features (e.g., filtering) will not work. ```bash # On macOS: open htmlcov/index.html ``` By clicking on a file you can see which lines are not covered An example coverage session We want to measure the unit test coverage of oms component from both fast and slow tests We start by running the fast tests: ```bash # Run fast unit tests i run_fast_tests --coverage -p oms collected 66 items / 7 deselected / 59 selected ... # Compute the coverage for the module sorting by coverage docker> coverage report --include=\"oms/ \" --omit=\" /test_*.py\" --sort=Cover Name Stmts Miss Branch BrPart Cover oms/locates.py 7 7 2 0 0% oms/oms_utils.py 34 34 6 0 0% oms/tasks.py 3 3 0 0 0% oms/oms_lib_tasks.py 64 39 2 0 38% oms/order.py 101 30 22 0 64% oms/test/oms_db_helper.py 29 11 2 0 65% oms/api.py 154 47 36 2 70% oms/broker.py 200 31 50 9 81% oms/pnl_simulator.py 326 42 68 8 83% oms/place_orders.py 121 8 18 6 90% oms/portfolio.py 309 21 22 0 92% oms/oms_db.py 47 0 10 3 95% oms/broker_example.py 23 0 4 1 96% oms/mr_market.py 55 1 10 1 97% oms/ init .py 0 0 0 0 100% oms/call_optimizer.py 31 0 0 0 100% oms/devops/ init .py 0 0 0 0 100% oms/devops/docker_scripts/ init .py 0 0 0 0 100% oms/order_example.py 26 0 0 0 100% oms/portfolio_example.py 32 0 0 0 100% TOTAL 1562 274 252 30 80% ``` We see that certain files have a low coverage, so we want to see what is not covered. Generate the same report in a browsable format ``bash docker> rm -rf htmlcov; coverage html --include=\"oms/*\" --omit=\"*/test_*.py\" # Wrote HTML report to htmlcov/index.html` open htmlcov/index.html ``` The low coverage for tasks.py and oms_lib_tasks.py is due to the fact that we are running code through invoke that doesn't allow coverage to track it. Now we run the coverage for the slow tests ```bash # Save the coverage from the fast tests run cp .coverage .coverage_fast_tests i run_slow_tests --coverage -p oms collected 66 items / 59 deselected / 7 selected cp .coverage .coverage_slow_tests coverage report --include=\"oms/ \" --omit=\" /test_*.py\" --sort=Cover Name Stmts Miss Branch BrPart Cover oms/locates.py 7 7 2 0 0% oms/oms_utils.py 34 34 6 0 0% oms/tasks.py 3 3 0 0 0% oms/pnl_simulator.py 326 280 68 1 13% oms/place_orders.py 121 100 18 0 15% oms/mr_market.py 55 44 10 0 17% oms/portfolio.py 309 256 22 0 18% oms/call_optimizer.py 31 25 0 0 19% oms/broker.py 200 159 50 0 20% oms/order.py 101 78 22 0 20% oms/order_example.py 26 19 0 0 27% oms/broker_example.py 23 14 4 0 33% oms/portfolio_example.py 32 21 0 0 34% oms/api.py 154 107 36 0 36% oms/oms_lib_tasks.py 64 39 2 0 38% oms/oms_db.py 47 5 10 2 84% oms/ init .py 0 0 0 0 100% oms/devops/ init .py 0 0 0 0 100% oms/devops/docker_scripts/ init .py 0 0 0 0 100% oms/test/oms_db_helper.py 29 0 2 0 100% TOTAL 1562 1191 252 3 23% ``` We see that the coverage from the slow tests is only 23% for 7 tests bash root@6faaa979072e:/app/amp# coverage combine .coverage_fast_tests .coverage_slow_tests Combined data file .coverage_fast_tests Combined data file .coverage_slow_tests An example with customized pytest-cov html run We want to measure unit test coverage specifically for one test in im_v2/common/data/transform/ and to save generated htmlcov in the same directory. Run command below after i docker_bash : bash pytest --cov-report term-missing --cov=im_v2/common/data/transform/ im_v2/common/data/transform/test/test_transform_utils.py --cov-report html:im_v2/common/data/transform/htmlcov \\ Output sample: bash ---------- coverage: platform linux, python 3.8.10-final-0 ----------- Name Stmts Miss Cover Missing ----------------------------------------------------------------------------------------------- im_v2/common/data/transform/convert_csv_to_pq.py 55 55 0% 2-159 im_v2/common/data/transform/extract_data_from_db.py 55 55 0% 2-125 im_v2/common/data/transform/pq_convert.py 126 126 0% 3-248 im_v2/common/data/transform/transform_pq_by_date_to_by_asset.py 131 131 0% 2-437 im_v2/common/data/transform/transform_utils.py 22 0 100% ----------------------------------------------------------------------------------------------- TOTAL 389 367 6% Coverage HTML written to dir im_v2/common/data/transform/htmlcov \\ Generate coverage report with invoke One can compute test coverage for a specified directory and generate text and HTML reports automatically using invoke task run_coverage_report ```bash i --help run_coverage_report INFO: > cmd='/data/grisha/src/venv/amp.client_venv/bin/invoke --help run_coverage_report' Usage: inv[oke] [--core-opts] run_coverage_report [--options] [other tasks here ...] Docstring: Compute test coverage stats. The flow is: Run tests and compute coverage stats for each test type Combine coverage stats in a single file Generate a text report Generate a HTML report (optional) Post it on S3 (optional) :param target_dir: directory to compute coverage stats for here :param generate_html_report: whether to generate HTML coverage report or not :param publish_html_on_s3: whether to publish HTML coverage report or not :param aws_profile: the AWS profile to use for publishing HTML report Options: -a STRING, --aws-profile=STRING -g, --[no-]generate-html-report -p, --[no-]publish-html-on-s3 -t STRING, --target-dir=STRING ``` Common usage Compute coverage for market_data dir, generate text and HTML reports and publish HTML report on S3 ```bash i run_coverage_report --target-dir market_data ... Name Stmts Miss Branch BrPart Cover market_data/real_time_market_data.py 100 81 32 0 16% market_data/replayed_market_data.py 111 88 24 0 19% market_data/abstract_market_data.py 177 141 24 0 19% market_data/market_data_example.py 124 97 10 0 20% market_data/market_data_im_client.py 66 50 18 0 21% market_data/ init .py 5 0 0 0 100% TOTAL 583 457 108 0 19% Wrote HTML report to htmlcov/index.html 20:08:53 - INFO lib_tasks.py _publish_html_coverage_report_on_s3:3679 HTML coverage report is published on S3: path= s3://cryptokaizen-html/html_coverage/grisha_CmTask1038_Tool_to_extract_the_dependency_from_a_project ``` Publishing HTML report on S3 To make a dir with the report unique linux user and Git branch name are used, e.g., html_coverage/grisha_CmTask1038_Tool_to_extract_the_dependency_from_a_project html_coverage is the common dir on S3 for coverage reports After publishing the report, one can easily open it via a local web browser See the details in htmlcov server E.g. http://172.30.2.44/html_coverage/grisha_CmTask1038_Tool_to_extract_the_dependency_from_a_project/ Running pytest directly Usage and Invocations reference See pytest documentation Some examples of useful command lines: ```bash # Stop at first failure pytest -x # Run a single class pytest -k TestPcaFactorComputer1 # Run a single test method pytest core/test/test_core.py::TestPcaFactorComputer1::test_linearize_eigval_eigvec # Remove cache artifacts find . -name \" pycache \" -o -name \".pytest_cache\" ./.pytest_cache ./dev_scripts/test/Test_linter_py1.test_linter1/tmp.scratch/ pycache ./dev_scripts/test/ pycache ./dev_scripts/ pycache find . -name \" pycache \" -o -name \".pytest_cache\" | xargs rm -rf # Run with a clear cache pytest --cache-clear # Run the tests that last failed (this data is stored in .pytest_cache/v/cache/lastfailed) pytest --last-failed ``` Custom pytest options behaviors Enable logging To enable logging of _LOG.debug for a single test run: ```bash # Enable debug info pytest oms/test/test_broker.py::TestSimulatedBroker1 -s --dbg ``` Update golden outcomes This switch allows to overwrite the golden outcomes that are used as reference in the unit tests to detect failures ```bash pytest --update_outcomes ``` Incremental test mode This switch allows to reuse artifacts in the test directory and to skip the clean up phase It is used to re-run tests from the middle when they are very long and one wants to debug them ```bash pytest --incremental ``` Debugging Notebooks run a failing test with -s --dbg to get detailed logs e.g., > pytest core/plotting/test/test_gallery_notebook.py -s --dbg from the logs take a run_notebook.py script command that was run by the test e.g., starting like /app/dev_scripts/notebooks/run_notebook.py --notebook ... Append --no_suppress_output to this command and run it again directly from the bash e.g., like > /app/dev_scripts/notebooks/run_notebook.py --notebook ... --no_suppress_output scroll up the logs and see a report about the problem, notebooks failures will be displayed as well e.g., Running tests on GH Actions The official documentation is https://docs.github.com/en/actions How to run a single test on GH Action Unfortunately there is no way to log in and run interactively on GH machines. This is a feature requested but not implemented by GH yet. All the code to run GH Actions is in the .github directory in lemonade and amp . E.g., to run a single test in the fast test target, instead of the entire regression suite You can modify .github/workflows/fast_tests.yml , by replacing bash # run: invoke run_fast_tests run: invoke run_fast_tests --pytest-opts=\"helpers/test/test_git.py::Test_git_modified_files1::test_get_modified_files_in_branch1 -s --dbg\" Note that the indentation matters, since it's a YAML file The -s --dbg is to show _LOG.debug in case you care about that to get more information Commit the code to your branch (not in master please) since GH runs each branch independently Kick off manually the fast test through the GH interface After debugging, you can revert the change from your branch to master and move along with the usual PR flow Guidelines about writing unit tests What is a unit test? A unit test is a small, self-contained test of a public function or public method of a library The test specifies the given inputs, any necessary state, and the expected output Running the test ensures that the actual output agrees with the expected output Why is unit testing important? Unit testing is an integral part of Pragmatic programming approach Some of the tips that relate to it are: Design with Contracts Refactor Early, Refactor Often Test Your Software, or Your Users Will Coding Ain't Done Till All the Tests Run Test State Coverage, Not Code Coverage You Can't Write Perfect Software Crash Early Design to Test Test Early. Test Often. Test Automatically. Use Saboteurs to Test Your Testing Find Bugs Once Good unit testing improves software quality. It does this in part by Eliminating bugs (obvious) Clarifying code design and interfaces (\"Design to Test\") Making refactoring safer and easier (\"Refactor Early, Refactor Often\") Documenting expected behavior and usage Unit testing tips Tip #1: Test one thing A good unit test tests only one thing Testing one thing keeps the unit test simple, relatively easy to understand, and helps isolate the root cause when the test fails How do we test more than one thing? By having more than one unit test! Tip #2: Keep tests self-contained A unit test should be independent of all other unit tests Each test should be self-sufficient Additionally, one should never assume that unit tests will be executed in a particular order A corollary of keeping tests self-contained is to keep all information needed to understand the test within the test itself In other words, when possible, avoid calling helper functions to load data or state to initialize the test; instead, specify the data explicitly in the test where it is used This makes the test easier to understand and easier to debug when it fails If multiple unit tests use or can use the same initialization data, do not hesitate repeating it in each test (or consider using parameterized testing) Tip #3: Only specify data related to what is being tested If a function that is being tested supports optional arguments, but those optional arguments are not needed for a particular unit test, then do not specify them in the test Specify the minimum of what is required to test what is being tested. Tip #4: Test realistic corner cases Can your function receive a list that is empty? Can it return an empty Series? What happens if it receives a numerical value outside of an expected range? How should the function behave in those cases? Should it crash? Should it return a reasonable default value? Expect these questions to come up in practice and think through what the appropriate behavior should be. Then, test for it. Tip #5: Test a typical scenario In ensuring that corner cases are covered, do not overlook testing basic functionality for typical cases This is useful for verifying current behavior and to support refactoring. Tip #6: Test executable scripts end-to-end In some cases, like scripts, it is easy to get lost chasing the coverage % E.g. covering every line of the original, including the parser This is not always necessary If you are able to run a script with all arguments present, it means that the parser works correctly So an end-to-end smoke test will also cover the parser This saves a little time and reduces the bloat If you need to test the functionality, consider factoring out as much code as possible from _main A good practice is to have a _run function that does all the job and _main only brings together the parser and the executable part Conventions Naming and placement conventions We follow the convention (that happen to be mostly the default to pytest ): A directory test that contains all the test code and artifacts The test directory contains all the test_*.py files and all inputs and outputs for the tests. A unit test file should be close to the library / code it tests The test class should make clear reference to the code that is tested To test a class FooBar the corresponding test class is named TestFooBar, we use the CamelCase for the test classes You can add a number, e.g., TestFooBar1() , if there are multiple test classes that are testing the code in different ways (e.g., setUp() tearDown() are different) To test a function generate_html_tables() the corresponding test class is Test_generate_html_tables It's ok to have multiple test methods, e.g., for FooBar.method_a() and FooBar.method_b() , the test method is: python class TestFooBar1(unittest2.TestCase): def test_method_a(self): ... def test_method_b(self): ... Split test classes and methods in a reasonable way, so each one tests one single thing in the simplest possible way Remember that test code is not second class citizen, although it's auxiliary to the code Add comments and docstring explaining what the code is doing If you change the name of a class, also the test should be changed If you change the name of a file also the name of the file with the testing code should be changed Our framework to test using input / output data helpers/unit_test.py has some utilities to easily create input and output dirs storing data for unit tests hut.TestCase has various methods to help you create get_input_dir : return the name of the dir used to store the inputs get_scratch_space : return the name of a scratch dir to keep artifacts of the test get_output_dir : probably not interesting for the user The directory structure enforced by the out TestCase is like: ```bash tree -d edg/form_8/test/ edg/form_8/test/ \u2514\u2500\u2500 TestExtractTables1.test1 \u251c\u2500\u2500 input \u2514\u2500\u2500 output ``` The layout of test dir: ```bash ls -1 helpers/test/ Test_dassert1.test2 Test_dassert1.test3 Test_dassert1.test4 ... Test_dassert_misc1.test6 Test_dassert_misc1.test8 Test_system1.test7 test_dbg.py test_helpers.py test_system_interaction.py ``` Use text and not pickle files as input The problem with pickle files are the usual ones They are not stable across different version of libraries They are not human readable Prefer to use text file E.g., use a CSV file If the data used for testing is generated in a non-complicated way Document how it was generated Even better add a test that generates the data Use a subset of the input data The smaller the better for everybody Fast tests Easier to debug More targeted unit test Do not check in 1 megabyte of test data! check_string vs self.assertEqual TODO(gp): Add Use self.assert_equal This is a function that helps you understand what are the mismatches It works on str How to split unit test code in files The two extreme approaches are: All the test code for a directory goes in one file foo/bar/test/test_$DIRNAME.py (or foo/bar/test/test_all.py ) Each file foo/bar/$FILENAME with code gets its corresponding foo/bar/test/test_$FILENAME.py It should also be named according to the library it tests For example, if the library to test is called pnl.py , then the corresponding unit test should be called test_pnl.py Pros of 1) vs 2) Less maintenance churn It takes work to keep the code and the test files in sync, e.g., If you change the name of the code file you don't have to change other file names If you move one class from one file to another, you might not need to move test code Fewer files opened in your editor Avoid many files with a lot of boilerplate code Cons of 1) vs 2) The single file can become huge! Compromise solution: Start with a single file test_$DIRNAME.py (or test*dir_name.py ) * In the large file add a framed comment like: python # ################## # Unit tests for \u2026 # ################## So it's easy to find which file is tested were using grep Then split when it becomes too big using test_$FILENAME.py Skeleton for unit test Interesting unit tests are in helpers/test A unit test looks like: python import helpers.unit_test as hut class Test...(hut.TestCase): def test...(self): ... pytest will take care of running the code so you don't need: python if __name__ == '__main__': unittest.main() Hierarchical TestCase approach Whenever there is hierarchy in classes, we also create a hierarchy of test classes A parent test class looks like: python import helpers.unit_test as hut class SomeClientTestCase(hut.TestCase): def _test...1(self): ... def _test...2(self): ... While a child test class looks like this where test methods use the corresponding methods from the parent test class: python class TestSomeClient(SomeClientTestCase): def test...1(self): ... def test...2(self): ... Each TestCase tests a \"behavior\" like a set of related methods Each TestCase is under the test dir Each derived class should use the proper TestCase classes to reach a decent coverage It is OK to use non-private methods in test classes to ensure that the code is in order of dependency, so that the reader doesn't have to jump back / forth We want to separate chunks of unit test code using: python # ######################################################################## putting all the methods used by that chunk at the beginning and so on It is OK to skip a TestCase method if not meaningful, when coverage is enough As an example, see im_v2/common/data/client/test/im_client_test_case.py and im_v2/ccxt/data/client/test/test_ccxt_clients.py Use the appropriate self.assert* When you get a failure you don't want to get something like \"True is not False\", rather an informative message like \"5 is not < 4\" Bad \\ self.assertTrue(a &lt; b) Good \\ self.assertLess(a, b) Do not use hdbg.dassert dassert are for checking self-consistency of the code The invariant is that you can remove dbg.dassert without changing the behavior of the code. Of course you can't remove the assertion and get unit tests to work Interesting testing functions List of useful testing functions are: General python Numpy Pandas Use setUp/tearDown If you have a lot of repeated code in your tests, you can make them shorter by moving this code to setUp/tearDown methods: setUp() \\ Method called to prepare the test fixture. This is called immediately before calling the test method; other than AssertionError or SkipTest , any exception raised by this method will be considered an error rather than a test failure. The default implementation does nothing. tearDown() \\ Method called immediately after the test method has been called and the result recorded. This is called even if the test method raised an exception, so the implementation in subclasses may need to be particularly careful about checking internal state. Any exception, other than AssertionError or SkipTest , raised by this method will be considered an additional error rather than a test failure (thus increasing the total number of reported errors). This method will only be called if the setUp() succeeds, regardless of the outcome of the test method. The default implementation does nothing. If you need some expensive code parts to be done once for the whole test class, such as opening a database connection, opening a temporary file on the filesystem, loading a shared library for testing, etc., you can use setUpClass/tearDownClass methods: setUpClass() A class method called before tests in an individual class are run. setUpClass is called with the class as the only argument and must be decorated as a classmethod() : python @classmethod def setUpClass(cls): ... tearDownClass() A class method called after tests in an individual class have run. tearDownClass is called with the class as the only argument and must be decorated as a classmethod() : python @classmethod def tearDownClass(cls): ... For more information see official unittest docs Update test tags There are 2 files with the list of tests' tags: amp/pytest.ini .../pytest.ini (if amp is a submodule) In order to update the tags (do it in the both files): In the markers section add a name of a new tag After a : add a short description Keep tags in the alphabetical order Mocking Refs Introductory article is https://realpython.com/python-mock-library/ Official Python documentation for the mock package can be seen here unit test mock Common usage samples Best to apply on any part that is deemed unnecessary for specific test Complex functions Mocked functions can be tested separately 3rd party provider calls CCXT Talos AWS S3 See helpers/hmoto.py in cmamp repo Secrets Etc... DB calls There are many more possible combinations that can be seen in official documentation. Below are the most common ones for basic understanding. Philosophy about mocking We want to mock the minimal surface of a class E.g., assume there is a class that is interfacing with an external provider and our code places requests and get values back We want to replace the provider with an object that responds to the requests with the actual response of the provider In this way we can leave all the code of our class untouched and tested We want to test public methods of our class (and a few private methods) In other words, we want to test the end-to-end behavior and not how things are achieved Rationale: if we start testing \"how\" things are done and not \"what\" is done, we can't change how we do things (even if it doesn't affect the interface and its behavior), without updating tons of methods We want to test the minimal amount of behavior that enforces what we care about Some general suggestions about testing Test from the outside-in We want to start testing from the end-to-end methods towards the constructor of an object Rationale: often we start testing very carefully the constructor and then we get tired / run out of time when we finally get to test the actual behavior Also testing the important behavior automatically tests building the objects Use the code coverage to see what's left to test once you have tested the \"most external\" code We don't need to test all the assertions E.g., testing carefully that we can't pass a value to a constructor doesn't really test much besides the fact that dassert works (which surprisingly works!) We don't care about line coverage or checking boxes for the sake of checking boxes Use strings to compare output instead of data structures Often it's easier to do a check like: ```python # Better: expected = str(...) expected = pprint.pformat(...) # Worse: expected = [\"a\", \"b\", { ... }] ``` rather than building the data structure Some purists might not like this, but It's much faster to use a string (which is or should be one-to-one to the data structure), rather than the data structure itself By extension, many of the more complex data structure have a built-in string representation It is often more readable, easier to diff (e.g., self.assertEqual vs self.assert_equal) In case of mismatch it's easier to update the string with copy-paste rather than creating a data structure that matches what was created Use self.check_string() for things that we care about not changing (or are too big to have as strings in the code) Use self.assert_equal() for things that should not change (e.g., 1 + 1 = 2) When using check_string still try to add invariants that force the code to be correct E.g., if we want to check the PnL of a model we can freeze the output with check_string() but we want to add a constraint like there are more timestamps than 0 to avoid the situation where we update the string to something malformed Each test method should test a single test case Rationale: we want each test to be clear, simple, fast If there is repeated code we should factor it out (e.g., builders for objects) Each test should be crystal clear on how it is different from the others Often you can factor out all the common logic into an helper method Copy-paste is not allowed in unit tests in the same way it's not allowed in production code In general you want to budget the time to write unit tests E.g., \"I'm going to spend 3 hours writing unit tests\". This is going to help you focus on what's important to test and force you to use an iterative approach rather than incremental (remember the Monalisa) Write skeleton of unit tests and ask for a review if you are not sure how what to test Aka \"testing plan\" Object patch with return value import unittest.mock as umock import im_v2.ccxt.data.extract.extractor as ivcdexex @umock.patch.object(ivcdexex.hsecret, \"get_secret\") def test_function_call1(self, mock_get_secret: umock.MagicMock): mock_get_secret.return_value = \"dummy\" Function get_secret in helpers/hsecret.py is mocked Pay attention on where is get_secret mocked: It is mocked in im_v2.ccxt.data.extract.extractor as \u201cget_secret\u201d is called there in function that is being tested @umock.patch.object(hsecret, \"get_secret\") will not work as mocks are applied after all modules are loaded, hence the reason for using exact location If we import module in test itself it will work as mock is applied For modules outside of test function it is too late as they are loaded before mocks for test are applied On every call it returns string \"dummy\" Path patch with multiple return values import unittest.mock as umock @umock.patch(\"helpers.hsecret.get_secret\") def test_function_call1(self, mock_get_secret: umock.MagicMock): mock_get_secret.side_effect = [\"dummy\", Exception] On first call, string dummy is returned On second, Exception is raised Ways of calling patch and patch.object Via decorator python @umock.patch(\"helpers.hsecret.get_secret\") def test_function_call1(self, mock_get_secret: umock.MagicMock): pass In actual function python get_secret_patch = umock.patch(\"helpers.hsecret.get_secret\") get_secret_mock = get_secret_patch.start() This is the only approach in which you need to start/stop patch! The actual mock is returned as the return value of start() method! In other two approaches start/stop is handled under the hood and we are always interacting with MagicMock object Via with statement (also in function) python with umock.patch(\"\"helpers.hsecret.get_secret\"\") as get_secret_mock: pass One of the use cases for this is if we are calling a different function inside a function that is being mocked Mostly because it is easy for an eye if there are to much patches via decorator and we do not need to worry about reverting the patch changes as that is automatically done at the end of with statement Mock object state after test run @umock.patch.object(exchange_class._exchange, \"fetch_ohlcv\") def test_function_call1(self, fetch_ohlcv_mock: umock.MagicMock): self.assertEqual(fetch_ohlcv_mock.call_count, 1) actual_args = tuple(fetch_ohlcv_mock.call_args) expected_args = ( (\"BTC/USDT\",), {\"limit\": 2, \"since\": 1, \"timeframe\": \"1m\"}, ) self.assertEqual(actual_args, expected_args) After fetch_ohlcv is patched, Mock object is passed to test In this case, it is fetch_ohlcv_mock From sample we can see that function is called once First value in a tuple are positional args passed to fetch_ohlcv function Second value in a tuple are keyword args passed to fetch_ohlcv function As an alternative, fetch_ohlcv_mock.call_args.args and fetch_ohlcv_mock.call_args.kwargs can be called for separate results of args/kwargs python self.assertEqual(fetch_ohlcv_mock.call_count, 3) actual_args = str(fetch_ohlcv_mock.call_args_list) expected_args = r\"\"\" [call('BTC/USDT', since=1645660800000, bar_per_iteration=500), call('BTC/USDT', since=1645690800000, bar_per_iteration=500), call('BTC/USDT', since=1645720800000, bar_per_iteration=500)] \"\"\" self.assert_equal(actual_args, expected_args, fuzzy_match=True) In sample above, that is continuation of previous sample, fetch_ohlcv_mock.call_args_list is called that returns all calls to mocked function regardless of how many times it is called Useful for verifying that args passed are changing as expected Mock common external calls in hunitest.TestCase class class TestCcxtExtractor1(hunitest.TestCase): # Mock calls to external providers. get_secret_patch = umock.patch.object(ivcdexex.hsecret, \"get_secret\") ccxt_patch = umock.patch.object(ivcdexex, \"ccxt\", spec=ivcdexex.ccxt) def setUp(self) -> None: super().setUp() # self.get_secret_mock: umock.MagicMock = self.get_secret_patch.start() self.ccxt_mock: umock.MagicMock = self.ccxt_patch.start() # Set dummy credentials for all tests. self.get_secret_mock.return_value = {\"apiKey\": \"test\", \"secret\": \"test\"} def tearDown(self) -> None: self.get_secret_patch.stop() self.ccxt_patch.stop() # Deallocate in reverse order to avoid race conditions. super().tearDown() For every unit test we want to isolate external calls and replace them with mocks This way tests are much faster and not influenced by external factors we can not control Mocking them in setUp will make other tests using this class simpler and ready out of the box In current sample we are mocking AWS secrets and ccxt library umock.patch.object is creating patch object that is not yet activated patch.start()/stop() is activating/deactivating patch for each test run in setUp/tearDown patch.start() is returning a standard MagicMock object we can use to check various states as mentioned in previous examples and control return values call_args, call_count, return_value, side_effect, etc. Note: Although patch initialization in static variables belongs to setUp , when this code is moved there patch is created for each test separately. We want to avoid that and only start/stop same patch for each test. Mocks with specs # Regular mock and external library `ccxt` is replaced with `MagicMock` @umock.patch.object(ivcdexex, \"ccxt\") # Only `ccxt` is spec'd, not actual components that are \"deeper\" in the `ccxt` library. @umock.patch.object(ivcdexex, \"ccxt\", spec=ivcdexex.ccxt) # Everything is spec'd recursively , including returning values/instances of `ccxt` # functions and returned values/instances of returned values/instances, etc. @umock.patch.object(ivcdexex, \"ccxt\", autospec=True) First mock is not tied to any spec and we can call any attribute/function against the mock and the call will be memorized for inspection and the return value is new MagicMock . ccxt_mock.test(123) returns new MagicMock and raises no error In second mock ccxt.test(123) would fail as such function does not exists We can only call valid exchange such as ccxt_mock.binance() that will return MagicMock , as exchange is not part of the spec In third mock everything needs to be properly called ccxt_mock.binance() will return MagicMock with ccxt.Exchange spec_id (in mock instance as meta) As newly exchange instance is with spec, we can only call real functions/attributes of ccxt.Exchange class Caveats # `datetime.now` cannot be patched directly, as it is a built-in method. # Error: \"can't set attributes of built-in/extension type 'datetime.datetime'\" datetime_patch = umock.patch.object(imvcdeexut, \"datetime\", spec=imvcdeexut.datetime) Python built-in methods can not be patched python class TestExtractor1(hunitest.TestCase): # Mock `Extractor`'s abstract functions. abstract_methods_patch = umock.patch.object( imvcdexex.Extractor, \"__abstractmethods__\", new=set() ) ohlcv_patch = umock.patch.object( imvcdexex.Extractor, \"_download_ohlcv\", spec=imvcdexex.Extractor._download_ohlcv, ) Patching __abstractmethods__ function of an abstract class enables us to instantiate and test abstract class as any regular class","title":"Unit tests"},{"location":"Unit_tests/#unit-tests","text":"Running unit tests Using invoke Docker image stage and version Specifying pytest options Save test output to file Show the tests but do not run Skip submodules Compute tests coverage Timeout Rerunning timeout-ed tests Compute tests coverage An example coverage session An example with customized pytest-cov html run Generate coverage report with invoke Common usage Publishing HTML report on S3 Running pytest directly Usage and Invocations reference Custom pytest options behaviors Enable logging Update golden outcomes Incremental test mode Debugging Notebooks Running tests on GH Actions How to run a single test on GH Action Guidelines about writing unit tests What is a unit test? Why is unit testing important? Unit testing tips Tip #1: Test one thing Tip #2: Keep tests self-contained Tip #3: Only specify data related to what is being tested Tip #4: Test realistic corner cases Tip #5: Test a typical scenario Tip #6: Test executable scripts end-to-end Conventions Naming and placement conventions Our framework to test using input / output data Use text and not pickle files as input check_string vs self.assertEqual Use self.assert_equal How to split unit test code in files Skeleton for unit test Hierarchical TestCase approach Use the appropriate self.assert* Do not use hdbg.dassert Interesting testing functions Use setUp/tearDown Update test tags Mocking Refs Common usage samples Philosophy about mocking Some general suggestions about testing Test from the outside-in We don't need to test all the assertions Use strings to compare output instead of data structures Use self.check_string() for things that we care about not changing (or are too big to have as strings in the code) Each test method should test a single test case Each test should be crystal clear on how it is different from the others In general you want to budget the time to write unit tests Write skeleton of unit tests and ask for a review if you are not sure how what to test Object patch with return value Path patch with multiple return values Ways of calling patch and patch.object Mock object state after test run Mock common external calls in hunitest.TestCase class Mocks with specs Caveats","title":"Unit tests"},{"location":"Unit_tests/#running-unit-tests","text":"Before any PR (and ideally after every commit) we want to run all the unit tests to make sure we didn't introduce no new bugs We use pytest and unittest as testing framework We have different test set lists: fast Tests that are quick to execute (typically < 5 secs per test class) We want to run these tests before / after every commit / PR to make sure things are not horrible broken slow Tests that we don't want to run all the times because they are: Slow (typically < 20 seconds per test) Related to pieces of code that don't change often External APIs we don't want to hit continuously superslow Tests that run long workload, e.g., running a production model","title":"Running unit tests"},{"location":"Unit_tests/#using-invoke","text":"invoke is a task execution which allows to execute some typical workflows, e.g. run the tests ```bash # Run only fast tests: i run_fast_tests # Run only slow tests: i run_slow_tests # Run only superslow tests: i run_superslow_tests To see the options use --help option, e.g. i --help run_fast_tests : Usage: inv[oke] [--core-opts] run_fast_tests [--options] [other tasks here ...] Docstring: Run fast tests. :param stage: select a specific stage for the Docker image :param pytest_opts: option for pytest :param skip_submodules: ignore all the dir inside a submodule :param coverage: enable coverage computation :param collect_only: do not run tests but show what will be executed :param tee_to_file: save output of pytest in tmp.pytest.log :param kwargs: kwargs for ctx.run Options: -c, --coverage -k, --skip-submodules -o, --collect-only -p STRING, --pytest-opts=STRING -s STRING, --stage=STRING -t, --tee-to-file -v STRING, --version=STRING ```","title":"Using invoke"},{"location":"Unit_tests/#docker-image-stage-and-version","text":"To select a specific stage for Docker image use the --stage option. E.g., this might be useful when a user wants to run regressions on the local Docker image to verify that nothing is broken before promoting it to dev image. ```bash i run_fast_tests --stage local ``` To run the tests on the specific version of a Docker image, use the --version option. E.g., this might be useful when releasing a new version of an image. ```bash i run_fast_tests --stage local --version 1.0.4 ```","title":"Docker image stage and version"},{"location":"Unit_tests/#specifying-pytest-options","text":"With the option --pytest-opts it is possible to pass any pytest option to invoke . E.g., if a user want to run the tests in the debug mode to show the output ```bash i run_fast_tests -s --dbg ```","title":"Specifying pytest options"},{"location":"Unit_tests/#save-test-output-to-file","text":"To save the output of pytest to tmp.pytest.log use the --tee-to-file option. ```bash i run_fast_tests --tee-to-file ```","title":"Save test output to file"},{"location":"Unit_tests/#show-the-tests-but-do-not-run","text":"To show (but not run) the tests that will be executed, use the --collect-only . ```bash i run_fast_test --collect-only ```","title":"Show the tests but do not run"},{"location":"Unit_tests/#skip-submodules","text":"To skip running tests in submodules use the --skip-submodules option. This is useful for repos with submodules, e.g., dev_tools where cmamp is a submodule. Using this option, only tests in dev_tools but not in cmamp are run ```bash cd dev_tools1 i run_fast_tests --skip-submodules ```","title":"Skip submodules"},{"location":"Unit_tests/#compute-tests-coverage","text":"To compute tests coverage use the --coverage option ```bash i run_fast_tests --coverage ```","title":"Compute tests coverage"},{"location":"Unit_tests/#timeout","text":"We use the pytest-timeout package to limit durations of fast, slow, and superslow tests The timeout durations for each test type are listed here The timeout restricts only running time of the test method, not setUp and tearDown time","title":"Timeout"},{"location":"Unit_tests/#rerunning-timeout-ed-tests","text":"Running tests can take different amount of time depending on workload and machine Because of this, we rerun failing tests using pytest-rerunfailures pytest-rerunfailures is not completely compatible with pytest-timeout . This is why we have to add the -o timeout_func_only=true flag to pytest-timeout . See https://github.com/pytest-dev/pytest-rerunfailures/issues/99 for more information We rerun timeouted fast tests twice and timeouted slow and superslow tests once There is a way to provide a rerun delay for individual tests. However, we can\u2019t use it for now due to #693 (comment)","title":"Rerunning timeout-ed tests"},{"location":"Unit_tests/#compute-tests-coverage_1","text":"The documentation for coverage is here . Run a set of unit tests enabling coverage: ```bash # Run the coverage for a single test: i run_fast_tests --coverage -p oms/test/test_broker.py::TestSimulatedBroker1 # Run coverage for an entire module like oms : i run_fast_tests --coverage -p oms ``` This generates and run a pytest command inside Docker like: bash docker> /venv/bin/pytest -m \"not slow and not superslow\" oms/test/test_broker.py::TestSimulatedBroker1 --cov=. --cov-branch --cov-report term-missing --cov-report html Which generates: A default coverage report A binary .coverage file that contains the coverage information An htmlcov dir with a browsable code output to inspect the coverage for the files One can post-process the coverage report in different ways using the command coverage inside a docker container, since the code was run (as always) inside the Docker container that contains all the dependencies. ```bash coverage -h Coverage.py, version 5.5 with C extension Measure, collect, and report on code coverage in Python programs. usage: coverage [options] [args] Commands: annotate Annotate source files with execution information. combine Combine a number of data files. debug Display information about the internals of coverage.py erase Erase previously collected coverage data. help Get help on using coverage.py. html Create an HTML report. json Create a JSON report of coverage results. report Report coverage stats on modules. run Run a Python program and measure code execution. xml Create an XML report of coverage results. Use \"coverage help \" for detailed help on any command. Full documentation is at https://coverage.readthedocs.io ``` ```bash coverage report -h Usage: coverage report [options] [modules] Report coverage statistics on modules. Options: --contexts=REGEX1,REGEX2,... Only display data from lines covered in the given contexts. Accepts Python regexes, which must be quoted. --fail-under=MIN Exit with a status of 2 if the total coverage is less than MIN. -i, --ignore-errors Ignore errors while reading source files. --include=PAT1,PAT2,... Include only files whose paths match one of these patterns. Accepts shell-style wildcards, which must be quoted. --omit=PAT1,PAT2,... Omit files whose paths match one of these patterns. Accepts shell-style wildcards, which must be quoted. --precision=N Number of digits after the decimal point to display for reported coverage percentages. --sort=COLUMN Sort the report by the named column: name, stmts, miss, branch, brpart, or cover. Default is name. -m, --show-missing Show line numbers of statements in each module that weren't executed. --skip-covered Skip files with 100% coverage. --no-skip-covered Disable --skip-covered. --skip-empty Skip files with no code. --debug=OPTS Debug options, separated by commas. [env: COVERAGE_DEBUG] -h, --help Get help on this command. --rcfile=RCFILE Specify configuration file. By default '.coveragerc', 'setup.cfg', 'tox.ini', and 'pyproject.toml' are tried. [env: COVERAGE_RCFILE] ``` ```bash i docker_bash # Report the coverage for all the files under oms using the workload above (i.e., the fast tests under oms/test/test_broker.py::TestSimulatedBroker1 ) docker> coverage report --include=\"oms/*\" Name Stmts Miss Branch BrPart Cover oms/ init .py 0 0 0 0 100% oms/api.py 154 47 36 2 70% oms/broker.py 200 31 50 9 81% oms/broker_example.py 23 0 4 1 96% oms/call_optimizer.py 31 0 0 0 100% oms/devops/ init .py 0 0 0 0 100% oms/devops/docker_scripts/ init .py 0 0 0 0 100% oms/locates.py 7 7 2 0 0% oms/mr_market.py 55 1 10 1 97% oms/oms_db.py 47 0 10 3 95% oms/oms_lib_tasks.py 64 39 2 0 38% oms/oms_utils.py 34 34 6 0 0% oms/order.py 101 30 22 0 64% oms/order_example.py 26 0 0 0 100% oms/place_orders.py 121 8 18 6 90% oms/pnl_simulator.py 326 42 68 8 83% oms/portfolio.py 309 21 22 0 92% oms/portfolio_example.py 32 0 0 0 100% oms/tasks.py 3 3 0 0 0% oms/test/oms_db_helper.py 29 11 2 0 65% oms/test/test_api.py 132 25 12 0 83% oms/test/test_broker.py 33 5 4 0 86% oms/test/test_mocked_portfolio.py 0 0 0 0 100% oms/test/test_mr_market.py 46 0 2 0 100% oms/test/test_oms_db.py 114 75 14 0 38% oms/test/test_order.py 24 0 4 0 100% oms/test/test_place_orders.py 77 0 4 0 100% oms/test/test_pnl_simulator.py 235 6 16 0 98% oms/test/test_portfolio.py 135 0 6 0 100% TOTAL 2358 385 314 30 82% ``` To exclude the test files, which could inflate the coverage ```bash coverage report --include=\"oms/ \" --omit=\" /test_*.py\" Name Stmts Miss Branch BrPart Cover oms/ init .py 0 0 0 0 100% oms/api.py 154 47 36 2 70% oms/broker.py 200 31 50 9 81% oms/broker_example.py 23 0 4 1 96% oms/call_optimizer.py 31 0 0 0 100% oms/devops/ init .py 0 0 0 0 100% oms/devops/docker_scripts/ init .py 0 0 0 0 100% oms/locates.py 7 7 2 0 0% oms/mr_market.py 55 1 10 1 97% oms/oms_db.py 47 0 10 3 95% oms/oms_lib_tasks.py 64 39 2 0 38% oms/oms_utils.py 34 34 6 0 0% oms/order.py 101 30 22 0 64% oms/order_example.py 26 0 0 0 100% oms/place_orders.py 121 8 18 6 90% oms/pnl_simulator.py 326 42 68 8 83% oms/portfolio.py 309 21 22 0 92% oms/portfolio_example.py 32 0 0 0 100% oms/tasks.py 3 3 0 0 0% oms/test/oms_db_helper.py 29 11 2 0 65% TOTAL 1562 274 252 30 80% ``` To open the line coverage, from outside Docker go with your browser to htmlcov/index.html . The htmlcov is re-written with every coverage run with the --cov-report html option. If you move out index.html from htmlcov dir some html features (e.g., filtering) will not work. ```bash # On macOS: open htmlcov/index.html ``` By clicking on a file you can see which lines are not covered","title":"Compute tests coverage"},{"location":"Unit_tests/#an-example-coverage-session","text":"We want to measure the unit test coverage of oms component from both fast and slow tests We start by running the fast tests: ```bash # Run fast unit tests i run_fast_tests --coverage -p oms collected 66 items / 7 deselected / 59 selected ... # Compute the coverage for the module sorting by coverage docker> coverage report --include=\"oms/ \" --omit=\" /test_*.py\" --sort=Cover Name Stmts Miss Branch BrPart Cover oms/locates.py 7 7 2 0 0% oms/oms_utils.py 34 34 6 0 0% oms/tasks.py 3 3 0 0 0% oms/oms_lib_tasks.py 64 39 2 0 38% oms/order.py 101 30 22 0 64% oms/test/oms_db_helper.py 29 11 2 0 65% oms/api.py 154 47 36 2 70% oms/broker.py 200 31 50 9 81% oms/pnl_simulator.py 326 42 68 8 83% oms/place_orders.py 121 8 18 6 90% oms/portfolio.py 309 21 22 0 92% oms/oms_db.py 47 0 10 3 95% oms/broker_example.py 23 0 4 1 96% oms/mr_market.py 55 1 10 1 97% oms/ init .py 0 0 0 0 100% oms/call_optimizer.py 31 0 0 0 100% oms/devops/ init .py 0 0 0 0 100% oms/devops/docker_scripts/ init .py 0 0 0 0 100% oms/order_example.py 26 0 0 0 100% oms/portfolio_example.py 32 0 0 0 100% TOTAL 1562 274 252 30 80% ``` We see that certain files have a low coverage, so we want to see what is not covered. Generate the same report in a browsable format ``bash docker> rm -rf htmlcov; coverage html --include=\"oms/*\" --omit=\"*/test_*.py\" # Wrote HTML report to htmlcov/index.html` open htmlcov/index.html ``` The low coverage for tasks.py and oms_lib_tasks.py is due to the fact that we are running code through invoke that doesn't allow coverage to track it. Now we run the coverage for the slow tests ```bash # Save the coverage from the fast tests run cp .coverage .coverage_fast_tests i run_slow_tests --coverage -p oms collected 66 items / 59 deselected / 7 selected cp .coverage .coverage_slow_tests coverage report --include=\"oms/ \" --omit=\" /test_*.py\" --sort=Cover Name Stmts Miss Branch BrPart Cover oms/locates.py 7 7 2 0 0% oms/oms_utils.py 34 34 6 0 0% oms/tasks.py 3 3 0 0 0% oms/pnl_simulator.py 326 280 68 1 13% oms/place_orders.py 121 100 18 0 15% oms/mr_market.py 55 44 10 0 17% oms/portfolio.py 309 256 22 0 18% oms/call_optimizer.py 31 25 0 0 19% oms/broker.py 200 159 50 0 20% oms/order.py 101 78 22 0 20% oms/order_example.py 26 19 0 0 27% oms/broker_example.py 23 14 4 0 33% oms/portfolio_example.py 32 21 0 0 34% oms/api.py 154 107 36 0 36% oms/oms_lib_tasks.py 64 39 2 0 38% oms/oms_db.py 47 5 10 2 84% oms/ init .py 0 0 0 0 100% oms/devops/ init .py 0 0 0 0 100% oms/devops/docker_scripts/ init .py 0 0 0 0 100% oms/test/oms_db_helper.py 29 0 2 0 100% TOTAL 1562 1191 252 3 23% ``` We see that the coverage from the slow tests is only 23% for 7 tests bash root@6faaa979072e:/app/amp# coverage combine .coverage_fast_tests .coverage_slow_tests Combined data file .coverage_fast_tests Combined data file .coverage_slow_tests","title":"An example coverage session"},{"location":"Unit_tests/#an-example-with-customized-pytest-cov-html-run","text":"We want to measure unit test coverage specifically for one test in im_v2/common/data/transform/ and to save generated htmlcov in the same directory. Run command below after i docker_bash : bash pytest --cov-report term-missing --cov=im_v2/common/data/transform/ im_v2/common/data/transform/test/test_transform_utils.py --cov-report html:im_v2/common/data/transform/htmlcov \\ Output sample: bash ---------- coverage: platform linux, python 3.8.10-final-0 ----------- Name Stmts Miss Cover Missing ----------------------------------------------------------------------------------------------- im_v2/common/data/transform/convert_csv_to_pq.py 55 55 0% 2-159 im_v2/common/data/transform/extract_data_from_db.py 55 55 0% 2-125 im_v2/common/data/transform/pq_convert.py 126 126 0% 3-248 im_v2/common/data/transform/transform_pq_by_date_to_by_asset.py 131 131 0% 2-437 im_v2/common/data/transform/transform_utils.py 22 0 100% ----------------------------------------------------------------------------------------------- TOTAL 389 367 6% Coverage HTML written to dir im_v2/common/data/transform/htmlcov \\","title":"An example with customized pytest-cov html run"},{"location":"Unit_tests/#generate-coverage-report-with-invoke","text":"One can compute test coverage for a specified directory and generate text and HTML reports automatically using invoke task run_coverage_report ```bash i --help run_coverage_report INFO: > cmd='/data/grisha/src/venv/amp.client_venv/bin/invoke --help run_coverage_report' Usage: inv[oke] [--core-opts] run_coverage_report [--options] [other tasks here ...] Docstring: Compute test coverage stats. The flow is: Run tests and compute coverage stats for each test type Combine coverage stats in a single file Generate a text report Generate a HTML report (optional) Post it on S3 (optional) :param target_dir: directory to compute coverage stats for here :param generate_html_report: whether to generate HTML coverage report or not :param publish_html_on_s3: whether to publish HTML coverage report or not :param aws_profile: the AWS profile to use for publishing HTML report Options: -a STRING, --aws-profile=STRING -g, --[no-]generate-html-report -p, --[no-]publish-html-on-s3 -t STRING, --target-dir=STRING ```","title":"Generate coverage report with invoke"},{"location":"Unit_tests/#common-usage","text":"Compute coverage for market_data dir, generate text and HTML reports and publish HTML report on S3 ```bash i run_coverage_report --target-dir market_data ... Name Stmts Miss Branch BrPart Cover market_data/real_time_market_data.py 100 81 32 0 16% market_data/replayed_market_data.py 111 88 24 0 19% market_data/abstract_market_data.py 177 141 24 0 19% market_data/market_data_example.py 124 97 10 0 20% market_data/market_data_im_client.py 66 50 18 0 21% market_data/ init .py 5 0 0 0 100% TOTAL 583 457 108 0 19% Wrote HTML report to htmlcov/index.html 20:08:53 - INFO lib_tasks.py _publish_html_coverage_report_on_s3:3679 HTML coverage report is published on S3: path= s3://cryptokaizen-html/html_coverage/grisha_CmTask1038_Tool_to_extract_the_dependency_from_a_project ```","title":"Common usage"},{"location":"Unit_tests/#publishing-html-report-on-s3","text":"To make a dir with the report unique linux user and Git branch name are used, e.g., html_coverage/grisha_CmTask1038_Tool_to_extract_the_dependency_from_a_project html_coverage is the common dir on S3 for coverage reports After publishing the report, one can easily open it via a local web browser See the details in htmlcov server E.g. http://172.30.2.44/html_coverage/grisha_CmTask1038_Tool_to_extract_the_dependency_from_a_project/","title":"Publishing HTML report on S3"},{"location":"Unit_tests/#running-pytest-directly","text":"","title":"Running pytest directly"},{"location":"Unit_tests/#usage-and-invocations-reference","text":"See pytest documentation Some examples of useful command lines: ```bash # Stop at first failure pytest -x # Run a single class pytest -k TestPcaFactorComputer1 # Run a single test method pytest core/test/test_core.py::TestPcaFactorComputer1::test_linearize_eigval_eigvec # Remove cache artifacts find . -name \" pycache \" -o -name \".pytest_cache\" ./.pytest_cache ./dev_scripts/test/Test_linter_py1.test_linter1/tmp.scratch/ pycache ./dev_scripts/test/ pycache ./dev_scripts/ pycache find . -name \" pycache \" -o -name \".pytest_cache\" | xargs rm -rf # Run with a clear cache pytest --cache-clear # Run the tests that last failed (this data is stored in .pytest_cache/v/cache/lastfailed) pytest --last-failed ```","title":"Usage and Invocations reference"},{"location":"Unit_tests/#custom-pytest-options-behaviors","text":"","title":"Custom pytest options behaviors"},{"location":"Unit_tests/#enable-logging","text":"To enable logging of _LOG.debug for a single test run: ```bash # Enable debug info pytest oms/test/test_broker.py::TestSimulatedBroker1 -s --dbg ```","title":"Enable logging"},{"location":"Unit_tests/#update-golden-outcomes","text":"This switch allows to overwrite the golden outcomes that are used as reference in the unit tests to detect failures ```bash pytest --update_outcomes ```","title":"Update golden outcomes"},{"location":"Unit_tests/#incremental-test-mode","text":"This switch allows to reuse artifacts in the test directory and to skip the clean up phase It is used to re-run tests from the middle when they are very long and one wants to debug them ```bash pytest --incremental ```","title":"Incremental test mode"},{"location":"Unit_tests/#debugging-notebooks","text":"run a failing test with -s --dbg to get detailed logs e.g., > pytest core/plotting/test/test_gallery_notebook.py -s --dbg from the logs take a run_notebook.py script command that was run by the test e.g., starting like /app/dev_scripts/notebooks/run_notebook.py --notebook ... Append --no_suppress_output to this command and run it again directly from the bash e.g., like > /app/dev_scripts/notebooks/run_notebook.py --notebook ... --no_suppress_output scroll up the logs and see a report about the problem, notebooks failures will be displayed as well e.g.,","title":"Debugging Notebooks"},{"location":"Unit_tests/#running-tests-on-gh-actions","text":"The official documentation is https://docs.github.com/en/actions","title":"Running tests on GH Actions"},{"location":"Unit_tests/#how-to-run-a-single-test-on-gh-action","text":"Unfortunately there is no way to log in and run interactively on GH machines. This is a feature requested but not implemented by GH yet. All the code to run GH Actions is in the .github directory in lemonade and amp . E.g., to run a single test in the fast test target, instead of the entire regression suite You can modify .github/workflows/fast_tests.yml , by replacing bash # run: invoke run_fast_tests run: invoke run_fast_tests --pytest-opts=\"helpers/test/test_git.py::Test_git_modified_files1::test_get_modified_files_in_branch1 -s --dbg\" Note that the indentation matters, since it's a YAML file The -s --dbg is to show _LOG.debug in case you care about that to get more information Commit the code to your branch (not in master please) since GH runs each branch independently Kick off manually the fast test through the GH interface After debugging, you can revert the change from your branch to master and move along with the usual PR flow","title":"How to run a single test on GH Action"},{"location":"Unit_tests/#guidelines-about-writing-unit-tests","text":"","title":"Guidelines about writing unit tests"},{"location":"Unit_tests/#what-is-a-unit-test","text":"A unit test is a small, self-contained test of a public function or public method of a library The test specifies the given inputs, any necessary state, and the expected output Running the test ensures that the actual output agrees with the expected output","title":"What is a unit test?"},{"location":"Unit_tests/#why-is-unit-testing-important","text":"Unit testing is an integral part of Pragmatic programming approach Some of the tips that relate to it are: Design with Contracts Refactor Early, Refactor Often Test Your Software, or Your Users Will Coding Ain't Done Till All the Tests Run Test State Coverage, Not Code Coverage You Can't Write Perfect Software Crash Early Design to Test Test Early. Test Often. Test Automatically. Use Saboteurs to Test Your Testing Find Bugs Once Good unit testing improves software quality. It does this in part by Eliminating bugs (obvious) Clarifying code design and interfaces (\"Design to Test\") Making refactoring safer and easier (\"Refactor Early, Refactor Often\") Documenting expected behavior and usage","title":"Why is unit testing important?"},{"location":"Unit_tests/#unit-testing-tips","text":"","title":"Unit testing tips"},{"location":"Unit_tests/#tip-1-test-one-thing","text":"A good unit test tests only one thing Testing one thing keeps the unit test simple, relatively easy to understand, and helps isolate the root cause when the test fails How do we test more than one thing? By having more than one unit test!","title":"Tip #1: Test one thing"},{"location":"Unit_tests/#tip-2-keep-tests-self-contained","text":"A unit test should be independent of all other unit tests Each test should be self-sufficient Additionally, one should never assume that unit tests will be executed in a particular order A corollary of keeping tests self-contained is to keep all information needed to understand the test within the test itself In other words, when possible, avoid calling helper functions to load data or state to initialize the test; instead, specify the data explicitly in the test where it is used This makes the test easier to understand and easier to debug when it fails If multiple unit tests use or can use the same initialization data, do not hesitate repeating it in each test (or consider using parameterized testing)","title":"Tip #2: Keep tests self-contained"},{"location":"Unit_tests/#tip-3-only-specify-data-related-to-what-is-being-tested","text":"If a function that is being tested supports optional arguments, but those optional arguments are not needed for a particular unit test, then do not specify them in the test Specify the minimum of what is required to test what is being tested.","title":"Tip #3: Only specify data related to what is being tested"},{"location":"Unit_tests/#tip-4-test-realistic-corner-cases","text":"Can your function receive a list that is empty? Can it return an empty Series? What happens if it receives a numerical value outside of an expected range? How should the function behave in those cases? Should it crash? Should it return a reasonable default value? Expect these questions to come up in practice and think through what the appropriate behavior should be. Then, test for it.","title":"Tip #4: Test realistic corner cases"},{"location":"Unit_tests/#tip-5-test-a-typical-scenario","text":"In ensuring that corner cases are covered, do not overlook testing basic functionality for typical cases This is useful for verifying current behavior and to support refactoring.","title":"Tip #5: Test a typical scenario"},{"location":"Unit_tests/#tip-6-test-executable-scripts-end-to-end","text":"In some cases, like scripts, it is easy to get lost chasing the coverage % E.g. covering every line of the original, including the parser This is not always necessary If you are able to run a script with all arguments present, it means that the parser works correctly So an end-to-end smoke test will also cover the parser This saves a little time and reduces the bloat If you need to test the functionality, consider factoring out as much code as possible from _main A good practice is to have a _run function that does all the job and _main only brings together the parser and the executable part","title":"Tip #6: Test executable scripts end-to-end"},{"location":"Unit_tests/#conventions","text":"","title":"Conventions"},{"location":"Unit_tests/#naming-and-placement-conventions","text":"We follow the convention (that happen to be mostly the default to pytest ): A directory test that contains all the test code and artifacts The test directory contains all the test_*.py files and all inputs and outputs for the tests. A unit test file should be close to the library / code it tests The test class should make clear reference to the code that is tested To test a class FooBar the corresponding test class is named TestFooBar, we use the CamelCase for the test classes You can add a number, e.g., TestFooBar1() , if there are multiple test classes that are testing the code in different ways (e.g., setUp() tearDown() are different) To test a function generate_html_tables() the corresponding test class is Test_generate_html_tables It's ok to have multiple test methods, e.g., for FooBar.method_a() and FooBar.method_b() , the test method is: python class TestFooBar1(unittest2.TestCase): def test_method_a(self): ... def test_method_b(self): ... Split test classes and methods in a reasonable way, so each one tests one single thing in the simplest possible way Remember that test code is not second class citizen, although it's auxiliary to the code Add comments and docstring explaining what the code is doing If you change the name of a class, also the test should be changed If you change the name of a file also the name of the file with the testing code should be changed","title":"Naming and placement conventions"},{"location":"Unit_tests/#our-framework-to-test-using-input-output-data","text":"helpers/unit_test.py has some utilities to easily create input and output dirs storing data for unit tests hut.TestCase has various methods to help you create get_input_dir : return the name of the dir used to store the inputs get_scratch_space : return the name of a scratch dir to keep artifacts of the test get_output_dir : probably not interesting for the user The directory structure enforced by the out TestCase is like: ```bash tree -d edg/form_8/test/ edg/form_8/test/ \u2514\u2500\u2500 TestExtractTables1.test1 \u251c\u2500\u2500 input \u2514\u2500\u2500 output ``` The layout of test dir: ```bash ls -1 helpers/test/ Test_dassert1.test2 Test_dassert1.test3 Test_dassert1.test4 ... Test_dassert_misc1.test6 Test_dassert_misc1.test8 Test_system1.test7 test_dbg.py test_helpers.py test_system_interaction.py ```","title":"Our framework to test using input / output data"},{"location":"Unit_tests/#use-text-and-not-pickle-files-as-input","text":"The problem with pickle files are the usual ones They are not stable across different version of libraries They are not human readable Prefer to use text file E.g., use a CSV file If the data used for testing is generated in a non-complicated way Document how it was generated Even better add a test that generates the data Use a subset of the input data The smaller the better for everybody Fast tests Easier to debug More targeted unit test Do not check in 1 megabyte of test data!","title":"Use text and not pickle files as input"},{"location":"Unit_tests/#check_string-vs-selfassertequal","text":"TODO(gp): Add","title":"check_string vs self.assertEqual"},{"location":"Unit_tests/#use-selfassert_equal","text":"This is a function that helps you understand what are the mismatches It works on str","title":"Use self.assert_equal"},{"location":"Unit_tests/#how-to-split-unit-test-code-in-files","text":"The two extreme approaches are: All the test code for a directory goes in one file foo/bar/test/test_$DIRNAME.py (or foo/bar/test/test_all.py ) Each file foo/bar/$FILENAME with code gets its corresponding foo/bar/test/test_$FILENAME.py It should also be named according to the library it tests For example, if the library to test is called pnl.py , then the corresponding unit test should be called test_pnl.py Pros of 1) vs 2) Less maintenance churn It takes work to keep the code and the test files in sync, e.g., If you change the name of the code file you don't have to change other file names If you move one class from one file to another, you might not need to move test code Fewer files opened in your editor Avoid many files with a lot of boilerplate code Cons of 1) vs 2) The single file can become huge! Compromise solution: Start with a single file test_$DIRNAME.py (or test*dir_name.py ) * In the large file add a framed comment like: python # ################## # Unit tests for \u2026 # ################## So it's easy to find which file is tested were using grep Then split when it becomes too big using test_$FILENAME.py","title":"How to split unit test code in files"},{"location":"Unit_tests/#skeleton-for-unit-test","text":"Interesting unit tests are in helpers/test A unit test looks like: python import helpers.unit_test as hut class Test...(hut.TestCase): def test...(self): ... pytest will take care of running the code so you don't need: python if __name__ == '__main__': unittest.main()","title":"Skeleton for unit test"},{"location":"Unit_tests/#hierarchical-testcase-approach","text":"Whenever there is hierarchy in classes, we also create a hierarchy of test classes A parent test class looks like: python import helpers.unit_test as hut class SomeClientTestCase(hut.TestCase): def _test...1(self): ... def _test...2(self): ... While a child test class looks like this where test methods use the corresponding methods from the parent test class: python class TestSomeClient(SomeClientTestCase): def test...1(self): ... def test...2(self): ... Each TestCase tests a \"behavior\" like a set of related methods Each TestCase is under the test dir Each derived class should use the proper TestCase classes to reach a decent coverage It is OK to use non-private methods in test classes to ensure that the code is in order of dependency, so that the reader doesn't have to jump back / forth We want to separate chunks of unit test code using: python # ######################################################################## putting all the methods used by that chunk at the beginning and so on It is OK to skip a TestCase method if not meaningful, when coverage is enough As an example, see im_v2/common/data/client/test/im_client_test_case.py and im_v2/ccxt/data/client/test/test_ccxt_clients.py","title":"Hierarchical TestCase approach"},{"location":"Unit_tests/#use-the-appropriate-selfassert","text":"When you get a failure you don't want to get something like \"True is not False\", rather an informative message like \"5 is not < 4\" Bad \\ self.assertTrue(a &lt; b) Good \\ self.assertLess(a, b)","title":"Use the appropriate self.assert*"},{"location":"Unit_tests/#do-not-use-hdbgdassert","text":"dassert are for checking self-consistency of the code The invariant is that you can remove dbg.dassert without changing the behavior of the code. Of course you can't remove the assertion and get unit tests to work","title":"Do not use hdbg.dassert"},{"location":"Unit_tests/#interesting-testing-functions","text":"List of useful testing functions are: General python Numpy Pandas","title":"Interesting testing functions"},{"location":"Unit_tests/#use-setupteardown","text":"If you have a lot of repeated code in your tests, you can make them shorter by moving this code to setUp/tearDown methods: setUp() \\ Method called to prepare the test fixture. This is called immediately before calling the test method; other than AssertionError or SkipTest , any exception raised by this method will be considered an error rather than a test failure. The default implementation does nothing. tearDown() \\ Method called immediately after the test method has been called and the result recorded. This is called even if the test method raised an exception, so the implementation in subclasses may need to be particularly careful about checking internal state. Any exception, other than AssertionError or SkipTest , raised by this method will be considered an additional error rather than a test failure (thus increasing the total number of reported errors). This method will only be called if the setUp() succeeds, regardless of the outcome of the test method. The default implementation does nothing. If you need some expensive code parts to be done once for the whole test class, such as opening a database connection, opening a temporary file on the filesystem, loading a shared library for testing, etc., you can use setUpClass/tearDownClass methods: setUpClass() A class method called before tests in an individual class are run. setUpClass is called with the class as the only argument and must be decorated as a classmethod() : python @classmethod def setUpClass(cls): ... tearDownClass() A class method called after tests in an individual class have run. tearDownClass is called with the class as the only argument and must be decorated as a classmethod() : python @classmethod def tearDownClass(cls): ... For more information see official unittest docs","title":"Use setUp/tearDown"},{"location":"Unit_tests/#update-test-tags","text":"There are 2 files with the list of tests' tags: amp/pytest.ini .../pytest.ini (if amp is a submodule) In order to update the tags (do it in the both files): In the markers section add a name of a new tag After a : add a short description Keep tags in the alphabetical order","title":"Update test tags"},{"location":"Unit_tests/#mocking","text":"","title":"Mocking"},{"location":"Unit_tests/#refs","text":"Introductory article is https://realpython.com/python-mock-library/ Official Python documentation for the mock package can be seen here unit test mock","title":"Refs"},{"location":"Unit_tests/#common-usage-samples","text":"Best to apply on any part that is deemed unnecessary for specific test Complex functions Mocked functions can be tested separately 3rd party provider calls CCXT Talos AWS S3 See helpers/hmoto.py in cmamp repo Secrets Etc... DB calls There are many more possible combinations that can be seen in official documentation. Below are the most common ones for basic understanding.","title":"Common usage samples"},{"location":"Unit_tests/#philosophy-about-mocking","text":"We want to mock the minimal surface of a class E.g., assume there is a class that is interfacing with an external provider and our code places requests and get values back We want to replace the provider with an object that responds to the requests with the actual response of the provider In this way we can leave all the code of our class untouched and tested We want to test public methods of our class (and a few private methods) In other words, we want to test the end-to-end behavior and not how things are achieved Rationale: if we start testing \"how\" things are done and not \"what\" is done, we can't change how we do things (even if it doesn't affect the interface and its behavior), without updating tons of methods We want to test the minimal amount of behavior that enforces what we care about","title":"Philosophy about mocking"},{"location":"Unit_tests/#some-general-suggestions-about-testing","text":"","title":"Some general suggestions about testing"},{"location":"Unit_tests/#test-from-the-outside-in","text":"We want to start testing from the end-to-end methods towards the constructor of an object Rationale: often we start testing very carefully the constructor and then we get tired / run out of time when we finally get to test the actual behavior Also testing the important behavior automatically tests building the objects Use the code coverage to see what's left to test once you have tested the \"most external\" code","title":"Test from the outside-in"},{"location":"Unit_tests/#we-dont-need-to-test-all-the-assertions","text":"E.g., testing carefully that we can't pass a value to a constructor doesn't really test much besides the fact that dassert works (which surprisingly works!) We don't care about line coverage or checking boxes for the sake of checking boxes","title":"We don't need to test all the assertions"},{"location":"Unit_tests/#use-strings-to-compare-output-instead-of-data-structures","text":"Often it's easier to do a check like: ```python # Better: expected = str(...) expected = pprint.pformat(...) # Worse: expected = [\"a\", \"b\", { ... }] ``` rather than building the data structure Some purists might not like this, but It's much faster to use a string (which is or should be one-to-one to the data structure), rather than the data structure itself By extension, many of the more complex data structure have a built-in string representation It is often more readable, easier to diff (e.g., self.assertEqual vs self.assert_equal) In case of mismatch it's easier to update the string with copy-paste rather than creating a data structure that matches what was created","title":"Use strings to compare output instead of data structures"},{"location":"Unit_tests/#use-selfcheck_string-for-things-that-we-care-about-not-changing-or-are-too-big-to-have-as-strings-in-the-code","text":"Use self.assert_equal() for things that should not change (e.g., 1 + 1 = 2) When using check_string still try to add invariants that force the code to be correct E.g., if we want to check the PnL of a model we can freeze the output with check_string() but we want to add a constraint like there are more timestamps than 0 to avoid the situation where we update the string to something malformed","title":"Use self.check_string() for things that we care about not changing (or are too big to have as strings in the code)"},{"location":"Unit_tests/#each-test-method-should-test-a-single-test-case","text":"Rationale: we want each test to be clear, simple, fast If there is repeated code we should factor it out (e.g., builders for objects)","title":"Each test method should test a single test case"},{"location":"Unit_tests/#each-test-should-be-crystal-clear-on-how-it-is-different-from-the-others","text":"Often you can factor out all the common logic into an helper method Copy-paste is not allowed in unit tests in the same way it's not allowed in production code","title":"Each test should be crystal clear on how it is different from the others"},{"location":"Unit_tests/#in-general-you-want-to-budget-the-time-to-write-unit-tests","text":"E.g., \"I'm going to spend 3 hours writing unit tests\". This is going to help you focus on what's important to test and force you to use an iterative approach rather than incremental (remember the Monalisa)","title":"In general you want to budget the time to write unit tests"},{"location":"Unit_tests/#write-skeleton-of-unit-tests-and-ask-for-a-review-if-you-are-not-sure-how-what-to-test","text":"Aka \"testing plan\"","title":"Write skeleton of unit tests and ask for a review if you are not sure how what to test"},{"location":"Unit_tests/#object-patch-with-return-value","text":"import unittest.mock as umock import im_v2.ccxt.data.extract.extractor as ivcdexex @umock.patch.object(ivcdexex.hsecret, \"get_secret\") def test_function_call1(self, mock_get_secret: umock.MagicMock): mock_get_secret.return_value = \"dummy\" Function get_secret in helpers/hsecret.py is mocked Pay attention on where is get_secret mocked: It is mocked in im_v2.ccxt.data.extract.extractor as \u201cget_secret\u201d is called there in function that is being tested @umock.patch.object(hsecret, \"get_secret\") will not work as mocks are applied after all modules are loaded, hence the reason for using exact location If we import module in test itself it will work as mock is applied For modules outside of test function it is too late as they are loaded before mocks for test are applied On every call it returns string \"dummy\"","title":"Object patch with return value"},{"location":"Unit_tests/#path-patch-with-multiple-return-values","text":"import unittest.mock as umock @umock.patch(\"helpers.hsecret.get_secret\") def test_function_call1(self, mock_get_secret: umock.MagicMock): mock_get_secret.side_effect = [\"dummy\", Exception] On first call, string dummy is returned On second, Exception is raised","title":"Path patch with multiple return values"},{"location":"Unit_tests/#ways-of-calling-patch-and-patchobject","text":"Via decorator python @umock.patch(\"helpers.hsecret.get_secret\") def test_function_call1(self, mock_get_secret: umock.MagicMock): pass In actual function python get_secret_patch = umock.patch(\"helpers.hsecret.get_secret\") get_secret_mock = get_secret_patch.start() This is the only approach in which you need to start/stop patch! The actual mock is returned as the return value of start() method! In other two approaches start/stop is handled under the hood and we are always interacting with MagicMock object Via with statement (also in function) python with umock.patch(\"\"helpers.hsecret.get_secret\"\") as get_secret_mock: pass One of the use cases for this is if we are calling a different function inside a function that is being mocked Mostly because it is easy for an eye if there are to much patches via decorator and we do not need to worry about reverting the patch changes as that is automatically done at the end of with statement","title":"Ways of calling patch and patch.object"},{"location":"Unit_tests/#mock-object-state-after-test-run","text":"@umock.patch.object(exchange_class._exchange, \"fetch_ohlcv\") def test_function_call1(self, fetch_ohlcv_mock: umock.MagicMock): self.assertEqual(fetch_ohlcv_mock.call_count, 1) actual_args = tuple(fetch_ohlcv_mock.call_args) expected_args = ( (\"BTC/USDT\",), {\"limit\": 2, \"since\": 1, \"timeframe\": \"1m\"}, ) self.assertEqual(actual_args, expected_args) After fetch_ohlcv is patched, Mock object is passed to test In this case, it is fetch_ohlcv_mock From sample we can see that function is called once First value in a tuple are positional args passed to fetch_ohlcv function Second value in a tuple are keyword args passed to fetch_ohlcv function As an alternative, fetch_ohlcv_mock.call_args.args and fetch_ohlcv_mock.call_args.kwargs can be called for separate results of args/kwargs python self.assertEqual(fetch_ohlcv_mock.call_count, 3) actual_args = str(fetch_ohlcv_mock.call_args_list) expected_args = r\"\"\" [call('BTC/USDT', since=1645660800000, bar_per_iteration=500), call('BTC/USDT', since=1645690800000, bar_per_iteration=500), call('BTC/USDT', since=1645720800000, bar_per_iteration=500)] \"\"\" self.assert_equal(actual_args, expected_args, fuzzy_match=True) In sample above, that is continuation of previous sample, fetch_ohlcv_mock.call_args_list is called that returns all calls to mocked function regardless of how many times it is called Useful for verifying that args passed are changing as expected","title":"Mock object state after test run"},{"location":"Unit_tests/#mock-common-external-calls-in-hunitesttestcase-class","text":"class TestCcxtExtractor1(hunitest.TestCase): # Mock calls to external providers. get_secret_patch = umock.patch.object(ivcdexex.hsecret, \"get_secret\") ccxt_patch = umock.patch.object(ivcdexex, \"ccxt\", spec=ivcdexex.ccxt) def setUp(self) -> None: super().setUp() # self.get_secret_mock: umock.MagicMock = self.get_secret_patch.start() self.ccxt_mock: umock.MagicMock = self.ccxt_patch.start() # Set dummy credentials for all tests. self.get_secret_mock.return_value = {\"apiKey\": \"test\", \"secret\": \"test\"} def tearDown(self) -> None: self.get_secret_patch.stop() self.ccxt_patch.stop() # Deallocate in reverse order to avoid race conditions. super().tearDown() For every unit test we want to isolate external calls and replace them with mocks This way tests are much faster and not influenced by external factors we can not control Mocking them in setUp will make other tests using this class simpler and ready out of the box In current sample we are mocking AWS secrets and ccxt library umock.patch.object is creating patch object that is not yet activated patch.start()/stop() is activating/deactivating patch for each test run in setUp/tearDown patch.start() is returning a standard MagicMock object we can use to check various states as mentioned in previous examples and control return values call_args, call_count, return_value, side_effect, etc. Note: Although patch initialization in static variables belongs to setUp , when this code is moved there patch is created for each test separately. We want to avoid that and only start/stop same patch for each test.","title":"Mock common external calls in hunitest.TestCase class"},{"location":"Unit_tests/#mocks-with-specs","text":"# Regular mock and external library `ccxt` is replaced with `MagicMock` @umock.patch.object(ivcdexex, \"ccxt\") # Only `ccxt` is spec'd, not actual components that are \"deeper\" in the `ccxt` library. @umock.patch.object(ivcdexex, \"ccxt\", spec=ivcdexex.ccxt) # Everything is spec'd recursively , including returning values/instances of `ccxt` # functions and returned values/instances of returned values/instances, etc. @umock.patch.object(ivcdexex, \"ccxt\", autospec=True) First mock is not tied to any spec and we can call any attribute/function against the mock and the call will be memorized for inspection and the return value is new MagicMock . ccxt_mock.test(123) returns new MagicMock and raises no error In second mock ccxt.test(123) would fail as such function does not exists We can only call valid exchange such as ccxt_mock.binance() that will return MagicMock , as exchange is not part of the spec In third mock everything needs to be properly called ccxt_mock.binance() will return MagicMock with ccxt.Exchange spec_id (in mock instance as meta) As newly exchange instance is with spec, we can only call real functions/attributes of ccxt.Exchange class","title":"Mocks with specs"},{"location":"Unit_tests/#caveats","text":"# `datetime.now` cannot be patched directly, as it is a built-in method. # Error: \"can't set attributes of built-in/extension type 'datetime.datetime'\" datetime_patch = umock.patch.object(imvcdeexut, \"datetime\", spec=imvcdeexut.datetime) Python built-in methods can not be patched python class TestExtractor1(hunitest.TestCase): # Mock `Extractor`'s abstract functions. abstract_methods_patch = umock.patch.object( imvcdexex.Extractor, \"__abstractmethods__\", new=set() ) ohlcv_patch = umock.patch.object( imvcdexex.Extractor, \"_download_ohlcv\", spec=imvcdexex.Extractor._download_ohlcv, ) Patching __abstractmethods__ function of an abstract class enables us to instantiate and test abstract class as any regular class","title":"Caveats"},{"location":"Visual_Studio_Code/","text":"Visual Studio Code Connecting via VNC Installing VNC Installation of VS Code Windows, Linux, Mac Connecting via VNC Make sure you have a VPN connection. Installing VNC Install VNC using this link: https://www.realvnc.com/en/connect/download/viewer/windows/ Sysadmin has sent you: os_password.txt your username $USER a key crypto.pub that looks like: -----BEGIN OPENSSH PRIVATE KEY----- b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAABlwAAAAdzc2gtcn NhAAAAAwEAAQAAAYEA0IQsLy1lL3bhPT+43sht2/m9tqZm8sEQrXMAVtfm4ji/LXMr7094 \u2026 hakqVTlQ2sr0YTAAAAHnNhZ2dlc2VAZ3BtYWMuZmlvcy1yb3V0ZXIuaG9tZQECAwQ= -----END OPENSSH PRIVATE KEY----- Let's say you are connected via VNC. Login into the OS. Run pycharm.sh using terminal (should be there): ``` bash /opt/pycharm-community-2021.2.3/bin/pycharm.sh ``` Installation of VS Code Windows, Linux, Mac Download the installer using this link: Download Visual Studio Code - Mac, Linux, Windows . Run the installer and follow the wizard steps. To run VS Code, find it in the Start menu or use the desktop shortcut. In the left navigation bar search for extensions ( or use Ctrl+Shift+X ) and search for \"ms-vscode-remote.remote-ssh\" and then click on the install button. Connect to the VPN. In bottom left corner click on this green button: Then you will see these options on top of the screen, click on \"Open SSH Configuration File\u2026\" and then click on the user\\.ssh\\config or user/.ssh/config . The config should look like this: HostName : dev1 (or dev2) server IP. User : your linux user name on the dev server. IdentityFile : private key that you use to SSH to the dev server. Save and close the config file and press the green button again, then for connection click on \"Connect to Host...\". You should see the IP address of the server, so just click on it and it will connect you in a new window. Open a preferred repo directory. Click on the \"Source control\" button on the left: Choose \"Open Folder\": Choose the desired repo directory from the drop-down menu, e.g., cmamp1 :","title":"Visual Studio Code"},{"location":"Visual_Studio_Code/#visual-studio-code","text":"Connecting via VNC Installing VNC Installation of VS Code Windows, Linux, Mac","title":"Visual Studio Code"},{"location":"Visual_Studio_Code/#connecting-via-vnc","text":"Make sure you have a VPN connection.","title":"Connecting via VNC"},{"location":"Visual_Studio_Code/#installing-vnc","text":"Install VNC using this link: https://www.realvnc.com/en/connect/download/viewer/windows/ Sysadmin has sent you: os_password.txt your username $USER a key crypto.pub that looks like: -----BEGIN OPENSSH PRIVATE KEY----- b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAABlwAAAAdzc2gtcn NhAAAAAwEAAQAAAYEA0IQsLy1lL3bhPT+43sht2/m9tqZm8sEQrXMAVtfm4ji/LXMr7094 \u2026 hakqVTlQ2sr0YTAAAAHnNhZ2dlc2VAZ3BtYWMuZmlvcy1yb3V0ZXIuaG9tZQECAwQ= -----END OPENSSH PRIVATE KEY----- Let's say you are connected via VNC. Login into the OS. Run pycharm.sh using terminal (should be there): ``` bash /opt/pycharm-community-2021.2.3/bin/pycharm.sh ```","title":"Installing VNC"},{"location":"Visual_Studio_Code/#installation-of-vs-code","text":"","title":"Installation of VS Code"},{"location":"Visual_Studio_Code/#windows-linux-mac","text":"Download the installer using this link: Download Visual Studio Code - Mac, Linux, Windows . Run the installer and follow the wizard steps. To run VS Code, find it in the Start menu or use the desktop shortcut. In the left navigation bar search for extensions ( or use Ctrl+Shift+X ) and search for \"ms-vscode-remote.remote-ssh\" and then click on the install button. Connect to the VPN. In bottom left corner click on this green button: Then you will see these options on top of the screen, click on \"Open SSH Configuration File\u2026\" and then click on the user\\.ssh\\config or user/.ssh/config . The config should look like this: HostName : dev1 (or dev2) server IP. User : your linux user name on the dev server. IdentityFile : private key that you use to SSH to the dev server. Save and close the config file and press the green button again, then for connection click on \"Connect to Host...\". You should see the IP address of the server, so just click on it and it will connect you in a new window. Open a preferred repo directory. Click on the \"Source control\" button on the left: Choose \"Open Folder\": Choose the desired repo directory from the drop-down menu, e.g., cmamp1 :","title":"Windows, Linux, Mac"},{"location":"Workflows/","text":"Navigate stack of failed test Inside the container ``` pytest_log dataflow/model/test/test_tiled_flows.py::Test_evaluate_weighted_forecasts::test_combine_two_signals ... =================================== FAILURES =================================== _ Test evaluate_weighted_forecasts.test_combine_two_signals _ __ Traceback (most recent call last): File \"/app/dataflow/model/test/test_tiled_flows.py\", line 78, in test_combine_two_signals bar_metrics = dtfmotiflo.evaluate_weighted_forecasts( File \"/app/dataflow/model/tiled_flows.py\", line 265, in evaluate_weighted_forecasts weighted_sum = hpandas.compute_weighted_sum( TypeError: compute_weighted_sum() got an unexpected keyword argument 'index_mode' ============================= slowest 3 durations ============================== 2.18s call dataflow/model/test/test_tiled_flows.py::Test_evaluate_weighted_forecasts::test_combine_two_signals 0.01s setup dataflow/model/test/test_tiled_flows.py::Test_evaluate_weighted_forecasts::test_combine_two_signals 0.00s teardown dataflow/model/test/test_tiled_flows.py::Test_evaluate_weighted_forecasts::test_combine_two_signals ``` Outside the container ``` i traceback ```","title":"Navigate stack of failed test"},{"location":"Workflows/#navigate-stack-of-failed-test","text":"Inside the container ``` pytest_log dataflow/model/test/test_tiled_flows.py::Test_evaluate_weighted_forecasts::test_combine_two_signals ... =================================== FAILURES =================================== _ Test evaluate_weighted_forecasts.test_combine_two_signals _ __ Traceback (most recent call last): File \"/app/dataflow/model/test/test_tiled_flows.py\", line 78, in test_combine_two_signals bar_metrics = dtfmotiflo.evaluate_weighted_forecasts( File \"/app/dataflow/model/tiled_flows.py\", line 265, in evaluate_weighted_forecasts weighted_sum = hpandas.compute_weighted_sum( TypeError: compute_weighted_sum() got an unexpected keyword argument 'index_mode' ============================= slowest 3 durations ============================== 2.18s call dataflow/model/test/test_tiled_flows.py::Test_evaluate_weighted_forecasts::test_combine_two_signals 0.01s setup dataflow/model/test/test_tiled_flows.py::Test_evaluate_weighted_forecasts::test_combine_two_signals 0.00s teardown dataflow/model/test/test_tiled_flows.py::Test_evaluate_weighted_forecasts::test_combine_two_signals ``` Outside the container ``` i traceback ```","title":"Navigate stack of failed test"}]}