[2023-05-04 18:01:40,132] {taskinstance.py:1035} INFO - Dependencies all met for <TaskInstance: perform_predictions.predict.postgres.google_trends scheduled__2023-05-04T17:57:00+00:00 [queued]>
[2023-05-04 18:01:40,140] {taskinstance.py:1035} INFO - Dependencies all met for <TaskInstance: perform_predictions.predict.postgres.google_trends scheduled__2023-05-04T17:57:00+00:00 [queued]>
[2023-05-04 18:01:40,141] {taskinstance.py:1241} INFO - 
--------------------------------------------------------------------------------
[2023-05-04 18:01:40,142] {taskinstance.py:1242} INFO - Starting attempt 1 of 1
[2023-05-04 18:01:40,143] {taskinstance.py:1243} INFO - 
--------------------------------------------------------------------------------
[2023-05-04 18:01:40,152] {taskinstance.py:1262} INFO - Executing <Task(BashOperator): predict.postgres.google_trends> on 2023-05-04 17:57:00+00:00
[2023-05-04 18:01:40,156] {standard_task_runner.py:52} INFO - Started process 859 to run task
[2023-05-04 18:01:40,160] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'perform_predictions', 'predict.postgres.google_trends', 'scheduled__2023-05-04T17:57:00+00:00', '--job-id', '27', '--raw', '--subdir', 'DAGS_FOLDER/***_predictions_scheduler.py', '--cfg-path', '/tmp/tmpu11tquk2', '--error-file', '/tmp/tmp1mwto5hx']
[2023-05-04 18:01:40,161] {standard_task_runner.py:77} INFO - Job 27: Subtask predict.postgres.google_trends
[2023-05-04 18:01:40,205] {logging_mixin.py:109} INFO - Running <TaskInstance: perform_predictions.predict.postgres.google_trends scheduled__2023-05-04T17:57:00+00:00 [running]> on host c896eb598e2f
[2023-05-04 18:01:40,249] {taskinstance.py:1429} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_EMAIL=***@example.com
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=perform_predictions
AIRFLOW_CTX_TASK_ID=predict.postgres.google_trends
AIRFLOW_CTX_EXECUTION_DATE=2023-05-04T17:57:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-05-04T17:57:00+00:00
[2023-05-04 18:01:40,251] {subprocess.py:62} INFO - Tmp dir root location: 
 /tmp
[2023-05-04 18:01:40,252] {subprocess.py:74} INFO - Running command: ['bash', '-c', 'sleep 5 && /cmamp/src/load_validate_transform.py --source_table google_trends_data --target_table google_trends_predictions --topic ipad']
[2023-05-04 18:01:40,260] {subprocess.py:85} INFO - Output:
[2023-05-04 18:02:01,727] {subprocess.py:89} INFO - Data fetched from 'google_trends_data' table:
[2023-05-04 18:02:01,728] {subprocess.py:89} INFO -     Topic      Time  Frequency
[2023-05-04 18:02:01,730] {subprocess.py:89} INFO - 0    ipad  Jan 2004        0.0
[2023-05-04 18:02:01,730] {subprocess.py:89} INFO - 1    ipad  Feb 2004        0.0
[2023-05-04 18:02:01,731] {subprocess.py:89} INFO - 2    ipad  Mar 2004        0.0
[2023-05-04 18:02:01,732] {subprocess.py:89} INFO - 3    ipad  Apr 2004        0.0
[2023-05-04 18:02:01,733] {subprocess.py:89} INFO - 4    ipad  May 2004        0.0
[2023-05-04 18:02:01,734] {subprocess.py:89} INFO - ..    ...       ...        ...
[2023-05-04 18:02:01,735] {subprocess.py:89} INFO - 227  ipad  Dec 2022       27.0
[2023-05-04 18:02:01,736] {subprocess.py:89} INFO - 228  ipad  Jan 2023       25.0
[2023-05-04 18:02:01,737] {subprocess.py:89} INFO - 229  ipad  Feb 2023       23.0
[2023-05-04 18:02:01,738] {subprocess.py:89} INFO - 230  ipad  Mar 2023       21.0
[2023-05-04 18:02:01,738] {subprocess.py:89} INFO - 231  ipad  Apr 2023       20.0
[2023-05-04 18:02:01,739] {subprocess.py:89} INFO - 
[2023-05-04 18:02:01,740] {subprocess.py:89} INFO - [232 rows x 3 columns]
[2023-05-04 18:02:01,741] {subprocess.py:89} INFO - 
[2023-05-04 18:02:01,742] {subprocess.py:89} INFO - Running all QA checks:
[2023-05-04 18:02:01,743] {subprocess.py:89} INFO - DenormalizedDatasetCheck: PASSED
[2023-05-04 18:02:01,743] {subprocess.py:89} INFO - 
[2023-05-04 18:02:01,744] {subprocess.py:89} INFO - Step 1: Converting to Dask Dataframe, Done
[2023-05-04 18:02:01,745] {subprocess.py:89} INFO - Step 2: Preprocessing before prediction, Done
[2023-05-04 18:02:01,746] {subprocess.py:89} INFO - Performing stepwise search to minimize aic
[2023-05-04 18:02:01,748] {subprocess.py:89} INFO -  ARIMA(0,1,0)(0,1,0)[12]             : AIC=1464.281, Time=0.02 sec
[2023-05-04 18:02:01,748] {subprocess.py:89} INFO -  ARIMA(1,1,0)(1,1,0)[12]             : AIC=1428.099, Time=0.08 sec
[2023-05-04 18:02:01,749] {subprocess.py:89} INFO -  ARIMA(0,1,1)(0,1,1)[12]             : AIC=1394.864, Time=0.20 sec
[2023-05-04 18:02:01,750] {subprocess.py:89} INFO -  ARIMA(0,1,1)(0,1,0)[12]             : AIC=1428.285, Time=0.04 sec
[2023-05-04 18:02:01,751] {subprocess.py:89} INFO -  ARIMA(0,1,1)(1,1,1)[12]             : AIC=inf, Time=0.59 sec
[2023-05-04 18:02:01,752] {subprocess.py:89} INFO -  ARIMA(0,1,1)(0,1,2)[12]             : AIC=inf, Time=1.17 sec
[2023-05-04 18:02:01,753] {subprocess.py:89} INFO -  ARIMA(0,1,1)(1,1,0)[12]             : AIC=1418.077, Time=0.10 sec
[2023-05-04 18:02:01,753] {subprocess.py:89} INFO -  ARIMA(0,1,1)(1,1,2)[12]             : AIC=inf, Time=1.35 sec
[2023-05-04 18:02:01,754] {subprocess.py:89} INFO -  ARIMA(0,1,0)(0,1,1)[12]             : AIC=inf, Time=0.24 sec
[2023-05-04 18:02:01,755] {subprocess.py:89} INFO -  ARIMA(1,1,1)(0,1,1)[12]             : AIC=1396.817, Time=0.25 sec
[2023-05-04 18:02:01,756] {subprocess.py:89} INFO -  ARIMA(0,1,2)(0,1,1)[12]             : AIC=1396.719, Time=0.29 sec
[2023-05-04 18:02:01,757] {subprocess.py:89} INFO -  ARIMA(1,1,0)(0,1,1)[12]             : AIC=inf, Time=0.19 sec
[2023-05-04 18:02:01,758] {subprocess.py:89} INFO -  ARIMA(1,1,2)(0,1,1)[12]             : AIC=1385.231, Time=0.44 sec
[2023-05-04 18:02:01,759] {subprocess.py:89} INFO -  ARIMA(1,1,2)(0,1,0)[12]             : AIC=inf, Time=0.23 sec
[2023-05-04 18:02:01,759] {subprocess.py:89} INFO -  ARIMA(1,1,2)(1,1,1)[12]             : AIC=inf, Time=1.10 sec
[2023-05-04 18:02:01,760] {subprocess.py:89} INFO -  ARIMA(1,1,2)(0,1,2)[12]             : AIC=inf, Time=1.75 sec
[2023-05-04 18:02:01,761] {subprocess.py:89} INFO -  ARIMA(1,1,2)(1,1,0)[12]             : AIC=inf, Time=0.60 sec
[2023-05-04 18:02:01,762] {subprocess.py:89} INFO -  ARIMA(1,1,2)(1,1,2)[12]             : AIC=inf, Time=2.42 sec
[2023-05-04 18:02:01,763] {subprocess.py:89} INFO -  ARIMA(2,1,2)(0,1,1)[12]             : AIC=inf, Time=0.50 sec
[2023-05-04 18:02:01,764] {subprocess.py:89} INFO -  ARIMA(1,1,3)(0,1,1)[12]             : AIC=inf, Time=0.50 sec
[2023-05-04 18:02:01,765] {subprocess.py:89} INFO -  ARIMA(0,1,3)(0,1,1)[12]             : AIC=inf, Time=0.51 sec
[2023-05-04 18:02:01,765] {subprocess.py:89} INFO -  ARIMA(2,1,1)(0,1,1)[12]             : AIC=inf, Time=0.44 sec
[2023-05-04 18:02:01,766] {subprocess.py:89} INFO -  ARIMA(2,1,3)(0,1,1)[12]             : AIC=inf, Time=0.80 sec
[2023-05-04 18:02:01,767] {subprocess.py:89} INFO -  ARIMA(1,1,2)(0,1,1)[12] intercept   : AIC=inf, Time=0.65 sec
[2023-05-04 18:02:01,768] {subprocess.py:89} INFO - 
[2023-05-04 18:02:01,769] {subprocess.py:89} INFO - Best model:  ARIMA(1,1,2)(0,1,1)[12]
[2023-05-04 18:02:01,770] {subprocess.py:89} INFO - Total fit time: 14.465 seconds
[2023-05-04 18:02:01,770] {subprocess.py:89} INFO - Step 4: Predicting best fit using AIRMA models, Done
[2023-05-04 18:02:01,771] {subprocess.py:89} INFO - Step 5: Reformatting predictions according to DB requirements, Done
[2023-05-04 18:02:01,772] {subprocess.py:89} INFO - Step 6: Reformatting historical data according to DB requirements, Done
[2023-05-04 18:02:01,773] {subprocess.py:89} INFO - Step 7: Concatinating historical data with predictions, Done
[2023-05-04 18:02:01,774] {subprocess.py:89} INFO - Step 8: converting frequencies to 'int', Done
[2023-05-04 18:02:01,942] {subprocess.py:93} INFO - Command exited with return code 0
[2023-05-04 18:02:01,964] {taskinstance.py:1280} INFO - Marking task as SUCCESS. dag_id=perform_predictions, task_id=predict.postgres.google_trends, execution_date=20230504T175700, start_date=20230504T180140, end_date=20230504T180201
[2023-05-04 18:02:01,995] {local_task_job.py:154} INFO - Task exited with return code 0
[2023-05-04 18:02:02,017] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
